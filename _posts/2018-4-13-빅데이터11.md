---
post : layout
title : 빅데이터11
---
## 빅데이터11
빅데이터11

## Bloom Filter
값이 들어왔을때 값을 멤버십 체크를 하는 filtering이다. 용량을 적게 먹는다. 복잡도 O(1)으로 처리된다. 성능도 좋다.

key값이 들어오면, stream data가 들어올때 hash를 하고 k개 (여러 개의 hash function을 받아들이고) hash function에 대해 hash를 하고 해당 bit 들을 1로 setting. 
false negative라는것은 존재하지 않는다. 
false positive는 존재할지 모르지만, false negative 존재하지 않는다.
false negative라는것은, 없다 라고 했는데 사실은 있다. 이런것은 존재하지 않는다.
반대로 false positive는 존재한다. positive 하다 라고하다 라고 생각했는데 있다. 사실은 거짓말 이다.(hash를 쓰게 될때 hash collision이 일어나서 )
false positive로 error 율이 나오는데 error 율은 숫자로 증명되긴 하지만, app에 따라 값을 조정함에 따라 (k,m,n) 낮출수도있고 높힐수도 있다.
error 율이 낮으면 다른 요소 하나를 희생해야 할 수 밖에 없다. 
error 율 높히면 size 줄여도 된다. 메모리를 적게먹어도 된다. 

## Bloom Filter -- Analysis
false positive 확률이 (1-e^-kn/m) 이다.
bloom filter가 8billion 이라 하자. ++n은 들어오는 stream data 개수 (key 값의 개수) k는 hash function의 숫자 m은 bloom filter size
++
## Bloom Filter -- Analysis
error율은 이미 정해져 있다. bloom filter를 계속 늘리면 성능이 좋아지는지 궁금한 그래프다.
bloom filter의 size는 정해져 있는데 hash 개수가 많아지면 거꾸로 저장해야 되는 bit가 많아진다. 어느 한순간 최적화 문제로 된다. 어느 일정 수준 이상되면 나빠진다. 
error율 을 보여주는데, k값이 점점 늘어나면 값이 어떻게 바뀌는지를 보여준다. hash function이 하나다. 0.1 정도의 error이 나오더라, hash function을 2개로 늘렸더니 0.04 로 error 율이 낮아져서 좋아지더라, 그러나 계속 좋아지다가 계속 나빠진다. (순전히 hash collision 이유때문에)
이럴땐 최적화 문제로 넘어간다.
변수 k,m,n 을 가지고 error율 이 제일 낮은 점을 찾고 싶어. 
그래서 n,m fix 시키고 생각했을때 k에 대한 변수값은 찾을 수 있지만, k를 찾기 위한 공식이 나온다.
optimal performance를 보여주기 위해서 n값도 정해져 있고 m값도 주어져 있고 몇개의 bloom filter가 필요할까?
공식 : k:(m/n)log2
증명 

optimal한 bloom filter의 개수를 찾는 공식은 m/nlog2 이다. m이 주어지고 n이 주어지면 현재상태에서 best performance를 받아내기 위한 bloom filter 개수는 몇개!?  8log2 가나온다.
자연로그2는 0.69 정도 된다. 5.54 정도 나온다.
flating이 되어야 하므로, silling function도 넣어서 6개(자연수) bloom filter가 6개가 필요하다 결론을 내린다.
app에 따라 달라진다.
k가 6일때 에러비율은 0.0437 정도 된다.

## Bloom Filter : Wrap-up
##### Bloom filters guarantee no false negatives, and use limited memory
bloom filter는 false negative는 나타나지 않는다. 하나의 장점이다. false positive return
false positive 를 우리가 줄일 수 있는 가능성이 있다. 그 수식이 존재한다(큰 장점)
pre-processing을 자주쓴다. bloom filter의 구조는 단순하다. hash function이 있고, bit array만 있으면 됨. 글고 값이 들어왔을때 hash function을 hash 하고 해당 bit에 1로 세팅. optimal performace를 뽑기 위한 공식도 다 나와있다. 하드웨어로 읽어들인다.(회사에서는) stream data를 app에서 처리하는거와 하드웨어단에서 처리하는것은 성능 차이가 비교가 안된다. 그래서, 하드웨어로 구워버린다. 그렇게 만들면 pre-filtering을 할 수 있다. spam으로 예를들면 들어올때 h/w적으로 걸러서 들어왔는데 100%라는 보장은없지만 그 이후의 post-processing은 따로 할 지 언정 우리가 들어온것을 보고 맞다 안맞다 정도는 순식간에 알 수 있다. 그 다음 일은 너희들이 알아서 해라
전처리 만 빨리 끝내줘도 app에서 하는일이 굉장히 줄어든다. (중요) 성능업

bloom filter는 하드웨어적으로 만들어서 pre-filtering 하는데 많이 쓴다.

##### It is better to have 1 big B or k small Bs(시작전에)
문제라기 보단, m의 사이즈가 8billion일때 들어오는 key의 개수는 1billion 이었다. 그런데, 길게 k=4 가정(hash function = 4) key값이 들어올텐데 bloom filter가 저장하는 방식을 고려했을때 m을 8billion이라고 가정하자. 들어왔을때 어딘가에 저장이 될것이다. 
1/4씩 깔끔하게 1/k 로 나눠서 저장하자. 두가지 방법이 있다. bloom filter를 4개로 쪼갠다. 그래서 8billion 이었지만 쪼갠 이후 2billion씩 하다. 메모리 사이즈는 동일
각각 하나씩은 자신의 hash function을 가져다 h1,h2,h3,h4 라 하자. 해시 함수가 있을때 1)에서는 하나가 4개를 share하지만, 2)에서는 h1은 니가쓰고 (단, 너한테 assign된 값은 너만 저장해)
해시함수는 2는 같은 이치로 너는 이곳만 저장을 해시함수 3,4 너는 요기만큼만 저장해 이렇게 생각할수 있다.
(중요, 다른점) 들어오는 input 개수 같고, 하지만 우리가 선택할 수 있는 기준은 bloom filter의 size의 hash function의 개수 이것은 우리가 조절할 수 있다. 근데, 메모리 같고 해시함수 개수도 같고, 똑같다. 그래서

##### It is better to have 1 big B or k small Bs?
++1 big B or k small Bs? 어떻게디자인할꺼냐?++
이렇게 하면 성능이 달라지는가?
성능은 안달라진다. 디자인적인 측면이다. bloom filter를 잘게 쪼개서 1) 2) 방법을 쓰는건 프로그래머 마음이다. bloom filter는 잘게쓰여진것을 잘안쓴다(data structure를 따로 잡아야 한다.)
통상적으로 하나로 퉁쳐서 하나의 길이로 잡는것이 편하다. 계산하기도 편한다. => 귀찬아서 전자방법을 쓴다.
하나로 길게잡을때 false positive error 는 파란색 1-e ~~~, k개 그냥 쪼갰을때 false potive error확률은 m(bloom filter size)가 k만큼 쪼개진다. 
각각의 error율은 ppt를 통해 보자. 값은 같다.
bloom filter 들어왔을때 큰거 잡으나 잘게쪼개서 잡으나 결과적으론 같다. 데이터 스트럭쳐가 더 심플하기 때문에 긴하나를 잡아서 하는것이 구현이 쉽다. 산업체에서는 이런방법으로 하나로 잡는것을 많이 쓴다.
구현하기 간단한다(bloom filter)
아이디어는 간단하고 성능이 좋은것이 best다. 단순하게 computatin 간단한것을 선호

## Counting Distinct Elements
stream data에서 distinct한 element를 셀 것이다. stream data가 많이 들어왔을때 전체 개수가 아니라, 중복되는거 빼고, unique한 element가 몇개인지 셀려고 하는것이다. app에 따라 달라진다. 

##### Problem : 
stream data가 들어온다. 빨리 분류해서 unique한 애가 몇개인지, n개를 놓고 바라볼때 이속에 unique가 몇개 있는지 알고싶다. 그 알고리즘이다.
hash 쓰면 된다. (중요, hash) hash는 O(1) 상수의 복잡도를 갖는다. 간단하고 하드웨어 구현하기좋고 성능좋다. 그렇게 때문에 hash를 많이 쓴다.

distinct 하면, element가 들어오면 hash 쭉하고 bloom filter에 저장하고 bloom filter 에 bit를 수를 세면된다. 매번 세줄수있으면 좋겠지만, 매번 불가.

## Applications
어떤 stream이 들어왔을 때 unique한 아이템만 세는것은 어떨때 많이 쓰냐? 아마존의 경우 지난 한주 동안 엄청난 양의 팔텐데 지난 한주동안 팔때 아이템 다른 종류를 팔았던거야??, 하나의 아이템이 몇백번을 팔리던 하나 팔린거니까 하나만 팔린것이다. 그게아니라, 그런식으로 팔린 데이터 tranjection이 들어올때 unique한 아이템만 우리가 셀수있으면 보고 할 수 있다.

++web page crawled 할때 데이터를 긁어오면 그중에서 각각 다른 워드가 그 웹페이지에 몇개가 있느냐?++ 이짓을 하는 이유 spam때문이다. (crawling - 웹페이지를 그대로 가져온뒤, 가져온 내용을 기반으로 데이터를 추출해 내는 행위다.)
spam이 보통 spam은 나쁜짓을 할때 근사하고 대단히 좋은 웹페이지를 만들지 않다.attack 목적이 아니니까 대칭적으로 웹페이지 대충 만들어서 거기다가 random 값을 다 때려 넣는다.(같은것으로) random 같은것으로 때려넣는다.
artificial하게 web page를 만들다 보면 이런데서  random 이 걸릴수있다.  그러다 보니까 이상하게 1)웹페이지를 crawled 해서 2)word counter를 해봤더니 같은 word가 너무 많거나 random 단어를 넣어는지 3)unique한 개수가 너무 많아. 중복되는게 없어 3가지 경우가 있다. (crawling 시 word count 해보면 위와 같은 현상이 발생한다.)

spam 이든지, 사람들이 악의를 가지고서 artificial page 만들었다던지, 그런 싸이트를 가져다 구분해 내기 위한 pre-filtering 작용을 한다. 또 쓰기도 한다.

어떤 custmer 가 지난 한주동안 웹 페이지 들어갔는데 웹 사이트 지난 주에 들어갔을까? (몇개의 웹 사이트들어갔을까?) - 통게치 잡을때 웹사이트 주소를 가져다 filtering 해서 unique한것을 잡아 내면 된다. app은 이게 아니더라도 여러가지 있을 수 있다. 가장 대표적인 app이 된다.(지금꺼)


## Using Small Storage
그많은 데이터를 가져다 메모리에 넣을 수 있으면 못할게 없지만 안된다. 8billion이 엄청나게 많다. 사실은 8billion이 1Gz일것이다.  8billion bit는 1Gzbyte다. 작게 느낄 수 있지만, 왜 크냐면 통상적으로 회사에서 일을 처리할때 multi로 처리한다. 대용량 서버를 두고 많은 stream을 동시에 처리한다. 서버가 비싼 서버 있다고 가정하자. 그 서버안에서 1Gzbyte가 얼마 안되지만,하지만 100,1000,10000개 stream을 뿌릴텐데 1000개 stream을 처리하면 1000GB, 1TB가 된다. 돈과 직결된 문제.(10000개면 10TB, 엄청 큼)

n개의 stream 용량을 다 저장하지 못하면 어떻게 할래? stream data 특성상 정확히 같은 값을 요구할순없음. (근사치 값을 요구한다.)

지난 한달동안 물건 팔렸다를 보고할때 어느정도 대략 12만개정도 팔렸다.해도 무리없다. 그런 용도로 쓰기 위함이다.

이것을 해결하기 위한 알고리즘은 

## Flajolet-Martin Algorithm
이 알고리즘을 이용하면 unique한 element 개수를 셀수가 있다. hash function이 있다. 어떤 용도로 쓸꺼냐면 mapping 용도가아니라, hash 함수를 가져와서 n개의 element만본다(window를 n으로 가정) 그것을 표현하기 위한 bit는 logn이다. n개의 element의 경우는 최악의 경우 각각 모두다 unique 할수있기 때문에 최소한 logn bit만큼은 그것을 표현하기 위해서 필요하다. logn bit 만들어놓고 key값이 들어오면 hash를 해서 그 나온 hash value를 가져다 이 logn bit에다가 저장을 한다. logn bit 저장하면 010101...이런식으로 저장하면 이것을 계속 분석을 한다. 

이 알고리즘은 hash 함수를 이용해서 값을 끌어오는것이 아니라, hash 함수를 떠서 그 logn bit에다가 해당 되는 값을 순전히 저장하기 위한 용도다. hash함수를 가지고 logn bit에다가 hash value저장하는 이유는 hash value를 쓰게 되면 굉장히 높은 확률로 evenly distributed가 된다. 
log n bit에 저장하는데 같은 값만 들어오더라도 다른 아이템일수 있다. stream의 각각 element 인데 그냥쓰면 evenly districuted 확률이낮다. (중요하다 evenly distribute 하는것이) hash 함수를 쓰는 이유는 순전히 이것이다. 

새로운값이 오면 hash를 하고 그 value를 가져다 log n bit에다가 저장을 한다. 어떤 특징이 있냐하면 결과부터, 각 hash bit, logn bit 가 결과적으로 0101 이런식으로 저장이 될텐데 그 bit의 제일 끝에서 0이 몇개 나오는지 센다. consecutive한 연속적인 0이 끝에서 몇개가 나오는지 센다. max값을 유지한다. 끝에서 부터 0000 나오는 값을 tail length 라 하는데, tail length를 max값을 계속 저장 max값이 나오면 계속 update 한다. 
결과적으로 어느 한순간에 보면 max의 tail length가 얼마나 나온다. tail length (4) 나오면 unique한 아이템이 2의 4승개다.(중요)
쉬운데 말이된다.(best)

## 정리
각 stream이 들어온다. 이게 a가 됬던 stream이 쭉들어온다. 그 값을 가져다 hash 함수로 잡아! hash 함수를 가져다 hash를 하면 어떤 값이 나올것이다. 그 값을 가져다 logn bit에다가 저장하고 그러고 나서 r(a) 값이나오는데(tail length), h(a)(hash된 hash 값)

h(a) 에서 t(0) 제일 끝에서부터 연속적으로 0이 몇개가 나오는지 r(a)에 저장.
저장을 한뒤 r(a) 값을 tail length라고 부르고, 오른쪽에서 부터 제일 먼저 1이 나오는 position 인데 (=연속적인 0의개수 말이같다.) 



