---
post : layout
title : 빅데이터5
---
## Data Deluge Era
60초안에 많은 양의 데이터를 처리할 수 있다.

## Outline

## Data Processing Market Trends
마켓에서도 빅데이터의 수요가 증가하고 있다.

## I/O Performance Issue
BI and analytics tools : I/O bottleneck

## Heavy Data I/O
##### Big data processing framework(e.g., Hadoop)
Input data는 클라우드에서 가지고 온다. 
빅데이터의 과정을 살펴보면, I/O가 많이 일어난다. CPU가 증가하면 성능이 늘지 않는다.

## Moving Data is Expensive
##### Moving computation is cheaper than moving data

## Near Data Processing Technologies
최대한 데이터에 가깝게 computing을 시도한다.
##### Exadata 
컴퓨팅 노드를 두고 데이터를 미리 처리해서 올린다.
##### Netezza S-blade[IBM]
컴퓨팅 노드를 CPU Core에 가깝게 두고 계산된 값만 넘겨준다.
##### Intelligent SSD

## SPU(Storage Processing Unit)
데이터 저장자체 있는곳으로 컴퓨팅을 시도한다.

## ISC
ISC SSD에 처리한것을 Host로 던져줌

## WHY?
컴퓨팅 으로 데이터 이동시 오버헤드가 크고 이것을 최소화하려고 했다.
컴퓨팅 이동시 먹는 전기량이 크다. 

## Hadoop
Hadoop = HDFS + MapReduce

## Hadoop MapReduce Framework

## Problem Statement
원래 host로 돌아가는것은 SSD 내부로 돌아가도록 설계했다.

## Outline

## ISC software architecture
Map은 ssd에 두고(ISC Device) reduce만 host에서 처리한다.(host)

## How to develop ISC applications
Host program 과 Device program(ssdlet)을 x86으로 컴파일 한후 ISC SSD Emulation 돌리고 ARM(SSD는 ARM에 적격이다.) Cross compile한뒤, SSD에 넣는다. 내부 코드가 저장된 SSD는 host와 직접 일을 처리할 수 있다.

## Design Challenges
###### 데이터 표현 불일치
SSD 내에서 64M를 읽을 방법이 없다. File System이 존재하지 않는다. (Mapper가 요구할때 File을 읽을 수 있는 방법이 없다.)
리눅스는 address로 쪼깨기 때문에(File System이), 메모리에서 어드레스를 다 찾아야 한다.
##### 시스템 인터페이스의 불일치

##### 데이터 쪼개기
word를 count 하는데 정확한 단어를 구분하기 어렵고, 파일시스템이 없어서 칼 같이 자르기 때문에 (기존의 하둡은 스트림으로 짤라서 64M으로 붙여버린다) ISC에는 기능이 없기 때문에 Host로 던져서 처리를한다. 그리고 SSD로 받는다.(크게 오버헤드는 없다.)

##### 특징을 없앴다.
ssd는 host cpu와 다르다. ssd에는 Map기능만 처리하는것을 내렸다. Reduce는 Install 측면에서 local에서 수행할때 map이 다른곳에서도 데이터를 끌고 오기 때문에 host에 남겨두었다.
ssd에 네트워크를 붙여줄 수 있다면 ssd가 알아서 다른 데이터를 읽어오고 하겠지만, 우리 일반적인 데스크탑은 기능을 제공하지 않기때문에 Reduce는 ssd로 내리지 않았다.

## 완전 분산 모드
하둡이 지원하는 기존의 3가지 모드에 지원이 안되는것이 있는데, 하나의 머신으로 완전 분산을 해야 했다.
namenode 하나에 datanode를 묶어서 cluster 기능을 구현했다.(port를 나눠서 데이터노드에 분산했다.)

