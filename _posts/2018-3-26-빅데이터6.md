---
post : layout
title : 빅데이터6
---
## Workflow
##### key
핵심적인 아이디어는 Mapper를 가져다 ssd내부로 내린것이다.

##### problem
하둡은 기본적으로 하둡 맵 + 리듀스는 맵퍼부터 시작을 한다.
Mapper에서 시작을 해야하는데 Mapper가 없다 호스트에 호스트에 맵퍼가 없다 보니까 호스트에서 맵퍼를 실행시키지 못한다.

##### No file system
여기서 곧바로 인터페이스로 넘어가서 64 M로 던져주고 64M당 Mapper에 asgin된다.

##### 파일 시스템이 없다!
우리는 ssd 속에 있는 Mapper가 파일시스템이 없기 때문에 들어오는 데이터에 대해서 읽으면 읽을수있는방법이 없다. 

##### 쪼개서 던져줘!
인터페이스속에서 데이터를 읽어 라고 하면 코드가 하는일이 LBA로 변환을 해서 SSD 속으로 이 파일이 64 메가 파일이 몇개로 나눠서, 어드레스 번지를 몇번부터 몇번까지를 반환한다. 
64 메가를 던지면 리눅스는 대부분은 큰 데이터를 던져주면 다시 쪼개기 때문에, 단 리눅스에서 몇개를 쪼개는지 알 수 없다. 순전히 파일시스템이 알아서 한다. 

##### 번지에 대한 주소를 리스트로 보내줘!
데이터를 읽을수없으니까 메모리 번지에 리스트를 가지고 와야 하는데, 그 일을 맵퍼 에 SSD 속에 Mapper가 데이터를 알 수 없지만 ++데이터가 무슨 번지++에 해당한다를 알려주기만 하면 된다. 
번지에 해당하는것은 Mapper에게 알려준 역할을 한다.

##### 실행
쪼개서 lsb 어드레스를 던져주고 이 ssd 내에서는 lba읽어 라고 하면 lba가 어디에 있는지 lba번지를 알려줘서 그 정보를 가지고 Mapper를 실행시킨다.

##### 로컬 파일 시스템
이제는 메모리 주소 번지가 64M에 해당되니까 그 번지수를 가지고 그 버퍼를 실행을 요청을 하면 맵퍼가 실행이된다. 데이터가 뭔지 몰라도 어드레스를 아니까 수행을 하고 나면 결과값을 로컬 파일시스템을 이용해서 써야 한다.

Mapper는 로컬 파일 시스템을 이용해서 써야 한다. 로컬로 자기자신이 일을 하고, 자기 자신이 자신한테 써야 하기 때문에 호스트에게 써달라고 다시 부탁해야 한다. 
Mapper의 아웃풋은 리눅스 파일 시스템을 이용해서 써야 한다. 

##### Reduce
리눅스 파일시스템을 이용해서 써야 한다. 중간 값이 다 나오고 Mapper가 호스트에서 리눅스 파일 시스템을 이용해서 쓰여지고, Mapper는 다 끝았다고 나온다.
Mapper가 끝이 나게 되면 Namenode에 알려준다.
Reduce는 Mapper의 데이터를 가지고 데이터 작업을 한다. Reduce가 맵퍼의 데이터의 값을 읽어서 최종 아웃풋값을 적어준다. 호스트에서 이제 Reduce가 다 끝났으니까 실질적으로 Mapper와 Reduce의 작업이 끝난다.

## Configurations
name 노드 1개에 data 노드 4개, 즉 5개의 클러스터로 구성
싼 데스크탑에 1기가, ssd로 구현하고 (8개-mapper구현각각)
데이터 사이즈도 다르게 주었다.
## Performance metrics
전체 수행시간(start ~ end)
mapper 수행시간
첫번째는 얼만큼빠르냐, 두번째 전기 얼마나 적게먹냐, 세번째 cpu를 얼마나 적게 먹냐
## Overall performance
2.3배 더 빠르다.

## Analysis : total elapsed time
2배이상 빠른 이유 검증, 증명
하둡 맵 리듀스 실행시키면 잡을 setup 타임이 요구
그러고 나서 setup 끝나면 jab tracker가 시킨다.(1~3초)
Map이 데이터를 다 읽고 시간이 오래걸린다. Reduce 중간결과 적으니까 처리.
중간 데이터를 쓰고 읽고 하는데 끝나면 다 지운다. cleanup

ISC도 같다. Mapper의 성능이 빨라진다. 나머진 같다. 보통 노란색 맵퍼를 가져다 세분화 시키면 맵퍼 자체도 계속 setup 이 요구된다. 짧은 시간동안 map일을 하고 Map도 cleanup 타임이 있는데 리듀스가 다 읽어갔다고 하면 중간 데이터를 다 지운다.(성능향상)
## Analysis : energy consumption
Host에서 cpu를 얼마나 많이쓰냐, 메모리를 얼만큼 쓰는지 체크
첫번째 cpu를 얼마나 많이 쓰냐이다. ISC가 두배 이상 빠르다. 수행시간이 빨리 끝난다. CPU 쓰는시간이 작다.

두번째가 메모리를 얼만큼 먹느냐, 노란색 ISC는 적게 먹는다. 빨리 끝난다. 적게먹고 짧게 쓰고 만다. 전기도 적게먹을 수 밖에 없는 구조다. 64M당 똑같은 일을 반복하기 때문에 패턴 반복

## Scalability
왼쪽이 싼 데스크탑, 오른쪽은 비싼 서버
검은색 라인이 일반 SSD 4개까지는 성능이 좋아진다. 4개이상은 성능이 나빠진다.
ISC는 계속 해서 성능이 좋아진다.
++Mapper 가 많으면 좋으냐, Mapper의 개수는 통상적으로 컴퓨터의 코어 개수만큼 Mapper를 만든다++
4개짜리 코어 컴퓨터라 코어당 1:1로 assign 하니까 성능이 좋아지는데 5개부터는 4개의 코어의 자원을 뽑아서 써야해서 성능이 악화.
ISC는 CPU를 많이 쓰지 않기 때문에 몇개를 달아도 성능이 향상된다.
서버에서는 자원이 크기때문에 4,5,6... 성능이 좋아진다.
++ISC 개념으로 CPU도 DRAM도 적게먹고 호스트의 자원이 남게 되니까 남는 자원을 다른데 효율성 있게 쓸 수 있기때문에 더 많은 일을 할 수 있다!++

## Analysis: scalability
4개를 꽂으니 성능이 100% 먹고 있는데 더 자원을 넣어봐야 성능이 나빠진다. 8개(빨간색) 더 집어 넣었더니 컴퓨터 효율 떨어짐.
ISC는 비례해서 좋아진다. 4개를 꽂아도 65% 밖에 안쓰고 자원이 남아돈다. 8대 100%에 육박하지만 95% 정도된다.

## Total elapsed time(cluster)
하둡은 클러스트에서 돌아간다. 5개의 노드를 만들어서 하둡 클러스트만들어서 실행, 한개의 네임노드 4개의 데이터노드 
실질적으로 데이터가 저장되고 맵퍼가 일을하는것은 4개의 데이터노드다. 
노드 한대일때 isc가 더 빠르다. 클러스트 한대 두개 절반.. 절반... 분산처리 이유다.. 비싸지 않는 머신으로 계속 데이터를 처리하는데 분산처리원리로 동시에 처리하니까 꽂음면 꽂을수록 성능이 좋아진다.(값싼것을 묶어서 성능을 증가시킨다.)

## Power consumption(cluster)
노드 하나일때 typical한 하둡
클러스트 하나 더 붙임(노드하나 더붙임) 성능은 2배 빨라진다 전기는 많이 먹지만.
isc는 전기 사용량이 크게 증가되지 않는다. 성능은 좋아지는데 2배로
## 결론
데이터가 중심이 되는것이다. 데이터가 I/O 많이 일어나는 원리 자체가 CPU중심이었다가 데이터를 이동하는 비용이 커서 계산코드가 올겨올 수 밖에 없다. 트렌드는 데이터가 이동이 아니라 Computation이 이동한다. 
Install의 문제, ssd 코드 많아져도 컴퓨터 cpu 반도 못따라간다. ssd에 좋은 cpu 못씀(가격 대비), ISC 구현한 SSD는 평범한 SSD와 같게 작동한다. 일반 SSD+ISC도 구현해야되기 때문에 CPU의 포화상태가 됨. 데이터를 처리하는 앱은 다양, Computation 요구를 많이 하는 cpu 성능을 중요시하는, 그런 앱에서는 ISC가 성능이 떨어진다.
SSD속 CPU의성능이 호스트 CPU를 못따라간다.

## Data Streams
많은 빅데이터를 처리하는 상황들이 있고, 미리 전체 데이터의 종류들을 알지 못한다. 정형화되지 않는 데이터들도 있다. Data stream 같은경우...
Retail sale data : 판매 실적 같은것을 확인하고 특정 상품의 팔린것도 확인가능하게 해주고 하지만 real time이 요구된다.
Media publisher : 많은 사람들이 좋아요 기록들 페이스북이나 트위치에 남겨 있다.
Sensor network : 버스 같은것을 생각해보면된다. 어느시간대에 어느 버스가 오는지
Financial institution : 증권이나 뱅킹에 디비에서 트랜잭션이 일어날경우 많은 데이터들이 오고 가니까 이런데서 나타나고..
Online game : 온라인게임들 모든 게임 아이템들 ..
웹 쿼리 도 있고, 이렇듯 이러한 자료들이 Stram data로 되어있어서 현실적으로 겪고 있는 것이지만 빅데이터 프로세싱을 통해 직접 쓰고 있는 것이다. 이런 데이터들을 처리하는 알고리즘을 공부하자!
## Data Streams
데이터가 무한하고 non-stationary 하다는것은 페이스북이나 트위치에 좋아요 클릭이 다르듯 일정하지 않다는것이다.
하둡은 사용자가 컨트롤한다면 Stream Data는 사용자가 컨트롤하지못하고 데이터를 발생시키는 위치에서 컨트롤한다는 것이다.
볼륨이 크고, Real Time으로 실행되고, 데이터를 대부분 한번 읽고 버린다. 메모리가 제한되어있다.

## Bath processing vs stream processing
|  | 배치 프로세싱 = 하둡 | 스트림 프로세싱 |
|--------|--------|-|
|데이터 스코프           | 전체데이터       |쿼리문 자체가 지난 몇시간 부분으로 부분만한다. |
|  데이터사이즈 	|큰 데이터를 받는다	|마이크로 단위로 작게, 짧게짧게 끊어서 수행한다	|
|퍼포먼스| 큰데이터를 수행하므로 오래걸린다 | 초 또는 밀리 초 단위의 대기시간이 필요하다.|
| 분석 | 전체 데이터 셋을 가지고 있기 때문에 다양한 분석을 할 수 있다. | 간단한 응답들의 작업들만 분석이 가능하다.|
## The Stream Model
Input element = 들어오는 데이터라고 생각
Input port = port  당 들어오는 스트림으로 하나가 될 수 도 있고 멀티 스트림으로 될 수 도 있다.
들어오는 element는 스트림들이 튜플형식으로 set되서 들어온다.
왜 튜플인지는 간단하게 구글을 보면 알 수 있는데 구글은 Id/검색유입/ 웹사이트 로그정보를 set으로 google에서 send되면 이자체로 튜플이 구성된다. 
++따라서, 시스템은 전체 데이터를 저장하는것이 불가하다.++
이러한 악조건 상황에서 할 수 있는것은 모델링인데 다음의 모델링을 보면.
## General Stream Processing Model
Processor안으로 데이터가 처리되고 Standing Queries는 말그대로 쿼리가 들어올때 질의를 규칙적으로 처리하는 애다. 그 시간에 항상 처리한다. 
Ad-Hoc Queries는 유저 request가 있을때만 처리를 한다. 
Limited Working Storage는 메모리로 생각하면, Archival Storage(하드디스크)에서부터 자주 쓰는 데이터만 넣어놓고 처리를 하고 데이터를 Archival Storage로 다 넣고 처리하는것이 불가능 하기 때문에 메모리롤 옮기는것이다. 자주쓰는것만 옮긴다. 메모리가 값이 비싼것도 한몫한다. 그렇다면 메모리를 적게 쓸 수 있는 방법이 있는것이 효율적일텐데 다음은 그 알고리즘을 살펴본다.

## Problems on Data Streams
Sampling은 데이터를 어차피 다 처리하지 못하기 때문에 부분만 처리해서 샘플링한다. 근사값을 구한다.
Sliding windows는 중요한것은 들어오는 데이터를 전부다 처리할 수 없다는것이다.
현재 기준으로 과거의 시간이나 날을 기준으로 그 부분만 보고 데이터를, 벗어나는것은 다 버린다. = 다 볼 필요가 없다.(시간이나 날을 기준으로 하는것이 window time 이라 한다.)
## Problems on Data Streams
여러 알고리즘이 더 나오는데, 
Filtering a data stream - 원하는것에 맞는 데이터를 보는것
Counting distinct elements - 샘플링하지않고 그자체를 보면서 수많은 데이터들중 (스트림으로 들어오는) 중복된 데이터를 빼고 유니크한 데이터를 근사하게 계산해서 어떻게 빨리 메모리를 적게 쓰면서 근사치에 알맞게 카운트 할 수 있는지 확인한다.
Estimating moments - moment에 값을 estimate해서 stand average를 빨리 구하는것
Finding frequent elements - 자주 나오는 스트림 데이터형을 찾을 때 
## Applications
Mining query streams - window time으로 잘라서 자주 쓰는 쿼리를 알고 싶을때 구글이나 웹 에서
Mining click streams - 페이지 방문이 전보다 얼마나더 비상적으로 증가하는지 알고싶을때 야후나 이런데서..
Mining social network news feeds - 트렌드 볼때 페이스북이나 트위치 등..
Sensor Networks - 많은 센서들이 중앙 컨트롤에서 공급된다.
Ip packets monitored at a switch - Ip가 인터넷에서 hufs.ac.kr 네임서버가 ip로 바꿔서 보내게 되는데 이러한 source와 destination을 헤더에 붙여서 tcp/io 프로토콜 계층을 통과할때 link-layer 계층에 스위치를 통해서 목적지를 찾아가는데 패킷이 순식간에 스위치로 들어왔을때 빨리 어디로 보내지는지 결정할 수 있는데 이것은 스위치의 성능이 가격에 따라 다르고 여기서는 Ip패킷이 Stream Data로 스위치에 보내지면 어떻게 처리하는지를 보게된다.(hop by hop) 그 윗단에는 라우터가 있겠지..

## Sampling from a Data Stream
Sampling이 데이터를 전체 다 보지 못하니까 일 부분만 볼 목적으로 샘플링 한다 했다.
1. Fixed proportion of elements in the stream
샘플링을 하는데 주기적으로 샘플링한다, 10초에 한번 element가 들어오면 10개에 무조건 하나 샘플링 이런식으로 하게된다. 이런식으로 주기적으로 샘플링 하는것이 fixed 다.

요기서 하드디스크의 용량이 제한되니까 10초마다 저장한다고 생각하고 이 데이터가 쌓이면 결국 제한이 발생하기 때문에 fixed size로 1G 데이터만 저장할 것이다. 혹은 100개의 샘플링만할것이다. 전체 데이터양의 size를 fixed하는것이다. 
a random sample of fixed size이다.

## Sampling a Fixed Proportion
구글에서 뭔가를 서치할때 튜플로 들어온다고 했는데, windows time 에 어떤 유저가 똑같은 검색을 얼마나 자주하는지 궁금하면 쿼리로 스트림의 튜플형식으로 들어올때마다 10개중 1나만 샘플링했다. 그래서 10개중에 특정 숫자 0 이 나오면 샘플링하고 다른 숫자가 나오면 버렸다. 하지만
쿼리가 변경되면 이전 스트림 데이터들은 쓸모가 없는 데이터가 된다. 사람이 중심되서 어떤 특정한 사람이 2틀에 한번꼴로 똑같은 검색을 한다하면 이전 알고리즘으로는 확인할 수가 없을 것이다. 따라서 유저중심이 아니기 때문에 유저에 대해서 쿼리를 날리면 알 방법이 없다. 
유저 중심의 샘플링은 유저별로 뽑을 수 있게 유저가 중심이 된다. 유저가 10명이 들어오면 한명을 샘플링하는형식이다.

## Solution: Sample Users
유저가 10명 들어오면 한명을 샘플링하고 샘플에서 검색을 할 수 있다.
해시펑션을 사용하는데 10개의 버킷안에 균일하게 유저id나 유저이름을 해시하는 함수를 사용한다.
key가 유저중심으로 바라보면 달라진다. main이 key가 되는데 유저가 key가 되서 (key는 하고싶은 쿼리나 app에 따라서 key를 결정) stream을 샘플링 하는데 a/b 확률로 샘플링한다. b번 들어올때마다 a개 샘플링한다.

## Generalized Solution
키를 가지는 튜플형식의 스트림은
key는 각각 튜플 구성요소의 일부 집합이되고,
키는 앱에 따라 다르게 설정할 수 있다.

a/b의 형식의 스트림은 샘플로서 아래와 같이 해석한다.
샘플링 b번들어올때마다 a개 샘플링 한다는 의미다.
가령 b=10, a=1이면
해시 버킷 10개에 키를 통해서 해시 하고 0번째 버킷을 선택하고 샘플링 한다.
30% 라는건 0,1,2 가 10개중 3개 버킷을 정해서 버킷에 들어가는 값이 key값을 헤시 했을때 나오면 샘플링 한다.

b 버킷들 안에 튜플의 키를 각각 해시한다.
해시된 값이 거의 a에 가까우면 튜플을 선택한다.
## 중간 정리
Data stream이 메모리를 적게 쓸려고 알고리즘 고안했는데
두가지 방법이 있다. fix랑, fix size건데 기본적으로 user id, 검색기록등 이런것들이 tuple로 되어있고 기존에는 검색어가 똑같은게 오면 찾을 수 있었지만 유저로 바뀌면 이전 데이터로는 확인할 방법이 없었다. 그래서 key를 두고 확인하는데 기본적으로 해시를 쓰고 a/b 형태로 보면 b가 전체 데이터고 a가 샘플링할껀데 튜플의 key를 정하고 b 버킷 개수만큼 key를 해시해서 a에 가장 가까운애를 뽑는 것이다.
30%는 그냥 10개중 3개에 튜플의 key값을 해시해서 나오면 샘플링한다.
	
## Maintaining a fixed-size sample
위의 방법에서 문제는 fix 개수의 데이터를 받아오면, 쌓이면 제한된 메모리에 수용할 수 없다는 점이다. 따라서 fixed - size 방법을 쓰는데 size를 고정시켜서 그 만큼의 데이터만 받는다는 것이다.
stream data가 커지든 저장용량은 fixed 된다.
suppose, random sampling 이다. k개만 샘플링 할 것이다.
n개중에 k개만 샘플링한다. 만약, n=7이고 k=2면 2개만 샘플링 한다. window는 7일 유지하고 2개만 샘플링한다. 하지만, 실제 데이터는 n은 limited data이고 k도 우리가 아는 숫자보다 큰 수이다. 
그래서 굉장히 많은 n개 중에 k개를 뽑는다. 라는 말이 된다. 많은 데이터를 하드디스크나 메모리에 keep해야 하는게 실상 불가능 하므로 다음의 알고리즘을 사용한다.
Reservoir Sampling
## Solution:Fixed Size Sample
Reservoir Sampling, 앞에 Reservoir은 저수지란 말이되고 물이 끊임없이 들어오지만 나가는 물이 있기때문에 저수지 안의 물의 양은 유지가 된다. 이 알고리즘 순서를 보면,
1. Stream s에서 k개만 sampling 하는데 초기상태에서는 k를 저장한다. (k는 저장되는 것이므로 메모리 사이즈랑 관련있다.)
2. 그 다음부터 n+1개의 stream 으로 데이터가 들어오면 한놈이 나가야하는데, 새로운 n번째 element가 들어오면 어느 비율, 어느 확률로 n번째 요소를 넣을지 말지를 결정하게 된다.넣게되면, k에 있는 데이터 하나를 random하게 (가중치를 두지 않고) 뽑아서 버리고 넣게된다. 코드에서는 index에 n값을 k에 버리는 위치로 덮어버리면된다.


## Why Sliding Windows?
이전에도 데이터를 다 볼 순 없으니까, 샘플링을 하는데 sliding windows에서는 타임이 중요하다.
window만큼만 데이터를 보고 처리한다. 
이때 time window가 있을 때 시간이 지나면서 새로운 data가 들어온다고 하자. Stream Data가 최근께 들어오면 window의 tail에 있는애는 나간다. 제일 오래된것이 나간다. 이러한 Sliding Window의 방법은 시간이 중요하고 한달 전 데이터는 중요하지 않다. Real - time이 중요하다. (Timeliness-이전부터 현재까지 데이터 범위를 보는것이다.) 
## Sliding Windows
Timeliness matters는 가장 오래된 데이터는 버림으로써 해결하고, Scalability matters는 전체 history는 비실용적이다.
해결방법 : 최근 데이터의 window 크기만큼 쿼리를 제한한다.
++현재 기준으로 예전까지 window time에 들어있는 데이터만 바라보고 가장 오래된 데이터는 window에서 빠져나간다.++
Sliding windows는 timeliness와 Scalability를 해결하고 윈도우에서 많은 데이터가 들어와도 범위를 확인하고 저장하기 때문에 해소할 수있다.

N개의 윈도우 사이즈가 들어오면 N개의 데이터를 최근것만 보는것이 Sliding Windows이다. window 알고리즘에서 n크기의 window를 주긴 했지만 다 보지 못하므로 일부만 보고 근사치해서 본 것처럼 할 수있고 정확하다. (Sliding Windows 기본 개념)
들어오는 데이터를 메모리에 저장하면 여러 컴퓨터 성능에 영향을 주기때문에 이러한 방법이 좋다.

## Sliding Windows ex
아마존의 경우 많은 상품중에 데이터가 한순간에 처리되는 방식을 팔림(1), 안팔림(0)으로 둔다면 n개의 데이터중에 일부 k를 가지고 k개의 sale 정보를 바탕으로 상품이 얼마나 팔렸는지 카운팅해서 근사치를 구해 apply한다. n개는 카운팅을 할 수 없다.(너무커서)
## Sliding Window: 1 Stream
데이터가 들어오는 위치고 h가 현재 a가 가장 오래된 데이터 앞부분은 데이터를  사실상 알주 없지만 뒤에는 데이터를 알 수 있고, 새로운 데이터를 받을때마다 tail의 데이터를 버린다. 이런 방식으로 Sliding 한다.