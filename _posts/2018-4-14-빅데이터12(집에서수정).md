---
post : layout
title : 빅데이터12
---
## 빅데이터12
4월 13일 빅데이터12

리눅스는 알아야만 한다.(방학때 리눅스 고고싱)
핸즈온 능력 : 문제가 주어졌을때 코딩으로 해결할 수 있는 능력
하둡으로 컴파일 할 수 있어야된다.

## Flajolet-Martine Algorithm
Unique 한 element 개수를 세는 알고리즘.
distinct한 element를 count한다. 
stream에 element를 가져와보면 hash를 해서 값이 나오면 (hash 함수 쓰는 이유 : 확률문제라서, 근사치를 구하는거기 때문에 분포가 중요하다. evenly하게 distributed 하게 하기 위해서 사용한다.)
이전에 사용했던 hash와는 용도가 다르다.

그 결과값을 bit stream으로 바꾼다.
n개의 window를 볼거끼 때문에 worst case가 n이다. logn bit만 있으면 된다. 자연수 n개를 count로 표현할 수 있다. 

bit 0,1,0,1...
tail length = 비트로 바꾸고 나서 끝에서부터 연속적인 0의 개수. 최대한 긴 0의 개수 라고 한다.
오른쪽에서 부터 제일 먼저 나오는 1의 position.
(단, 0은 끊어 지면 안된다.)

tail length 가 r이라고 가정하면, 이 알고리즘은 그러면 stream에서 unique한 element의 개수는 대략(중요) 2^r 이다.

r이라는 변수는 keeping 한다. r은 tail length를 저장. 값이 들어올때마다 체크해서 max 값이 변경되면 update를 한다. r이라는것은 maximum taillength를 저장하고 있을 뿐이다.

어느 순간 지금까지 들어온것 중에서 unique 한것이 몇개냐? 현재까지 저장되어 있는 r을 그대로 써서 2^r 개다.
굉장히 단순하다. computation이 필요없다. 

## Why It Works : Intuition
증명방법
a값을 hash 하면 N 값으로 변경하는데
분포를 equal 하게 하려고 hash를 쓴다.

##### Heuristic intuition
element가 들어왔을때 hash 값이 h(a) 라하면
logn bit로 conversion 한다. tail length가 r이다. tail length가 r이 될 확률은 1/2^r(중요)

예제) 숫자값을 conversion 해서 bit stream 바꿀때 맨 끝이 0으로 끝날 확률은 1/2 이다. 비트가 0아니면 1이기 때문에(50%이다)

50% 는 유니크한것이 2개란 말이다. (둘중하나다.)
4개중에 한명을 뽑는다. 25% 인데, 독립된 유니크한 변수가 4개가 존재한다 라는 의미이다.(basis intelligente)

끝에 0에 끝난다 라는것은 2개 의 unique (대략)
끝이 00으로 끝난다 (1/2)^2 =1/4 이다. 4개중에 하나 25%(각각은 독립된 변수다.)

unique한 개수가 1/2^r 로 수렴을 한다. 

## Why It Works : More formally
수학적인 증명
진짜로 작동하느냐? flow가 존재.
끝에 만약에 tail length가 r이다. r개로 끝나는 bit stream을 찾을 수 있을 확률!
m이 2^r 보다 훨씬 크다!(100%육박한다)
혹은 반대다. (이런 경우는 거의 일어나지 않는다.)
m 이라는것은 정답이다. 진짜로 unique한것이 m개가 나온다! 

2^r 주장하는 측정치(추정치), unique한 개수가 주장하는 flow보다 훨씬더 많다. 라 했을때 끝이 r개인 0로 끝이나는 stream을 발견할 확률은 거의 100%다. 

반대로 unique한 값이 이 flow보다 훨씬 작다. 이런상태에서 r개의 0으로 끝이 나는 stream을 찾을 확률은 거의 없다.

결과적으로 m 이라는값을 2^r 과 비슷한 값이 나오게 된다. 
끝이 r개의 0으로 끝이날 확률은 2의 r승 /1 이다.

hash한 값 자체가 random 이고, unifrom 하게 분포된다. 가정이 깔린다. 

## Why It Works: More formally(2)
끝이 r개의 0으로 끝이날 확률은 2의 r승 /1 이다.
반대로 그렇지 않을 확률은
1 에서 그 값을 뺴주면 된다.

m이라는것은 실제적으로 몇번 일어나느냐(앞에서 m은 unique한 개수이고 여기서는 실질적으로 몇번 일어나는지)

m개의 distinct한 element에 대해서는 독립적으로 존재하는 변수다. 

distinct한것이 m개다. m개 에 대해서 그렇지 않을 확률은 m승을 곱해주면 된다.

## Why It Works: More formally(3)
m개의 distinct 한 element가 들어왔을때 끝이 r개로 0으로 끝이 나질 않을 확률 이다 (Note에 적혀있는것은) 
결과적으로 e로 표현된 이 식은 m개의 unique한 아이템이 들어왔을 때 끝이 r개의 0으로 끝이 나지 않을 확률이다.

m이 훨씬 적다고 하면 m/2^r은 0으로 간다. 
식에 대입하면 1이 된다.
r개의 0으로 끝나지 않을 확률이 가는것이다.
r개인 stream을 찾을 확률은 반대로 0이 된다.

m이 반대이다. m/2^r이 훨씬더 크다.
식에 대입하면 무한대로 가기때문에 
r개의 0으로 끝나지 않을 확률은 0이된다.
r개인 0으로 끝날 stream을 찾을 확률은 반대로 1이 된다.

이 논리가 먹히는 이유는 정답을 찾는것이 아니기 때문이다. 중간 값이 어느정도 성립된다를 주장하기 위해서.

# Computing Moments

## Generalization: Moments
data set이 존재하는데 이 데이터가 n개의 데이터로 구성된 하나의 set이다. 집합이다. 집합이 n개로 원소로 구성되어 있다. 

그때 mi 라는 값은 집합에 들어있는 특정한 원소, i는 인덱스다. 그 원소가 이 전체 stream 속에 몇번 나타나는지를 count해주는 것이다.


## Special Cases


count다. value가 straem에서 볼려고하는 window 속에 들어있는 값이 몇번 나타나는지를 볼려하는 출력값이다 라고 가정했을 때 
k번째 moment라는 정의는 출연빈도에 그 값에 k승 갔다해서 다 더한 값이다.

k번째 moment는 k승을 해서 쭉 더하는것이고 
그렇기 때문에 first moment라면 1승이다. 
2.3.4... 제곱승만 하면된다.

moment를 가져다가 배우는 이유는 이것 역시도 정확한 값을 요구하는것이 아니다. 표준편차라고 생각하면된다. 분포도를 보는것이지 값을 확인하는것이 아니다. 표준편차를 간단하게 근사하는것.

굉장히 적은 메모리와 컴퓨테이션으로 분포를 근사치를 뽑아낼 수 있는지 하기위한 moment를 두고 얼만큼의 evenly하게 혹은 uneven하게 데이터가 분포되어있는지를 근사치를 보기 위한것이 moment의 값이다.

c가 30번 나타났다 (30)^2 
0번째 moment와 1번째 moment는 많이 쓰지 않는다.
대부분 moment는 2번째를 많이 쓴다. 표준편차를 근사하는 역할을 한다.
0번째 moment는 k가 0이라는 의미이다.
0번째 moment = distinct한 숫자의 개수 
왜? 집합 set 이 존재할때 stream이 존재할때 데이터가 중복되서 들어올 수 있는데, 집합은 중복되지 않는다. stream이 들어오는데 k가 0이되버리면 무조건 1이 되는데, A라는 집합은 stream에서 unique한 원소들의 집합이다. stream이 아무리 길어도 stream이 aaaaaaabbbbccccc 라도 a는 abc 이다.(집합이니까) a,b,c 각각에 대해서 1을 더해준다.
k가 0니까 a,b,c가 몇번 나타나는지 중요하지 않는다. 무조건 1이 되버린다. 
그래서, a에 대해서도 한번 더하고 b,c에 대해서 한번 더해주고
return 하는것은 3이다(abc) 그래서 이것은 
++0th moment = number of distinct elements를 나타낸다.++

같은 이치로 1st moment는 stream의 길이다. 
1이 되는것이다. aaabbbcccdddd 들어오면 a는 3 (나타난 횟수의 1승이므로) b는 3 c 3 d 3 이것이 stream의 length다.
++1st moment = count of the number of elements++

2nd moment는 surprise number라고 하는데 k가 제곱이 된다. a가 나타나는 횟수 ^2 b, c ^2 해서 더한다. ++더하면 distribute가 얼만큼 even한지 uneven 한지 그 분포도를 보여준다. 제곱값이!++

## Example: Surprise Number
표준편차의 값에서 표준편차의 값이 크다 라는것은 분포도 evenly하게 분포가 안되있다. 평균을 기준으로 그 관점에서 볼때
stream length = 100이다. 100개의 stream length 중에 distinct한 value가 11개가 들어있다. 11개가 반복된다. 첫번째 distinct한 value가 10번 나타났다. 나머지는 9번 나타났다. 가정
++이렇게 나타났을때 suprise number 2nd moment는 각각 출연빈도의 제곱의 합이다.++

++suprise number++
910 = 거의 빈도수가 even하게 나타난다.
8110 = 굉장히 uneven 하게 분포가 되어있다.

표준편차처럼 근사값을 추정하는 역할을 하게 된다.

## AMS Method
알고리즘이다. 모든 moment에서 작동 잘 된다.
2nd moment에 대해서 ams 를 가져다 맞는지 안맞는지 본다. 어떤 변수 x를 두고 x를 변수를 계속 keep track 한다. 변수는 어떤 2개의 element 값을 갖는데 그 변수당 x.el(element 값 자체와) 그 변수의 x.val(값, count 몇번 나타나는지) 
어떤 stream이 들어왔을때 어떤 한지점을 찍어서 너가 x야 라고 찍으면 x가 a, b, c 일수도 있는데 a라면 x.val = a 가 된다. x.val는 그 시점에 (x라고 찍은 시점)서 부터 끝까지(현재까지) a가 몇번 나타나는지 count 하라는것이다. 

variable마다 x라는 variable마다 element value와 count 값을 가져다 계속 keep track하고 update 한다. 그래서 값을 찾아낸다.

++ams 알고리즘의 목적
2nd moment 값을 계산하고 싶은데 2nd moment 값을 계산하려다 보니 정답을 찾으려면 모든 stream을 다 뒤져서 count를 다찾고 2해서 다 더해야한다. 현실적으로 어렵다. 우리가 하고싶은것은 우리는 조그만 노력을 들여서 값을 찾고 싶은게 목표다.++

찾고싶은건 second moment 값을 찾고싶어!
모든것을 다 뒤집고 싶진 않다. 적은 수의 variable과 memory와 computation만 써서 값을 추정해 보는것이다. =>ams method


## One Random Variable(X)
어떻게 작동하느냐???
ams method 로 second moment를 찾고싶다.
정답을 찾을 수 있으면 좋지만, 근사값을 찾는다. (변수 몇개만 써서)

변수 x라는것을 두고 x는 몇개를 둔다.
변수가 각각 item value다 . element 자체를 저장한다. 

length가 n이라고 가정하자. n이 굉장히 길다.
n이라는것은 window 내에서 시간을 뽑는다. 
t라는 시간을 뽑았다고 치자. sampling 한다고 생각. 
그리고 거기서 몇개를 찝을지는 app에 따라 다르다.
하나는 찝는다 생각. n개라는 수많은 stream속에 하나를 찝는다. 그 타임을 t라고 한다. 
t라는 순간에 있는 item이 a라고 가정
그때 있던 x.elemnt는 a가 된다. i를 a라고 가정
그다음부터 count를 하기 시작하는데 찍은 그 t부터 
n끝까지 봐가면서 a가 몇번 나타나는지를 센다. 다른건 무시한다.

그 값이 x.value가 된다. 그렇게 했을때 우리가 찾는 원래 값이 시그마 m의 제곱의 값인데 이것의 근사치를 가져다 구할려고 해봤더니 n*(2*c-1) 이다. 이렇게 해주면 이 근사치 값이 나온다.
(원래라면 전체 stream을 다보고 전체 모든 value를 가져다 메모리 올려서 세고, 제곱해서 다 더해야하는데 이것은 x라는 변수 몇개만 던져주고 얘네들 두개변수(n과 c만 유지)만 유지해서 계속 update해서 그 값들만 유지해서 어느 한순간 query가 들어왔을때 순간에 second moment 값이 어떤지 이 공식에 넣어서 return 하면 된다.

굉장히 빠르다. 다 보지 않아도 된다. 
x라는것은 하나를 두지 않는다. x를 여러개 둔다. 정확도 때문이다. 샘플링을 하게 되는 어떤 지점(변수 각각의 지점마다) 변수 x1, x2, x3를 두게 되는데 상식적으로 이 변수개수가 많을수록 정확도는 높아진다.(evenly하게??)

이 xk 까지 있다고 가정하자. n이라는 window 속에서 k 개를 시간을 찍어서 그중에서 각각 sampling을 할것이다. 
그 위치에 존재하는 그 값만 update 한다. 값이 구해지면 각 x당 하나의 값이 나온다. c라는것은 각각 x1, x2, ... xk마다 하나의 값이 나온다. n은 정해져있다(전체 window size) c값이 x마다 달라진다. 그렇게 했을때 x가 k개다. 

k개 변수를 뒀으면, 각각 k개 값을 더해서 j=1~k까지니까 *1/k 을 한다. 평균을 낸값이다. 
이것이 final estimate 값이다.
간단하다.





## Example
a,b,c,b,d,a,c,d,a,b,d,c,a,a,b 가 우리가 보고 있는 stream이라고 가정하자. 
stream이 있다고 가정할때 15라는것이 n이다(window size)
그리고 seconde moment 값을 구해보면, a가 5번 나타나고 b가 4번 c가 3번 d가 3번 나타난다. 
정답인 seconde moment 값은 각각 값의 제곱의 합이다. 
정답은 59다. 근사치가 꽤나 맞는지 볼려고 한다.
##### ams 알고리즘을써서 진짜 맞는지 확인해본다.
15개의 stream에서 그냥 random으로 아무데나 찍는다. n 이라는 범위 내에서 3개 찍는다 아무거나.
3번째 8번째 13번째 찍었다.
각 x당 element랑 count value가 2개가 유지가 된다. 
initializae 단계에서는 값을 계속 update 할꺼기 때문에 초기값이다. c는 한번 나타났으므로 1이다. 끝까지 c가 얼마나 나타날지는 아직 모른다. (시작단계)





## Example
3에서 x1 value를 찍고 초기화 value를 알고있다.
t가 현재 3이기 때문에 4번쨰로 간다. 
c랑 상관이 없으므로 skip한다.(b-4, d-5)
a도 무시한다. 7번째 도착했을때 c가 다시 나타난다. 이 x1.val=2로 증가시킨다(두번 나타났으니까) x1의 할일을 끝이다.
++변수는 현재에서부터 다음 변수까지만 보고 걔에 변수는 할일이 끝난다++
d는 x2다. x2.val = 1이고 element 는 d이다.
a전까지 진행한다. 8번째가 d이기 때문에 그 count값을 하나 증가시킨다.
++c로 온다. 아까 c를 봤다. 이때는 c를 무시하지 않고 아까 c를 countg나 x1.val을 2에서 3이 된다.++
13번째 자리로 와서 a니까 그렇기 떄문에 x3.val 하나더 count증가 시킨다. 2가 된다. 맨마지막에 15번째 b는 아무것도 없으니까 끝난다. update 값을 끝난다.

이 값을 근사치를 계산해야 한다. secondmonet 보면 n(2*c-1) 뒷장에서 나온다. 

x1에 대해서 75가 나오고 x2에 대해서 45가 나오고 x3에 대해서 45가 나온다.

value가 count 값

결과적으로 평균 55랑 계산값 59랑 거의 비슷하다.

## Expectation Analysis
왜그럴지, stream이 들어온다. 
2nd moment의 정답, 각각의 원소가 나타난 횟수에 제곱값을 다 더한것이다. 이게 알고있는 정답, 시그마 m의 제곱

기대값은 확률 통계에서 평균값이라고 말을 많이 한다. 평균값은 실질적으로 값이 주어지고, 그 값에 중앙값 이다. 기대값은 평균값을 맞는데 그 value에다가 확률을 곱해서 더해주는것이다.

평균의 기대치값이다. 정확한 평균값이 아니라 확률이 곱해져서 나올 수 있는 value에 이것을 각각 다 더해주는것이 기대값이다.

잘 생각해보면 평균값에 대한 에상치, 그게 기대값이다. 기대값을 쓸수밖에 없는 이유가 근사치화된 값을 구하는거기 때문이다. 평균처럼 보일뿐이다.
1/n 에 시그마 n(2c-1) 
ct는 어느 한순간 값부터 끝까지 몇번 나타나는지 의미
c1 = t=1의 item 부터 끝까지 몇번 나타나느냐(a가) c1=a 전체 stream에서 나타나는 횟수
c1=ma랑같다.
c2=a, 2에서부터 끝까지 갔을때 a가 몇번 나타나냐
ma-1...
c3=b, 3에서부터 끝까지 몇번 나타나냐 
mb(전체나타난횟수)

기대값을 구해본다. 2c-1 이 뭐가 되느냐 찾을려한다. n은 없어진다. 2c-1만 보게 된다. 다 더했을때 값에 합이 뭐일거 같냐?

a만 본다고 생각하자.
t=1~n
a입장에서 바라보면 c1=ma, c2=ma-1 가게되는데
4번 갔다고 생각하자.
##### else
stream 전체라고 생각 a가 4라고 생각하자.
첫번째 c1의 값은 4가 된다. c2의 값은 3이 된다.
a만 본다. 4, 3, 2, 1 이 된다. 그대로 넘어간다.
1.3.5.7.9...
mi까지..
a면 2ma까지 가는것이고 b면 2mb까지
모든 i에 대해서 니까 i는 set, A에 들어있는 unique한 값들(set A에 들어있는 각각의 값들에 대해서 모두 더해준다.)


## Expectation Analysis
mi{2+(mi-1)2}/2 =(mi)^2
등차가 2인 수열의 합 공식.
Expectation value가 우리가 생각했던것과 맞아진다.
증명이되고, 공식이 second moment 구하는공식이 맞다. 가 된다.




## High-Order Moments
second moment도 좋지만, second moment 쓰기 위함 ams 자체는 모든 차수에 대해서 다 적용된다.
k=2 n(2c-1) 
k=3 n(3c^2-3c+1)로 공식이 바뀐다.
k=4일때도 나온다.
규칙도 없어보이는데, 규칙은 찾는데 라는 form을 보여준다.
n=(c^k-(c-1)^k) 이다.
k에 2를 넣어보면 n(2c-1)이 나온다.
나머지도 적용하면 나온다.
k가 2대신 3이 나온다.

## Combining Samples
문제는 없는가? 결과적으로 x라는 value 값을 update 해가면서 근사치 값을 찾는게 목적이었는데, x는 몇개까지 만들어야 하는가? 
메모리 사이즈 만큼 유지한다. 많이 만들수록 좋다.
그러나 불가하면 메모리가 허용하는한 많이 만든다.

stream은 계속 들어온다. n이 100들어와도 계속 들어와서 바뀐다. n이 100이라는 윈도우에서 10/50/70 샘플링해서 찍어도, 계속 와서 늘어나면 의미가 없다. 이런 문제가 생긴다. 정확도가 시간이 지나면서 떨어진다.
## Streams Never End:Fixups
정확도 값이 말도 안되게 떨어지지 않는데, 설사 이 x라는 변수가 시간이 흐름에 따라 옛날에 형태가 되더라도 알고리즘이 지나가는 순간 그 값을 update 한다. 최신값을 유지한다. 
기존공식을 그대로 쓰고 n값을 변하니까 (계속 들어오니까) 2000, 3000, 5000 되면 n값이 변하니까 나중에 곱해주면 되지 않나, 실제로 그렇다.

이렇게 하면 정확도가 떨어짐.

1)새로들어온 n값 사이즈 를 곱해준다.
2)sampling 방식을 이용해서 최신 x로 update를 하자. fixed size sampling 데이터 사이즈가 증가하지 않는다. fix가 되어있으니까 직관적으로 알고리즘을 알고있는데,
코딩을 한다 생각하면 value의 개수 x1~xk 유지한다고 생각 k개의 x 변수 있다 가정하면 따로 sampling 귀찬으니까 (찍어서 하지말고) 하지말고 어차피 그 위치는 sampling하면서 update 될것이기 때문에 처음부터 1,2,3,4 로 ++첫번째 element를 x1에 할당하고 두번쨰를 x2를 할당하고 순서대로 k까지 순서대로 할당한다.++ 초기단계에 만들어 놓고
그다음부터 들어오는 element에 한해서 sampling을 한다. ++어떤 비율로 k/n 확률로 굉장히 낮을 확률 (n은 윈도우, x가 k) 그 다음 element가 들어오면 k/n 확률로 샘플링 맞으면, k개중에 아무나 한놈을 랜덤하게 뽑아서 날려보내고 k/n개로 온것으로 대체해버린다.++
예전에 old값이 시간이 지나면서 최신값으로 x가 업데이트 된다.

sampling 하기때문에 최신값으로 업덷이트 된다.
x가 k개라는 메모리 용량에 제약을 해치지 않는다.
확률 자체가 k/n 확률로 뽑기 때문에 x값이 저 확률에 맞아 주는한 k개의 값이 유지되면서 최신값으로 update 된다. 아니면 그대로 간다.

데이터가 계속들어오는것을 해결할 수 있다.

# Finding frequent element
## Counting Itemsets
한 bit를 찾는다.
자주 나오는 놈을 찾는다. 어떻게 찾을까?
stream이 들어왔을때 counting 개념에서 바라봤는데 여기서는 이중에서 제일 자주 나타나는 것이 무엇인가? 이중에서 30%이상 굉장히 hot data는 무엇인지? app으로 오면, 영화로 예를 들수있다.

가장 인기 있는 영화가 뭐냐, 탑10 영화를 찾고싶다. frequent한 element를 찾는다.
a라는 영화가 팔렸던 순간이 1이라고 가정한다.
b라는 영화가 팔렸다면 그 순간 tranjection은 1이다. c도 마찬가지. 이런 stream이 들어왔을때
그중에서 이 데이터를 보고 탑 10 영화를 찾을 수 있냐? 그것을 찾아주는 algorithm이 된다.

new problem : stream이 있고, 어떤 window 내에서 특정한 s1보다 더 많이 나타나는 element를 찾아보라! 영화 50만개 이상 찾아보자. 그럼 굉장히 핫한 영화인것을 app측면에서 알 수 있다.

이것을 어떻게 풀까? 이것은 DGIM method를 사용한다. 
간단하지 않다. 각각 영화당 stream data를 다 뽑아낸다. 1010100010 이런식일것이다. a라는 영화를 다 세고 전체 stream에서 b라는 영화에 stream만 뽑아낸다. 세고, c라는 영화도 이런식으로 센다.

하지만, 영화의 개수가 많으면 셀수없다.
그러면 item당 stream을 하나씩 붙여야 됟나.(stream에서 해당 영화의 정보를 뽑는데, 영화의 개수가 많을수록 반복된 작업이 엄청나게 증가하므로)

그런 overhead가 생긴다. 각 item당 stream을 붙이고 1의 개수를 세면 되는데,단점은 굉장히 많다.
많으면 적당한 방법이 아니다. 그래서 item당 stream으로 단위를 주는것이 어렵다. 
41
DIGIM Method 자체도 근사치다. 에러율을 생각해보자 에러가 포함되있으므로 정확한것은 아니다.

a/ b 영화에서 a는 전에 1개 현재 999개가 팔리고 b영화는 전에 999개 현재 1개 팔렸다. total은 똑같다. 2개중에 인기있는 영화는 a이다. ++hot하다는 element의 정의는 단순히 count만 고려하지 않는다.++

## Extensions
얼마만큼 recent는 얼만큼 최근경향을 반영하는지, 그것이 recent다. DIGIM method를 이용하면 count 만 고려하는 frequency만 생각하지 recent는 고려하지 않는다. 최근에 인기있는 영화에서 digim은 잡아낼 수 있는 방법이 없다. 
## Exponentially Decaying Windows
위의 방법을 처리하기 위한것으로 Exponential decay가 고려되었다. 
그림부터 보면, sliding window 에서 recent 값은 똑같이 1이다. 그런데 exponential 하게 decay하는 경우에는 최근에 일어난 값에 weight를 더준다
기존의 sliding window는 똑같이 count만센다. 현재껏, 옛날껏 weight를 주지 않는다.

높이가 weight. 
exponentially decaying window 최근은 더 많은 weight를 주고 시간이 지남에 따를수록 똑같은 하나의 값이어도 값을 적게 준다.
recent를 반영하게 된다.이것은 과거와 현재를 고려해 낼수있다.(팔린 표에 대해서)


## Example:Counting Items
47 
앞에서 했을때는 frequency뿐만 아니라 recent도 반영해야 한다. 그래서 에전에 window의 n에 있는 경우는 count만 하므로 frequency만 반영하는거니까 우리가 원하는것을 찾을 수 없으니까 조금더 smooth하게 아까 특성 방정식을 써 가지고 weight 를 줘서 rececy도 반영한 그런 count값을 return하게 된다.

stream에 a1, a2.. 이렇게 들어온다고 가정해보자 그 특정방정식은 어떻게 구성되냐면 1이라고 생각해도 좋다,(티켓으로 따지면 팔렸다 안팔렸다 구분) 이자체가 0이나 1이라고 생각하면되고 이런 stream이 들어왔을때 그 값에다가 뭔가를 곱해줘야 decay 그래프(그림)잉 나올것이다. weight값 (1-c)를 줘야 한다. 곱해줘야한다. 

c는 통상적으로 작은값을 잡는다. window size 만큼 줄어든다. billion 분에 1정도..
1에서 위 값을 빼면 1보다 작으므로 거기에 지수승을 계속 한다. 그렇기 때문에 그래프가 아래처럼 보일 수밖에 없다.

1보다 작은값을 곱해주니까..

첫번째부터 현재(t)까지 이 범위에서 각각 들어오는 element값에 해당되는 weight값을 곱한다.  이것을 다 더해지는데 결과적으로 total 값을 정해야 하니까 어떤 순간에 더한값이 존재한다고 생각하자. 어느 한순간에 또 다른 데이터가 들어오면, 그 이전에 합친결과가 바껴야 한다. 처음에 위치해 있던 곳에서부터 한칸씩 shift 된다. 기존의 weight가 한칸씩 밀려나간다. 지금까지 구했던 합친결과가 다 업데이트 되야 한다. 다시 구해서 weight를 곱해서 다 더하는 반복적인 행위를 안해줘도 된다.

++왜냐하면, 들어온만큼만 밀리기 때문이다. total값은 새로운 값이 오는순간 기존의 합 에서 weight값이 1-c만 곱해주면 shift된 전체 합의 값과 같다. 복잡하게 다시 계산 안해도 된다. 따라서 computation이 굉장히 낮다. 그리고 현재 들어온 값을 더해줘야 한다.++
통상적으로 1 , 1을 더해줘야 한다.
더해주고 기존에 있던 값을 1-c 해서 곱해주고 +현재값을 해주면 지금 현재 합의 결과가 나온다.(중요)


## Sliding vs Decaying Windows
