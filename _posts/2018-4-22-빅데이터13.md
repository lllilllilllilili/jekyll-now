---
post : layout
title : 빅데이터13
---
## 빅데이터13
빅데이터13

## 복습
frequent한 element를 갔다가 찾는것이다.
counting이라기 보다는 데이터 stream이 들어왔을때 그중에서 제일 핫한 데이터를 그런식으로 가장 frequent하게 나오는놈을 가져다 찾고싶은것이다. 

우리가 뭘한다고 했냐면 가능한것이 각각 하나의 data stream이라고 생각하고 각각 자기의 이아템에 해당되는 bit stream을 별도에 stream이라고 생각하고 dgim을 돌려서 count를 세면 된다고 했다. 라이브한 방법이다.
그렇게 하게 되면 메모리를 많이 잡아 먹으니까 그렇게 하지말고 여러가지 문제가 있는데
지금 frequent한것 지금 핫한거 그런 뭔가를 찾는데 그 방법을 쓰게 되면 제약점이 생긴다. 그런 recency를 반영하지 못하고 너무 count만 세게 되는 문제가 생기다 보니까 그것을 반영을 해서 윈도우 를 더욱더 개선을 한 exponentially dacaying window를 쓴다고 한다.

이런식으로 exponential하게 decay하다. 이렇기때문에 이런 윈도우를 쓴다. exponentially decaying window다.

이렇게쓰면 최근에 들어온놈일수록 weight를 높게 주고 오래된 데이터일수록 weight를 적게 주기때문에 자연스럽게 recency를 반영하게 된다. 그 count가 똑같더라도 서로 값이 달라지는것이다. 차별화두면서 recency를 주게 된다.
그때 주게되는 공식이 요것이다. (1-c)이거였나..
공식이 그때 들어오는 데이터가 ai다. 아니면 010101 이라고 생각해도 좋다. 그렇게 들어오는 데이터가 있을때 그 데이터 값을 갔다가 weight랑 곱한다. (1-c)^t-i승 해당되는 weight를 준다. 

이것을 자세히 보면 윈도우가 1에서부터 현재 값인 t까지 현재가 t다. t에다 t-i를 넣어버리면 t-t는 0이 되어 버리니까 이값이 1이 되버린다. 지금 현재 들어온놈한테 가장 높은 weight를 주겠다 라는 의미다. weight는 1을 넘지 않는다. weight는 1에서부터 내려간다. 1-c는 c는 통상적으로 million분에 1이라든지 굉장히 작은값이다.

이 값을 뺀것은 1보다 작습니다. 그것에 지수승이면 값은 무조건 줄어든다.

그래서 그런식으로 weight를 준다. 

ai가 우리가 현재 보고 있는 아이템이다. 각각 데이터의 스트림이다 라고 가정하자. 이거를 가져다 일반화된것이고 보면 들어오는 ai값 자체가 뭐가 되느냐에 따라 달라지는데 델타 i가 우리가 영화티켓을 판다라든지 바이너리 데이터가 0101010 로 들어온다. 가정하면, ++이 델타 값이 내가 찾고 자 하는 데이터다. 라고하면 1이 되는것이고 내가 찾는데이터가 아니다 라고 하면 0으로 처리한다. ++

그래서 상식적으로 어떤 영화에 인기도를 측정하고 싶으면 티켓에 팔리는것을 0101010 주고..
a라는 영화가 요기 요기 요기 팔렸다 그러면 해당위치가 1이되고 나머지는 0으로 처리해서 날려버리면 자기가 원하는 영화의 데이터만을 찾을 수 있다. 그래서 그런식으로 데이터를 더해주는데 결과적으로 들어오는 weight 값인 곱해지는 값을 가지고 더한다. 계속 더하게 되는데 어느 한순간에 새로운 데이터가 들어왔다고 가정하면 제일 먼저 지금 있는 모든 카운터값 각각 모든 카운터값에다가 1-c를 곱해준다. 왜냐하면 딱 하나가 들어오는 순간 지금 우리가 그때까지 봤던 모든 데이터는 한칸씩 과거가 된다.

a라는 b라는 c라는 무비가 각각에 대해서 다 카운터가 
그런 모든값이 사실은 따지고보면 지금 하나들어오는값때문에 move shift된다.


각각 무비로 예를들면 sumation 있는 값을 각각 1-c를 곱해줘버리면 된다. 원래대로라면 count를 다시 계산해야 된다. 근데 weight를 줘버리면 그럴필요없다. 그냥 어차피 한칸씩 1-c만큼 그 weight만큼씩 다 올드한 값이 되기때문이다. 공평하게 다 값이 한꺼번에 shift되서 각각 summation에 대해서 1-c를 만 곱해주고 그리고 만약에 새로들어온 element가 왔는데 얘가 a라는 무비꺼야 a라는 무비에 카운트값에 +1해준다. 

그리고 1-c라는것이 weight function이다. weight 값이다. 
1보다는 작다. 

## Sliding vs Decaying Windows
우리가 그냥 기본적으로 만든 sliding window는 recency느느 고려하지 않는다. frequency만 고려한다. count만 세고 만다. 걔네들은 똑같이 1표

다시 얘기하면 현재라고 가정하면 현재도 weight 1이면 과거도 똑같이 weight1이다. sliding window는 weight 가 같다. weight가 1이라고 가정하면 된다.

이게 exponentially decaying window에서는 첫번째 만 1이지 이후에는 1-c로 내려간다. 사실은 현실적으로 무한대까지 간다. 첫번째 데이터에서 계속들어온다. 어느한순간 데이터를 바라보면 첫번째부터긴 하지만 현재값을 계속 지나간다. 현재입장에서 바라보면 거의 무한대로 limit으로 가버리는것과 똑같다.

이게 무조건 1-c 가 인위적인 값이 아니라, 어떤 속성을 만족해야 한다. 1-c라는 weight는 현재 지금 우리가 있는 sliding window 있는데 그냥 우리가 weight 없다라고 sliding window에 면적값 이랑 우리가 exponential decay하게 요 데이터들, 요 밑에있는 면적이 같아야 한다.

그값이 달라지면 안된다. 그래서 모든 속성을 보면 모든 weight에 대한 sum 이 
c는 million분의 1 이든지 굉장히 작은값이다. 0.00001 낮은값이다. 근데 그거 분에 1이다. 
++
결과적으로 여기는 window 사이즈가 되는데 우리가 c값을 100만분의 1로적었다 얘는 여기가 100만이 된다. 시그마(1-c)^t = 1/c인데 여기는 100만인된다. 
++

1/c라는게 결과적으로 우리가 지금까지 봤던 전형적인 윈도우 사이즈이다. 어쨋든 exponential하게 decaying 면적은 다더하게 되면 이 값이랑 같다.

요 면적이 높이가 1이다. 사실은 면적의 값이면 윈도우사이즈가 100만이면 100만 곱하기 1일것이다. 

그러면 요값도 1-c 의 t승씩 줄어드는 이그래프도 에 면적을 다더해보면 처음부터 현재 t까지 다 더해보면 1/c이다. 윈도우사이즈 그런식의 고값이 나와줘야 한다.

실질적으로 이계산을 해보면 그 값이 나온다.
1-c 의 t승.

등비수열, 수열의 합 공식을 구하면 알수있다. 등비수열의 공식을 그대로 적용시켰다. 
공식값이 이렇게 나온다.
정리를 하게되면 1/c가 그대로 나온다. t를 가져다 사실상 무한대로 보는것과 마찬가지다. t를 가져다 1에서부터 무한대까지 보내버리게 되면 1-c의 t승 얘가 0이 되버린다. 그래서 실질적으로 1-c의 ... 1/c가 나온다. 

이거는 수학적으로 어떻다 라기보단 만족시켜야하는 속성이다. 왜냐하면 이 weight를 맘대로 주게되면 이 기존의 슬라이딩 윈도우는 형평성이 없다. 그래서 기존에 있는 기존에 있던 슬라이딩 윈도우 랑 똑같이 비교를 할 수 있어야 하기 때문에 그리고 고만큼의 해당되는 weight을 알아서 잘 쪼개서 주는게 목적이다. exponentially decaying window의 목적이다.

그래서 이면적이랑 요면적은 서로 같다.
그걸만족시키는게 ++1-c의 t승이다++
저 weight가 저렇게 나오는것이다.

그래서 현재 제일 인기있는 영화가 뭐냐 라고 질문이 들어올수있는데 어떤 티켓 판매 회사에 정보를 보낸다고 생각해보자.
각각에 대해서 0101010 에 대해서 티켓판매가 됬으면 1 판매가 안됬으면 0 이런식으로 바이너리가 쭉 들어온다고 생각해보자 그러면 이데이터에서는 a라는 무비는 요기요기요기 b라는 무비는 요기요기요기 각각에 대해서 데이터가 위치가 달라질것이다. 그러면, a라는 무비는 자기 위치에 해당되는 개네들을 가져다 exponential decaying window를 써가지고 sumation 값을 유지를 하고 c란느 영화는 똑같은 이치로 자기 하나 위치에 해당되는 1의 비트 스트림만 뽑아서 exponential decaying window를 그대로 적용시켜서 sumation을 그래도 유지한다.

## Example:Counting Items
뭐가 인기 있느냐? 현재 가장 인기 있는것이 뭐냐?? 인기도를 어떻게 측정할 것이냐?? decaying 하게 되는 sum 
그 weight를 준 summation을 가져다 다 더한 그 값을 인기도라고 가정하자.

threshold를 주게 되는게 다른게 아니라, 1/2를 주든 뭐를 주든 상관없다. 우리들 마음이다. 예를들어들이는것이다.

예를들어서 200개의 무비가 있는데 그중에서 5개 가장 파퓰러한 무비를 찾아 라고 했는데 200개 데이터를 뭐하러 유지하냐 가능성없는애는 버리자. 유지하지말고 그렇다고해서 1/2을 준것이다. 딱 1/2은 아니다. threshold를 줘서 summation을 주는데 그 값이 절반도 안되는 놈들은 아예 별로 탑 5에 들어갈 자격이 없으니 그냥 알아서 다 지워버릴거야! 유지안할거야 그게 threshold 이다. 그래서 만약에 무비가 딱 들어왔을때 무비별로 스코어를 다 서메이션을 해가지고 갖고잇는데 그 값이 예를들어서 절반값도 안된다. 그러면, 미리 그냥 버려버리겠다 그말이다. 구지 그걸 유지 안하고 어떤 무비가 들어오게 되면 알고리즘이 그렇게 될것이다. 

무비가(a,b,c,d,e) 별로 영화별로 sumation 값을 유지한다. 각자가 다 sumation을 가지고 있다. 어느 한순간에 데이터가 들어오면 체크를 해서 나머지 들어온순간 나머지 모든 200개 데이터에 대한 서메이션 각각에대한 모두 1-c만큼 모두 곱해준다. 알아서 weight값 줄것이다. 
처음에 현재 들어온 데이터가 무비가 m에 해당되는 걔네에 값에다가 무비 m에 해당되는 값에다가 +1을 해준다. 나머지는 -1을 곱했다.

그게 아니라 데이터가 쌩뚱맞은 새거다. 이영화는 나온적이 없어 새영화다. 기존에 유지하고 있는 데이터가 없으니까 initialize 한다. 새로운 데이터를 하나 만들고 걔에 대해 +1을 해주면된다.

summation을 해서 모든 200개의 데이터에 대한 값을 가지고 있는데 1-c를곱해서 어떤 1/2값으로 떨어지더라 그러면 그 아래값보다 낮아지더라그러면 날려버린다.

이런식으로 해서 popularity를 측정한다.
단순히, 하나의 예제이다. 실질적으로 빅데이터에서는 popularity 이기 이전에 핫 데이터 라는 표현을 많이 쓴다.
핫하냐 라고 물어보는것이다. 핫 하다라는것은 이거를 적용할 수 있는데가 많다. 

만약에 컴퓨터를 캐시를 디자인할것이다. 무슨 데이터를 가져다 캐시를 할것이냐 똑같은 개념이다. 이 예제가 똑같이 적용된다. 데이터가 들어왔는데 메모리에 캐시를 한다. 컴퓨터속도 빨라지는 이유. 데이터가 들어왔는데 캐시에 넣을지 말지 결정해야 한다. 빠른순간에 이 데이터를 넣을지 말지 어떻게 결정할것이냐 이게 핫하다 생각하면 넣고 콜드하면 버린다. 그대로 적용한다. frequent 한 건지 판단하는 그런 알고리즘 이지만 이거 역시도 현재 top10 이니 얘기했을 뿐이지만 캐시면 가장 핫한놈 한명을 뽑는다.

아니면 버린다.

## Clustering
클러스터 링. 그륩핑하는것이다. 여러데이터가 있을때 유사한 놈들끼리 그륩핑하는것도 데이터에서 많이 쓰인다.
## High Dimensional Data
데이터를 갔다가 보이게 하는것처럼 보이지만 사람입장에서 
하지만 보이게 하지 않는것도 많다.

데이터들이 이렇게 주어졌을때 보시면 데이터 특성들을 잘 분석을 해서 걔네들을 이해를 하기 위해서는 가장 실질적으로 많이 쓰는방법이 클러스터링 이기도 하다.


## Example:Clusters & Qutliers
Set of points 주어졌는데 이것은 굉장히 많은 포인터가 주어진다. 그리고 나서 포인터들을 가져다 그륩핑하게 되는데 k개면 k 몇개의 그륩으로 그륩핑을 할것이다. 그게 클러스터링이다. 어떤 데이터를 가져다 데이터 포인터들이 잡혀있을때 걔네들을 그륩핑할때 어떤 속성이 있을때 통상적으로 어떤 개념을 많이 쓰게 되냐면? 거리란 개념을 많이쓴다.
왜냐하면, 정의이기때문에 
그륩을 묶을때는 가장 쉬운것이 distance다. 거리가 가까운 노드끼리 클러스터링 하면 어느정도 속성이 맞아진다.

distance가 가까운 놈들끼리 묶어준다. 그 distance 가 생각보다 간단하지 않다. 

## The Problem of Clustering
그렇게 해서 클러스트를 갔다가 지냬들끼리는 비슷한 속성을 가지고 있을것이고 
다른 클러스터끼리들끼리는 그 멤버들끼리는 어느정도 dissimilar할것이다.
다른 클러스터

그러기는한데 통상적으로 dimension이 굉장히 높다.
그게 유클리드 앤 스페이스, 통상적으로 빅데이터에서 나오는 데이터에 그륩핑 클러스팅 하는경우는 디멘젼이 높다. 

벡터가 있을때 벡터가 원소가 한두개 아니다. 벡터 디멘전 얼마든지 만들수있다. 데이터들이 굉장히 high-dimensional하다. 그리고 유사도, 유사한놈들끼리 묶게 되는데 유사한 놈들을 묶기 위한 distance 개념을 쓴다. distance가 유클리드 공간에서 distance , cos distance jaccard distance라든지...쓰게된다.

어떤 distance 쓰는지는 app에 따라 다르다.
다양한 distance를 다룬다. 그런 distance가 비슷한것끼리 묶는다. 

## Exmaple : Clusters & Qutliers
클러스터가 이렇게 나온다.
눈으로 보면 유치하다. 그렇게 클러스터링 했을때 삐져나온것을 outlier라고 한다.

데이터는 눈에 보이는게 아니다. 쉽다고 착각하는것이다.

## Clusering is a hard problem!
클러스터링은 우리가 흔히 아는 x축 y축 스페이스 투 디멘전은 간단하다. 쉽게 그륩핑 할 수 있다. 데이터가 작다. 몇개 안된다. 그륩핑하는게 쉽다. 그리고 대부분 케이스 경우는 
눈으로 보는게 그닥 틀린것이 아니다. 보는게 그대로 생각하는게 틀리지 않다. 단 데이터가 작은경우에
## Why is it hard?
실제 x축으 weight y축이 height가 되는데
개들의 데이터다.
분석을 해서 데이터를 포인터 찍어봤더니 이렇게 나오더라
이그륩의 들어가는 치와와, .. 개 분류를 쉽게 한다.
그런데, 강아지 처럼 
현실적으로는 시궁창이네
대부분 dimension 은 굉장히 크다. 벡터를 많이 쓰기도 하고 set도 많이 쓴다. 실질적으로 굉장히 크다. 
데이터 사이언스에서 분석해야 하는 데이터는 크다. 
이 데이터 디멘전이 높아지면 그놈이 같고 그놈이 같다.
분류가 안된다.
유클리드 디스턴스말하는데 그게 3차로 넘어가면 우리가 인지가 안된다.

디멘전이 100,000이면 분석불가
## Clustering Problem:Galaxies
예제를 살펴보면, 갤럭시를 갔다가 분석을 한다.
비슷한 놈들끼리 그륩핑해봐 데이터가 나온다.
2억개의 하늘의 sky object들을 분석하고 싶다.
그것을 7개의 디멘전으로 나눠서 분석을 하는데 어떤 거는 갤럭시고 붙어있는 별일 수도있고 어떤놈은 이도 저도 아닌 놈일수도있고...
이런 다른 뭔가들이 있는데 한꺼번에 사진을 찍어놓면 그놈이 그놈같고 저놈이 저놈같다.

이것을 그륩핑하는것은 어렵다.

현실적으로 

## Clustering Problem: Music CDs
cd같은 경우, cd판매하는경우 cd를 그륩핑좀 해봐 
음악의 장르가 아니라, 아마존이나 판매자 입장에서 바라보면 굉장히 많은 cd를 분석하게 되는데 카테고리 하면 분류기준을 뭘로 잡을것인가? 그륩핑을 할때 기준잡는것도 어렵다. 분류하기 학문조차도 존재할정도로 어렵다.

어떤식으로 그륩핑을 하고 클러스터링만들면 결과적으로 분류를 해야한다. 분류가 쉽지 않다. 

요런 cd산 사람들은 요런 특징이 있더라, 그러면 customer입장에서 생각해보자 그런 속성들을 가진놈들은 요런 cd를 많이사고 반대로 요런 속성의 cd를 많이 산 사람들은 요런 사람이더라! 라는 관점에서 celler라고 생각하고 여러분들이 그런식으로 분리한다 생각해보자.

space를 갔다가 이런 사람 저런 사람 k명의 사람이 있다고 가정해보자. 어떤 a라는 cd를 산 사람이 보면 그게 2번째 사람이 그 cd를 사고 k번째 사람이 그 cd를 삿고 이런식으로 시스로 표현하다고 생각하게 되면 요런 데이터들을 구축할 수 있다고 가정하면 그 cd를 산 사람들을 갔다가 분석을 각 dimension 별로 분석을 할 수 있을것이다. 원소별로 따져서 몇번째 몇번째 있는 놈들이 인 사람들은 요 cd를 많이 사더라 

그런식으로 분류를 할 수 있을것이다. 우리가 정하는것이다.
사람은 1,2 명인데 데이터가 빌리언까지 가버리면 대책이 없다. 관리 유지 불가 힘들다.

## Clustering Problem: Documents
웹 페이지, 어떤 다큐먼트를 던져주고 다큐먼트를 가져다 알아서 그륩핑을 해봐! 라고 하면 나오면 제일 직관적이고 쉬운방법은 실질적으로 다큐먼트에 나오는 워드, 키워드들을 뽑아낸다. 그리고 나서 100개 , 10000개 키워드 정해놓고 키워드 나온 위치에다 1을 세팅한다. 첫번째 다큐먼트를 분석했더니 x2, ... xk 키워드 나왔다 b라는 다큐먼트를 분석해서 봤더니 어디어디 해서 나왔어 그럼 1이 세팅이 될것이다. 그러면 결과적으로 1의 포지션이 비슷한 놈들끼리 그륩핑하면 거의 대부분이 그런 다큐멘터들은 같은 키워드를 가지고 있기 때문에비슷한 종류의 document일 확률이 높다.

그런식으로 클러스터링을 할 수 있다. 이런식으로 하는것을 singling이라고 한다.

## Cosine, Jaccard, and Eucidean
다큐먼트 조금더 살펴보면
그런식으로 다큐먼트를 가져다 클러스터링 하게 되면
그런 키워드들의 set으로 많이 표현하게 되는데 
키워드의 set으로 표현하는것이 shingles 라고 한다.
이때 예를 들어서 결과적으로 다큐먼트의 디스턴스가 가까운 놈들끼리 묶는게 맞는거 같은데 이것 역시도 쉽지 않다. 다큐멘트가 벡터로 표현된다. 그러면 얘를 측정하기 위해서 cosine distance를 써야 한다.

그리고 만약에 아까 콤마 콤마 셋으로 표현이 되더라 함은 jaccard distance를 쓰면된다.

point로 좌표로 우리가 알고있는 좌표로 표현이 되면 떙큐하게도 Euclidean distance를 쓰게 된다. 좌표상의 거리를 의미한다.

5가지 디스턴스가 나오는데 