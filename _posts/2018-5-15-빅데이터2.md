
---
post : layout
title : 빅데이터가2
---
## clustering
다 했음

## Overview: Methods of Clustering
Hierarchical
계층을 가지고 클러스터링 방식

Point assignment
계층을 무시하고 포인터 순서대로 점 하나하나 제일 가까운 클러스터링에다 할당 하는 방식

가장 가깝다. 
디스턴스라는건 여러 개념이 있음
클러스터링 distance 개념으로 접근함
어떤 distance 쓰느냐는 app에 따라감
가장 가까운 distance에다 점을 할당(포인트 어사인먼트방식)

## Hierarchical Clustering
초기상태는 점 자기자신이 클러스터임
그리고 나서 클러스터끼리 묶어서 올라감
어느 순간 클러스터를 k개 까지만 한다. app에 따라 기준이 있음
어느정도 선까지(여기서는 k개까지 스탑한다 기준이 있음)

여러 문제점이 있음
생각하는 이슈가 있음
클러스터를 표현할때, 클러스터 여러개 데이터 모여있음
그 맥락에서 센트로이드 개념이 나왔음, 유클리드 공간에서
평균이라는 존재(유클리드에서만 존재함)
유클리드 공간을 가정하지 않으면 평균이라는 개념이 없음(average)

유클리드 distance 표현할때 센트로이드 개념씀
어떤 클러스터에 모든 점들에 average 값 구해서 한 점을 이런 점이 클러스터 전체를 대표하는 반장이다. 그게 만약에 유클리드 디스턴스가 아니면 가정이 맞지 않많다.

두번째 가깝다 라는것은, distance 개념에서 가깝다 라는것은 가장 가까운 놈들끼리 클러스터링 해가는 놈들. 기준에 따라 다르다.

클러스터를 7개 가정한다. 클러스터 7개 만들었을때 스탑하면 됨(기준이 존재함)

## Hierarchical Clustering
가까운 얘들끼리 묶어서 클러스터 형성 올라감

(1) 포인터들 클러스터 구성함 많은 포인터들, 어떻게 표현할 수 있을까?
유클리드 스페이스는 센트로이드 쓴다. average 이다.
센트로이드 개념쓴다. 가깝다 멀다 distance 개념을 배웠다.
어떻게 hierarchical 한지도 말했다.

두개 묶고 클러스터 묶고 centroid 만들고 또 묶으면 centroid 바뀌고 또 묶고 	이런식으로함 다른 점이 있으면 걔네를 묶어서 센트로이드가 바뀌고 
센트로이드랑 가까워서 어사인됨 
센트로이드는 계속 이동함 (묶일수록 그렇겠지..)

두개를 한번에 얘기할 수 있는 그런 이론들임
## Efficiency
큐빅임
실질적으로 굉장히 큰 데이터에서는 hierarchical한 방법이 적용이힘듬
그런 알고리즘을 이용해서 굉장히 큰 데이터 셋에서는 클러스터를 할지 알아본다.

## And in the Non-Euclidean Case?
유클리드 공간을 가정하고 말했다.
유클리드 공간이 아니면 average 개념이 존재 하지 않는다. 센트로이드도 없다.
그런 클러스터 들을 대표하는 놈 어떻게 찾을지 고민해야 한다.
새로운 개념, 클러스터 로이드
유클리스트 개념이 아니면 클러스터 로이드 라는 개념이 들어온다.
평균이라는 개념이 없기 때문에 어떤 클러스터가 데이터 포인트 있으면 그중에 존재하는 어떤 한놈 잡음 니가 반장해 그렇게함
센트로이드는 실제 존재하는 데이터 포인트 아님, 실제 존재하는 데이터 포인트들의 평균으로 만들어진 가상의점임
유클리드 공간 아닌것에서 클러스터 로이드는 어떤 한놈을 짚어서 니가 반장해

어떤놈이 클러스터 노이드 반장 기준은 있다.


clustroid = (data)point "closest" to other points
클러스터 로이드는 어떤 데이터 포인트냐면 클러스터에 있는 어떤 한점인데 그 점이 다른 자기 클러스터 있는그 모든 데이터 과의 디스턴스 따졌을때 제일 가까운 그놈을 짚어서 니가 클러스터 로이드하셈 대표가 되셈
클러스터 로이드 바꿨을 뿐이지 어떤 클러스터와 어떤 클러스터 가깝다 멀다 개념자체는 센트로이드 대신에 쓰면됨
유클리드가 아닐때 도입한 개념

## "Closest" Point ? 
클러스터 노이드를 가져다 
어떤 한놈을 뽑긴 하는데 그 자기 클러스터 에 있는 다른 애들이랑 모든 제일 디스턴스 제일 가까운놈을 뽑음
어떤 놈을 가져다 클러스터 로이드를 뽑느냐? 기준도 있다. 
이 3가지 중에서 app에 따라서 되는놈 따라 쓰면 됨

어떤 클러스터가 있을때 한놈이 다른 포인트랑 디스턴스 있을때 그 맥시멈 디스턴스 제일 작게 하는놈 
어떤 한점을 잡아서 자기 클러스터에 있는 다른 놈이랑 디스턴스를 따지는데 긴것도 잇고 짧은것도 잇을 수 도있다. 제일 긴놈이 max distance 있을텐데 제일 작은놈을 클러스터 로이드 잡는방법임
(smallest maximum distance to other points)
뒤에 예제가 있는데 그게 시험임
아니면
다 한놈을 다해서 다른 포인터 디스턴스 다 구함, 다 더함 그럼 각각의 데이터 포인트 다 더하면 전체 디스턴스 각 포인트에 대한 썸이 다 나온다 그 썸이 제일 작은놈을 잡아서 니가 클러스터 로이드를 해라 이런 방법이 있고
아니면 
그 어떤 포인트가 있을때 다른 데이터 포인트 랑에 거리에 제곱값 을 가져다 다 더함
해가지고 그중에서 제일 작은놈 
거리에 제곱 자체가 = 디스트릭션 제일 적게 만든다.
이런 식으로 채택할 수 있다.
예제
Smallest maximum distance to other points
Smallest sum of distance to other points
Smallest sum of squares of distances to other points

## Example : clustroid
심플하다. 데이터 포인터 있을때
묶고싶음
예를들어서, 이 3개가 데이터 포인터 average 값 구하면 실제 존재하지 않은 점 = 센트로이드(유클리드 공간일때)
그게 아니라 어느 한 데이터 포인트 잡아서(실제존재하는) 자기 클러스터 다른 데이터 포인터와 비교해서 제일 근거리 있는 놈을 잡아서 걔를 대표값으로 잡음 = 클러스터 노이드

average= 구라
클러스터 노이드 = 존재함
클러스터 노이드 예를 들어보면, 유클리드 공간이 아님
예를 들어서 넣어놈
edit distance 잡았음 유클리드 공간 아님
average 없음 그래서 실제 4개의 포인트 있는데 각 포인트 스트링임 데이터가
4개의 데이터 존재함 가정함
예제니까 
각각의 edit distance 따져보면 이런식으로 값이 나옴
edit distance 어떻게 구하는지 암
이렇게 디스턴스 구해진다고 가정
클러스터 로이드 점을 구해야 하는데
3가지 방식(앞에 나왔던 이중 하나 를 선택 일반적으로 해보면 어느정도 한놈 찍힌다. 어떤것을 하던지 각각 뜬금없이 다른놈이 안나옴) 이것보면 
3가지 기준에 따라서 SUM/MAX/Sum-sq

표를 볼때
제일 작은값 max distance 제일 작은놈 
sum-sq 그값이 제일 작은놈이다. 
이런식으로 따졌을때 aecdb 클러스터 로이드로 잡아서 실제 클러스터 만들어 나간다.

## Defining "Nearness" of Clusters
클러스터 멀다 가깝다 는 디스턴스 개념임
직관적이다.
Min distance = 제일 미니멈 디스턴스. 두개의 클러스터 가깝다 디스턴스 개념이 들어감
각각 클러스터 한놈을 뽑아냄 각각 디스턴스 뽑아서 가장 가까운게 민 디스턴스다.

## Distance between clusters
점 사이에 제일 가까운 distance 값 두 클러스터의 민 디스턴스임
반대로 max distance, 지름
어떤 클러스터에서 두개의 포인터들을 잡아서 제일 큰 값임

어떤 클러스터 두개가 있을때 제일 큰 길이가 제일 먼 두점을 뽑아낸 max distance라고함 
이 두개가 클러스터가 됬을때 반지름이 됨 일명 (클러스터의 지름이라고함)
클러스터의 지름 

클러스터 노이드가 있다.
클러스터 노이드당 자기 클러스터 속한 다른 모든 점들의 거리를 다 잼. 그중 제일 큰놈을 잡는다. 걔가 그 클러스터의 반지름이 됨
diameter
radius of a cluster(참고사항)

## Defining "Nearness" of Clusters
그륩 별로 이제 ??? 모두다 구해
모두 다 구해서 average를 distance로 잡자
자세히 보면 보인다(그림) 각각 점들끼리 n by n 으로 연결되어있음
디스턴스 구해서 average 값 나옴

센트로이드 디스턴스 = 클러스터의 센트로이드 끼리 디스턴스 구한게 센트로이드 디스턴스이고

Density-based approach 거리를 따질때 클러스터가 있는 그 클러스터가 굉장히 densy한 클러스가 있는데 어떤것은 스파러스 하다 이 두개의 거리를 비교할때 불공평할 수 있음
그럴때 보증해주기 위한 방식
포인터 개수를 나눠서 노멀라이징 시켜서 그러고 나서 디스턴스 구하는 경우 생김
그런 density 고려해서 해주는 변형된 방법도 존재함
고런 부분 고려해서 고정해주기 위한 application 임

으이 어려워!!!

## k-means clustering
클러스터링 공부할때 나옴
그정도로 가장 대표적 가장 많이 쓰임

## k-meas Algorithm(s)
average 가 유클리드 공간에서 나옴
유클리드 공간에서 가정하고 있음
average 라면 센트로이드 개념을 반드시 쓸것임


## Populating Clusters
k가 클러스터의 개수임
그래서, k개의 클러스터링을 함
클러스터에 centroid 이용해서 포인트 어사인먼트로 클러스터링을 만들어감
k-means 방식은 하이러리커한 방식이 아님
계층적 방식이 아님 
점들이 우리한테 많이 있다. 점이 라고 말해도됨 유클리드 공간임
점들이 존재함
몇차인지 중요하지 않음
점들이 있을때 우리가 k-means clustering을 어떤 식으로 해갔는지 보면

## k-means Algorithms(s)
클러스터를 초기화 한다. 클러스터 할때 어느 한 대표가 되어줄만한 한놈을 찝는다. 
잘 생각해보면 말이 안맞음 클러스터 한적도없고 데이터 포인터만 잔뜩 있음 어떤놈이 센트로이드 가 될지 모름 백그라운드 정보가 없음

하나씩 클러스터당 뽑아서 클러스터 init을 해라 -> 말이 안됨
어떤 점을 대표점으로 뽑을지 approach가 있음

가장 쉬운게 랜덤임 (랜덤으로 함) 랜덤이 구현적인 측면에서 
k-means는 랜덤으로 뽑되 제일 처음 데이터 뽑을때 랜덤으로 뽑음
랜덤한 점을 뽑고
두번째 점은 그점이랑 가능한한 먼놈을 뽑는다. 그러고 나서 두번째 뽑히고
세번째는 이놈들이랑 따져서 먼놈을 뽑는다. k개를 뽑는다. 
k개의 클러스터링을 할거라서 가정했기 떄문에 이 점들이 걔네들의 센트로이드가 되어줄것이라고 믿으면서 k개의 초기포인트 뽑는다.
(가까이 있는 애를 뽑으면 센트로이드 값이 바뀔 수 있으니까 그런게겠지?)

두번째 방식은 샘플링 방식인데
많은 데이터들을 일일이 클러스터 미리 하기 힘듬 샘플링 방식은 미리 클러스터링 함
이 클러스터링이라는게 궁극적으로 제대로 된 최종값의 클러스터링이 아니라 
그 샘플링(1/10, 1/100 이든..) 데이터 포인터 샘플링만 해서 걔네들끼리 먼저 클러스터링 막 함
그렇게 하면 굉장히 빨리 끝날 수있음 전체 데이터를 다하는게 아니기 때문에 샘플링을 몇개만 데이터를 가지고 계층적이던 간에 통상적으로 계층적으로 한다. 
샘플링 된 놈들로 초기화를 시켜서 클러스터 대충 만들고 클러스터에서 한점을 뽑음.
이렇게 뽑을 때는 
클러스터 이미 만들어졌음 
클러스터 만들어지고 나니 센트로이드 구할 수 있고 센트로이드가 있으니까 걔랑 제일 가까운 한놈을 대표점으로 뽑아서 k개로 만듬
1번 ,2번을 쓰든 k개의 대표점을 뽑아 놨음

k개가 현재로서는 센트로이드가 됨 센트로이드라고 가정함
센트로이드를 만들어 놨기때문에 클러스터링을 함

## Populating Clusters
클러스터 ..??
각각 모든 점들을 다 봐야함
만들어 놨고 이제는 각각 처음부터 모든 점 살펴봄
제일 가까운것 
어떤 한점이 있음
그 한점이 우리가 어떤 k개의 뽑아놓은 가정을 가지고 뽑아놓은 k개의 점들과 누가 제일 가까운지 따져서 거기에 제일 가까운 놈에게 할당함
두번째 점도 k개의 점을 따져서 할당하고 assign한다 할당한다.

점을 가지고 
점을 클러스터에 할당하면 센트로이드가 바뀐다.
그래서 k-means라는게 단순하지 않음
봐야 하는 점이 100만개라면 처음에 내가 앞에 있는 애 뽑아서 k개의 센트로이드가정하고 계산해봤더니 얘랑 제일 가까워 
a라는 포인트랑 제일 가까워 이 점을 거기다 assign 했음 알고보니 센트로이드가 바껴버림
뒤에 있는 놈들은 그 영향을 받음
그 뒤도 영향을 받음 
그런 프로세스를 거침

얘는 어떤 아웃라인 이 있을때 얘의 클러스터링할때 큰 영향을 미침
그 한놈 때문에 센트로이드가 확 바뀐다. 지금까지 한것은 바보되는것일 수 도 있음

k-means은  outline에 약함(민감하게 작동)

그런 식으로 업데이트 하고 계산하고 그런식으로 클러스터 할당
어디까지 하냐면 
요 number2랑 number3랑 assign 하고 센트로이드 계산 이작업을 몇번에 걸쳐서 반복함
2,3 번 반복을 더이상 움직이지 않을때까지 
어느 한순간이 되면 더이상 센트로이드가 움직이지 않음 센트로이드가 움직이지 않으면 데이터포인트가 움직이지 않음
이미 우리가 가정해가지고 어사인 할때 그 센트로이드로 기준으로 계산했기 때문에 더이상 움직이지 않음 그렇게 따지면 ???함 k-means의 경우

## Example : Assigning Clusters
k-means를 이용해서 클러스터링 해야하는데
k개의 점을 뽑아야함
한점을 랜덤으로 뽑음 
뽑았을때 그 다음에 여기서는 편의상 k가 2라고 가정
하나 뽑음 나머지 한점을 뽑아야 하는데
얘랑 제일 먼데를 뽑음
이렇게 해놓고 이 두개가 현재 초기상태에서 클러스터에 센트로이드라고 가정
초기화 상태완료
나머지 데이터 포인트 뒤져서 거리계산함
하나하나씩 계산함
디스턴스 나옴
1차적으로 걔네들한테 assign함
파란색 x라는 클러스터 구성 나머지 얘네들은 이 핑크의 클러스터를 만듬한 라운드를 돌고나면 클러스터링이 되는데 클러스터링이 되는순간 센트로이드가 바뀔 수 있음
이 핑크랑 블루는 더이상 센트로이드가 아니다. 센트로이드 정보를 업데이트해야한다. 값이 바뀜 
센트로이드가 바뀌면 얘네들 assignment가 달라짐 
1 라운드 돌았고 2라운드 다시 돔 
2 라운드 다시 돌땐 각각 데이터 포인트에 대해서 다시 distance 계산 해서 자기랑 그 센트로이드랑 제일 가까운 놈에게 다시 어사인함
어떤 놈은 그대로 어떤놈은 바뀌기 도함

## Example: Assigning Clusters
블루에 있는 x가 이제는 핑크에 가까워졌다. (센트로이드가 바껴서??)
다시 어사인 하게 되면 값이 바뀜 
이게 2라운드 돌린거다
2 라운드 도니까 다시 클러스터링 다시 되고 
센트로이드가 다시 바뀌게 된다.
라운드를 돌때마다 센트로이드가 바뀜 그러고 나서 다시 한 라운드 또돔

이제는 클러스터가 다시 바뀜
핑크는 요렇게 됨
이렇게 바껴서 나감
어느 한순간 되면 더이상 움직이지 않음 데이터포인트 움직이지 않음 
센트로이드도 바뀌지 않음 
그럴때 스탑을 함 
그게 k-means clustering의 기본 원리임

우리가 볼땐 k값이 먼지아냐?
많은 데이터를 보고서 우리가 k값이 먼지 어떻게 아냐?
10이될지 1이 될지 어떻게 암?
k-means가 굉장히 ????
클러스터 굉장히 잘하는 알고리즘 맞지만
k값을 어떻게 구하느냐에 따라서 결과값이 달라짐
k값을 어떻게 선택함? 

## Getting the k right
k값을 각각 바꿔봄
클러스터의 개수 바꿔봄
k값을 바꿔서 각각 포인터들 
클러스터에서 점들과 센트로이드 간의 디스턴스 구함
디스턴스 구해서, 걔네들 값을 다 더해보면 k값이 작을때는 
average distance가 클수밖에 없음
너무 작으면 거리가 멈

반대로 k 너무 크면
average distance 작아짐
어느 한점에 가서 더이상 k개의 개수에 비해서 어느순간 distance값이 그닥 크게 변화가 생기지 않는 변곡점이 나옴 그게 옵티멀한 k의 값이 됨


k를 두개정도만 잡았다.
각각 데이터 포인트 들이랑 센트로이드 랑 그 거리가 자체가 커질수밖에 없음
그게 아니라 어느정도 적당한 선에서 클러스터링 이루어졌다. k값이 적당이 주면
값은 당연히 아까보다 작아짐
너무 또 잘게 나눴음
당연히 값은 작아짐 값이 작아지는 만큼 
값이 작아지는 폭이 줄어듬 
k값이 커지면 시간 복잡도 커짐
센트로이드가 많아짐 그럴때 k값을 너무 늘이면 말이 안됨
k값이 너무 작아도 말이 안됨 어느정도 선에서 스탑할꺼냐 우리의 몫
앱에 따라감

k개의 값을 구해냄, 

그림을 좀 봐라.

## Example :Picking k

그림은 위의 설명이랑 참고해서 보셈.

## k-means : advantages vs disadvantages
장 단점
시험문제다.
장점
구현하기 간편한다.
포인트 어사인먼트 방식
계층적으로 방식보다 포인트 어사인먼트 방식(컴퓨테이션이 적은방법 선택)
k-means 방법은 점이 새로 assign 되면서 계속 바껴나감
클러스터 자체가 타이트하게 만들어진다.
인스턴스 = 데이터 포인터
어떤 데이터가 들어왔을때 데이터 포인트 들어오게 되면 클러스터를 바꿀 수 있음
클러스터가 어사인되고 나서 센트로이드가 다시 업데이트됨 다시 오고 업데이트되고
결과적으로 데이터 포인터 하나하나 결과값을 바꿀 수 있는 그런 있다.(클러스터 이동)

단점
1. k값은 어떻게 아느냐 가장 큰 단점 k개가 알수있는 방법이 잘 없다.
샘플링 해서 k값 구하는 방법이 괜히 고안된것이 아니다.
2. 처음에 initial seed가 final 유저에게 영향을 줄 수 밖에 없음
3. k값을 어떤 k값을 구하느냐에 따라서 결과값이 달라질 수 있음
센트로이드가 업데이트 되긴 하는데 결과값이 달라질 수 있지만 몇 라운드 도는지도 결정됨 
잘 골라두면, 몇 라운드 만에 끝날것을 잘 못골라놓으면 몇 단계 거쳐서 엄한짓을 할 수 있음
데이터 포인트 순서가 결과값에 영향을 미친다. 데이터가 들어왔을때 오는 수고에 따라 얘가 어느 클라스터형태 달라질 뿐더러 클러스터가 할당되면서 센트로이드 값이 바뀌기 때문에 이 순서값에 따라서 결과값이 영향 받을 수 있음
4. 데이터값이 있을때 데이터 처리할때 노멀라이징 표준화 함
그런식으로 노멀라이징 결과값이 달라질 수 있음

그러면 
그거랑 상응해가지고 포인트 어사인트먼트 방식이아니라 계층적인 어떤 장 단점있는지 살펴봄

## Hierarchical clustering: advantages vs disadvantages
장점 
more informative 
k-means는 계층 개념이란게 없음 계층적이라고 되어있는것은 이런식으로 나뭇가지처럼 뻗어나감 그런 struct 만들어짐
그게 뭐가 좋으냐면 이게 cluster 몇개 나올지 알 수 있음
k-means는 알방법이 없음. 최종값이 나오기전까지 예측 어려움
계층적인 방식은 중간중간 값을 가지고 struct 있으면 더 많은 정보 제공해주기 때문에 우리가 클러스터가 몇개 대입하면 다 알 수 있음
간단하다. 구현하기

이전꺼는 포인트 어사인 먼트 방식이니 비교하면 간편할꺼임
단점
이 방식의 단점은 언두가 안됨
두개 클러스터 합쳐 한개 클러스터 만들어짐 올라가버리면 언두가 안됨 (되돌아갈 수 없음)
인스턴스가 클러스터에 할당되고나면 어사인된 인스턴스가 다른 클러스터가 속할 방법이 없음(데이터 포인트 = 인스턴스)
타임 컴플릭스 문제 n의 큐빅이었음 3제곱 컴플렉서티 요구 n에 큐빅
n값이 굉장히 크다고 생가하면 컴플렉시티 굉장히 커짐 대용량 다루기엔 적합하지 않다.
(k-means 샘플링 작은 수 k을 잡아서 hierarchical 방식으로 init으로 클러스터 만들어서 걔네들의 센트로이드 방식으로 많이쓰인다. 이거 잘못된거 같음 point to assignment방식인듯)
initial seed는 fianl result에 영향을 줌 (초기 단계는 자기자신이 클러스터가 됨) initial seed 는 어떤 놈을 두냐에 따라 결과가 달라질 수 밖에 없음
데이터 순서가 영향을 미칠 수 밖에 없음
outlier에 민감함

포인트 어사인먼트 방식도 outline에 민감함, 얘만의 문제는 아니다.
큐 알고리즘 자체도 해결하기 위한 방법 이런 문제점 때문에 나옴

## The BFR Algorithm
bfr Algorithm = k-means 임
굉장히 큰 데이터 set에다가 k-means 쓸려고 했더니 상식적으로 아까 봤던 그런 알고리즘이면 안됨 n by n 계산 모두다 하는 이런 알고리즘은 메모리 로드가 되있을때 함
메모리 넘어가는 데이터 셋은 그짓을 못함
굉장 히 큰 데이터 셋 가지고 있음
거기다 k-means algorithm을 적용시킬까 고민한게 bfr 알고리즘임
k-민스 바탕, 굉장히 큰 데이터 셋 
메모리 다 로딩 될 수 없을 정도 사이즈에 데이터 셋이 있을때 쓸 수 있는 그런때 쓸 수 있는 k-means 알고리즘의 변형본임

클러스터가 있을때 shape이 이런식으로 클러스터 모양이 타원형이 된다 가정.
이게 뭐랑 연결되냐면 분포 디스트리비우션이랑 연결됨. 분포를 가우시안 디스트리비우션, 노말 디스트리비우션 가정함
실제 사실은 많은 클러스터링이 그렇다. 클러스터링한 방식만 봐도 많은 클러스터링 자체가 타원형이나 원형을 중심으로 클러스터링이 되어있는게 사실임
통상적으로 대대분
실제 클러스터링은 실제 그렇게 만들어진다.(타원형으로???)
가우시안 분포, 가운데가 min값 중심으로 점점 작아지는 구조
그 중심으로 데이터 포인터가 작게 모여서 
중간에 가까울수록 덴스하고 밖으로 갈수록 적어짐 
클러스터를 그런식으로 함 
걔중심으로 클러스터 모여있는놈을 당연히 클러스터 하는데 당연한 말
strong assumption이 뜬금 없는 말이아님 
이 bfr 알고리즘은 유클리드 디스턴스에서 가우시안 분포를 가정하고있음

얘네들은 k-means의 변형본임 k-means는 유클리드의 디스턴스 스페이스 가정하고있음 
bfr 알고리즘도 유클리안 스페이스 가정함
유클리드한 스페이스 가정하고 있음 그런 말은 summation이라던지 average 값이 나옴
min 값이 라든지 standard devation이라든지 클러스터당 평균과 표준편차 값들이 나오게 됨 이게 뭐가 달라지냐면 뒤에 나오지만
어떤 값이 x1, x2 ... xk 있다고 가정하면 
디멘전당 다르다 이유가 
디멘전이 k 고, 이런식에 데이터 포인터가 있다고 가정하면 이 알고리즘에서는 min 과 stdd 기준이 뭐냐면 얘네들끼리 더한것이다. 더해서 민값이랑 stdd값이 나온다.
각각 디멘전당 그 더한값이 min값이 나온다 stdd 값이 각각 디멘전당 다 다르게 나올 수 밖에 없다. 디멘전당 다르게 나옴 이값이 클러스터당 도 달라진다. 클러스터가 달라지면 그 속에 어사인 점들의 개수랑 분포가 달라짐 분포가 달라짐 자기 디멘전당 이런식으로 summation으로 min값을 구하고 standard devation 구하고 역시나 각각 다 달라질 수 밖에없다. 

각각 다 달라진다.
클러스터 모양자체가 타원형 형태를 가정한다. 애네들의 또다른 가정 자체가 얘네들의 타원형이 이런 이런 타원형이 있는데 기울어진것은 안됨 

클러스터 자체가 스페이스 유클리드 스페이스인데 각각 축이 있는데 축이 존재하는데 축당 
이런 스페이스에 축이 존재함
타원형의 형태가 길게 기울어지는건 형태자체가 2차원 이라고 가정하면 x축이든 y축이든 축에다가 aligned 이 되어야 한다. 축과 상관없이 aligned은 제외한다. 
통상적으로 
bfr 알고리즘을 이런식의 클러스터 가정하고 클러스터링을 했다.
노말 디스트리비우션을 따른다. 

Efficient way to summarize clusters
클러스터를 summarize 한다. 메타데이터를 가져가 데이터를 기록하기 위한 extra data들 어떤 클러스터링 했을때 bfr 큰 데이터 가정있기때문에
k-means 다르게 어떤 클러스터가 와가지고 클러스터 어사인 되고 이정보를 다 keep할 수 가 없음 데이터 크면 모든 정보 메모리 keep 불가
실제 정보는 버려버리고 메타데이터 정보만 (서버리 정보만) 클러스터에 대한 메타데이터만 가지고있음 똑똑하다. 클러스터 가져다 서머링한다. 그게 굉장히
summarize 할때 데이터 개수에 거기에 비례해서 증가하는게 아니라 클러스터당 그런 정보를 얘가 그냥 keep 한다. 데이터당 아 아니라 원래 같으면 데이터 당이 되어야 하는데 큰 데이터를 클러스터 불가하다. 이런식으로 해서 데이터를 가져다 keep할 데이터를 줄여놓고 굉장히 큰 데이터를 가져다 얘네들이 클러스터링 할 수 있는것이다. 

over view였다.

## BFR Algorithm
큰 데이터 자체가 생각하는 디스크에 저장하는 데이터임
메모리에 올라올 수 없는 데이터량임

데이터 포인트 하드디스크 읽어와서 메인메모리에 읽어옴
데이터를 청크사이즈로 나눔
굉장히 큰 데이터읽어오는거 불가 ,적당한 사이즈로 나눔 (청크를함)
그 한 청크 메모리에 로드 될 수 있을 정도 분량
그래서 메모리에 로드 할 수 있는 정도의 청크 사이즈 만큼의 사이즈 하드디스크에 읽어옴
그리고 메모리에 올림
계속 처리함

메모리에다 청크 읽어들임
읽어 들인 데이터를 메모리에 있으니까 빠름 그런식으로 처리해서 
summarize를 한다고 	standard한 information만 keep하고 실제 데이터는 버린다.
그리고 summarize 인포메이션만 가지고 처리한다(메타테이터..?)

시작은 한다.
알고리즘 시작은 제일 먼저 
init 단계에서 하나를 로드함 
k-means 업그레드 버전
k개의 센트로이드 선택함
k개를 어떻게 선택하는지에 대한것은 난제 문제임 그것을 해결 하기위한 알고리즘은 아님(뒤에 큐알고리즘임)
k를 랜덤잡고 (멀리있는거 잡고 이런식), 선택을 하든
k개의 센트로이드 구함
어떤 써머리, 메타데이터를 스탯 인폴메이션 정보를 저장하냐면 이 3가지다.
이 3가지로 뭔가를 만들어낸다.

우리가 keep을 해야하는

## Three Classes of Points
우리가 keep을 해야하는 그정보가
discard set
버려버린다. 정보를 메모리들어와서 읽ㅇ어서 걔를 처리를 하고 나머지 버린다
그래야 새로운 데이터 읽어들인다.
굉장히 나이스 좋은 포인트
청크 해서 메모리 읽어 들어와서 포인트가 왔는데 센트로이드와 가까워 
이 클러스터에 속할꺼같아 명확한 점을 말함 
그 클러스터에 할당하고 나서 그 클러스터는 ??? 버려버리면 됨 그때 실제 클러스터
센트로이드에 해당되는 그 클러스터 어사인되는 그 점 그 데이터 셋을 가져다 디스카드 셋이라고함
실제 그런식으로 클러스터에 할당하고 그리고 걔네들을 가져다 스탯을 업데이트하고 그 정보를 버려버린다. (메타정보만 업데이트하고 나머지는 버린다.)
이 점은 클러스터에 들어가는 구나
점들이 이런것만 잇는게 아님

compressed set
이게 점으로 따지고 봤더니 뭔가 잘 안맞는다. 거기에 애매해 할당하는게 
지네들끼리 잘 모여있다. (애매한 애들끼리 잘 몰려있는데) 어디 클러스터 하기에는 애매하다.
그런 놈들에 그런 스텟 인포메이션 저장하고 유지하는게 컴프레스 셋
그런 애들의 모임을 cs라고함
다른 센트로이드와는 가깝지 않음 미니 클러스터라고 함 
실제 메인 되는게 아니라 지네들끼리 조그맣게 만들어짐
클러스터가 만들어짐 
k개의 클러스터에 속해지지 않는것 뿐이지 지내들끼리 클러스터 만들어짐 
지내들끼리 다시 그런 어떤 정보가 나옴 그런 정보를 나오면 메타 정보 유지해놓고 얘네들 역시도 버려버린다. 
summarized 되기 하는데 클러스터에는 어사인 되지 않는것을 말한다.

retained set
또라이 같은 애들이 존재함
outlier, 이게 어떤 책에는 compressed set 이라고 적혀지고 있음
따로 노는 outlier 있는데 이런놈들은 버릴수가 없음 데이터는 keep하고 있음
기존에 어떤 클러스터에 저장되 있으면 그 클러스터에 스탯 인포메이션이나 매센덱 인포메이션 을 업데이트 하고 나머지 버려버리면 그만인데 이놈들은 거기에 안들어가니까 그 방식으로 저장할 수 없음 얘네들은 keep해야함 메모리에 

## Example: points in DS, CS, RS
k개의 분류한 클러스터가 있으면, 포인트에다 어사인 한것을 ds 
k개 그륩에는 속하지 않음 지내들끼리 놀고 있음 이런것을 미니클러스터 cs 라고함
summarize를 하고 버려버린다. (메타데이터는 유지하고 나머지는 버려버린다)
outlier는 keep해야함 

3가지 set을 유지한다.
3가지 set을 유지하고 업데이트 한다.

ds와 cs는 클러스터가 구성됨
클러스터 구성하는 메타데이터정보 summarized 정보를 가져다 keep하고 나머지 데이터 버림
그러면 무슨 데이터를 keep 할꺼냐 
무슨 데이터로 summarized 유지? 
제일 먼저 저장, 그 클러스터에 포인터 숫자(N) 전체 카운터 숫자 저장
그리고 어떤 벡터를 가져다 벡터인포메이션 저장함 
d의 length를 가지고 있는 SUM이라고 부르는 벡터임
유클리드 스페이스에서 벡터가 있다고 가정, 
k이다. 요 얘랑 똑같은 디멘전 가지고 있는 그런 하나의 이 값들 얘네들 유지함 
얘의 벡터를 sumㅇ ㅣ라고 함 
왜 sum임? 얘네들 이 데이터 값, 각 디멘젼 당 같은 디멘전당 묶는 데이터를 다 더해서 
여기다 더함
얘네들의 더한값이 여기들어감 다더해가지고 들어감
세번째도 마찬가지
자기 디멘젼까지 감
sum vector 라고 함 이것을 유지함
sum에 
sumi라고 나오는데 단게 아니라 이 인덱스를 가리치는것이다. sum1 이라는것은 얘가 되는것이고 sum2는 얘가 되는것이다. 값 자체를 sum vector에 있는 그 위치 
그 디멘전에 있는 값을 sum i라고 함 
또 다른 벡터 유지 이벡터는 sumsq 임 똑같음 더하는데 얘네 각각 자기 디멘전에 있는 이 데이터값을 sqare 함 해서 다 더해서 아까 처럼 똑같이 각각 자기 위치에다 유지함
이거 sumsq vector임

클러스터 속한 포인터의 개수 n이랑 각각 자기디멘져 위치에 있는 그런 값들을 다 더한 sum vector, square 해서 다 더한 sumsq vector 이 3가지만 유지를 함

유지를 하는거지 실질적으로 알아야 하는건 저정보가 아님 
궁극적으로 클러스터링을 위해서 필요한 정보는 count 정보 필요함
뭐가 필요하냐면 센트로이드 정보가 있어야함, 이게 k-means라며 그럼 센트로이드 정보가 있어야함
클러스터당 각각 summarize 정보를 유지를 하는데 클러스터에 대한 우리가 지금 알아야 정보는 센트로이드임 
센트로이드를 알아야함 그리고 n개수 알아야함 그리고 stdev를 알아야함
자세히 보면 우리가 원래는 이 정보를 이용해서 

센트로이드 있어야함 n값이 필요하고 stdev가 왜 필요하냐?
뒷부분에 나옴
## Summarizing Points: Comments 
이 3개의 정보가 원래는 우리가 필요한 정보다. 그런데 이 정보를 가지고 킾을 하는데 
이 n과 sum vector, sumsq vector 이 값을 가져다 우리가 킾을 하면서 실질적으로 우리가 필요한 3개의 정보를 가지고 만들어냄
쉽게 만들어낼수있음 이 3가지 데이터를 keep함

N=>count keep
두번째로 센트로이드 를 가져다 클러스터당 유지해야함 굉장히 간단
sum vector를 가져다 각각 디멘전 N으로 나눔 걔가 센트로이드가 댐
sum vector 유지 n값을 알고있으면 쉽게 만듬

각각 sumi를 가져다 n으로 나눔 얘가 센트로이드가 댐
마지막으로 
standard deviation 필요함
어떻게 구하냐(시험문제)
ppt에 구하는 방식이 있음
sumsq 각각 i값에 n으로 나눔 sumi값 / n 의 제곱 빼! 
얘네가 variance =분산이됨

분산을 sqare root 씌여 버리면 표준편차나옴
분산의 정의가 어떤 값이 있는데 그 값에 다 평균값을 빼서 평균값을 뺀거에 스퀘어 값 다 더해서 평균값을 했음 
값에다가 평균값을 빼서 스퀘어 해서 그거에 평균값임 
분산임

이것을 가져다 풀면 나옴
실제 분산값은구해도 되지만 이 스퀘어 값 평균값 있으면 빼주면 됨 
제곱값의 평균 - 평균에 제곱값 
sum을 n으로 나누면 평균임
이 공식을 쓰면 분산값을 쉽게 구할 수 있음
이런것을 계산하기 위해서 메타데이터를 유지하고 있는다. 
결과적으로 필요한건 표준편차랑 카운트 정보 센트로이드다. 이게 필요함
클러스터당 유지하면댐 
애시당초 그 값을 keep하고 있지 뭐하러 귀찮게 저렇게 해서 하냐! 
그런데 한 스텝 더 들어가서 생각해보면 데이터 일일이 그런 값을 유지하면
클러스터에 n값을 알고있고 표준편차 따로 갖고 있고 센트로이드 따로 갖고 있고 점하나 들어가면 표준편차 센트로이드 값은 어떻게 구해? 그짓을 우리가 일일이 다 해줘야함 
점이 하나 들어간다고 STDEV 값을 유추해낼 수 없음
STDEV 한점 때문에 뭔가를 다시 다 계산해야댐 
그런 뻘짓을 안하기 위해서 만든것이 데이터 구조임

PPT 소개된 내용은 N, SUMi, SUMSQi 값을 가지고 있으면 
점이 들어오면 각각 계산해서 분산이랑 공식 그대로 가지고 와서 구하면댐
그런 효율성 떄문에 이 3가지 데이터 정보를 유지를 함 

디멘전이 길이가 있으면 실제 keep 해야 하는 정보는 2d+1 정보가 끝
sum에 vector 유지하기 위해서 d개의 데이터가 필요함
그리고 sumsq vector 유지하기 위해서 d개의 데이터가 필요함
그리고 +1은 n임
2d+1 개의 값만 keep하면 이 필요한 복잡한 계산 필요없이 원 큐에 끝낼수있음
직접적으로 사용하는게 아니라 저런 말도 안되는 하나의 이상해보이는 것을 유지를 하느냐 그런 이유 때문임

실제 알고리즘으로 들어가보면.



