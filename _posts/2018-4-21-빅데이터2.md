---
post : layout
title : 빅데이터2
---
# MapReduce
키워드, 중요한 챕터다.
하둡이라는것

프레임 워크다, 이론상으로 나온 구글이 제안한
하둡은 프레임 워크를 직접 구현해서 시스템으로 만들어 놓은것

이론의 제안은 하둡으로 구현한것은 야후다.
빅데이터 라는것은 어마어마한 데이터양을 처리하는것을 빠르게 처리할때 컴퓨터 1대로 처리하지 않는다. 빅데이터 처리하기 위해서 클러스터(분산 환경처리) 대규모로 처리를 실행한다.

클러스터 단위의 대규모의 클러스터 컴퓨팅을 하게 되면 컴퓨테이션이 있을때 어떻게 많은 서버들한테(클러스터들에게) 분산을 할꺼냐? 사실은 문제점이 분산프로그램이나 어렵다.

싱크로율을 다 맞춰야한다. 1000대 서버가 있다고 하면 싱크를 다 맞춰야 같이돌아가야 하므로 분산 프로그래밍은 쉽지 않다.

컴퓨터로 한대로 쓰레드로, 멀티쓰레드 해도 싱크문제가 발생한다.
이런 문제점이있는데 해결책이 뭐냐면 MapReduce다. 
MapReduce는 구글이 제안했다. 

## Large scale computing for bid data
분산하기 위한 인터페이스를 맵 리듀스라고 하고
클러스터에서 고성능 갖다가 달성하게끔 만들어지는 구현까지 포함해서 전체 프레임워크 자체를 맵 리듀스라고 한다.
맵 리듀스는 명령 프로그래밍 모델이다.

여러개 클러스트에서 동시에 돌아가게 되는 골치아플 문제를 굉장히 간단하게 인터페이스를 제공해주면서 병렬프로그램을 만들어주는 프로그래밍 모델이다. 구현은 포함되어있다.

맵 리듀스에서는 맵, 리듀스 스페셜한 펑션을 제공한다. 두개의 펑션을 가지고서 빅 데이터를 처리를한다.

맵 리듀스는 맵은 맵핑이다. 1:1 맵핑이다. 데이터와 맵퍼를 1:1 처음에 맵핑을 시킨다. 맵이라는 단어를 쓰고 리듀서는 각각의 맵퍼가 처리해서 데이터를 삭 줄여서 파이널 데이터로 리턴한다.

두개를 붙여서 맵리듀스 프레임워크라고 말한다.

## MapReduce & Single Node Architecture
컴퓨터 한대로 뚝딱하면 어느세월에 끝내지도 못한다.
굉장히 typical한 컴퓨터, 디스크 에 데이터 담겨져있고 데이터 처리하기 위해서는 폰-노이만 방식에 기본적인 typical한 architecture에서는 모든 데이터를 디스크에서 메인 메모리로 다 읽어들인다. 그리고 나서 cpu가 처리한다. 데이터를 가지고 언제 읽어서 언제 처리할까 ?? 무리다.

## Motivation: Google Example
구글이 고민했던것이다. 웹페이지를 갔다가 20kbyte다. 홈만 해도 이미 400 TB가 넘는다. 400TB당 컴퓨터 하드디스크에서 초당 30~35MB로 읽어들이면 읽기만해도 4달이 걸린다. 이거를 저장을 할려면 수천개의 하드디스크가 필요하다. 맞지가 않다. 
읽어들여서 뭔가를 처리해야 하는데 단순히 읽어들이는걸로 끝나면 안되고 컴퓨테이션 까지 하면 훨씬 오래걸린다.
말이안된다. 이런 빅데이터처리는 클러스트로 이루어진다. 어마어마한 수백만대의 데이터 센터를 둬서 돌린다.
리눅서서버를 갔다가 굉장히 많은 클러스트들을 묶어서 분산처리하게 된다.
전형적인 모델이다.

## Cluster Architecture
스위치가 뭔지 알지!
편의상 한대의 서버 혹은 한대의 컴퓨터라고 생각하자. 이런 컴퓨터가 수십대 붙인다. 하나의 reck 인쿨러져속에
렉이다. 
각각 넙적한 형태의 렉서버형태가 렉이클루져 속에 수십대가 들어간다. 16대~64대 들어간다.
굉장한 고가다. 컴퓨터들이 다 연결되어야 한다. 반짝이는게 스위치다.
각각 하나당 
렉서버를 갔다가 뒤에 백보드로 더 성능이 좋은 비싼 다른스위치로 또 연결을 한다. 
한대한대가 64대 수십대가 하나로 묶여서 스위치로 묶인다. 백보드로 더 빨리 처리하기 위해묶여있다. 
구글이든 페이스북이든 웹서버, 빅데이터 처리자체가 그림에서 보는것처럼 되어있다.

## Large-scale Computing
commodity hardware는 미국에서 많이쓰는데 시중에서 쉽게 구입할수있는 뜻이다.
장치다. 이런 서버에다가 Large-scale computing을 하게 되는데 
여러가지 문제점이 있다.

컴퓨테이션을 서버에 어떻게 디스트리비유투 시키고 
그런프로그램들을 어떻게 쉽게 할수 있을지?
이 많은 렉서버들 중에서 어느 하나가 죽어버리면 어떻게 처리하냐?
굉장히 단순한 문제를 보면
서버 한대가 3년만에 한번씩 죽는다
서버 1000대 가지고 있으면 산술적으로 매일 한대씩 죽는것이다. 

문제점이 데이터를 가져다 네트워크를 통해서 카피를 하게 되는데 
## Idea and solution

문제점이 데이터를 가져다 네트워크를 통해서 카피를 하게 되는데 데이터를 클러스트에다가 넣어버리면 각각 데이터들이 분산되서 들어간다. 내가 만약에 필요한 데이터가 이상한데 있으면 가지고 와야한다. 네트워크 통해서 가서 읽어와야 한다.
데이터가 워낙 커서 문제가 있다..
데이터를 가지고 네트워크 망을 통해서 읽고 쓰고 이짓을 하는데 클러스트로 구성되어있기 때문에 오버헤드가 굉장히 크다.
백본같은데 40기가 100기가 망이 달려있는것이다. (속도향상)
어떻게 처리할수있느냐?

key ideas
(중요)데이터가 너무 커지는데 읽어오자니 오버헤드가 너무 크다. 읽지말고, 컴퓨테이션 너가 데이터로 가. 컴퓨테이션을 갔다가 데이터에 최대한 가깝게 보낸다. 
컴퓨테이션은 우리가 짠 코드를 의미한다. 코드를 처리. 우리가 만든 데이터를 처리하기 위한 펑션이다. 코드를 갔다가 코드는 별로 크지 않고 작다. 차라리 코드를 가라!! 
최대한 데이터에 가까이가서 프로세싱을 하자.

죽어나가는 서버를 어떻게 해결하느냐, 그냥 데이터를 갔다가 replication 하자 똑같은 데이터의 카피 버전이다. 다 그렇게 쓴다. 분산처리에서 대부분다 reliability 카피를 한다.
통상적으로 3개가 복사가 된다. 각각 다른 서버에 퍼지면서 똑같은 데이터가 3개가 복사가 된다. 한놈이 죽을 확률이 높지만 똑같은 데이터가 동시에 죽을 확률은 낮다. (통상적으로 3개)

##### 맵 리듀스 문제는 어떻게 해결하느냐?
제일 먼저 빅데이터라고 하는것은 데이터가 저장이 되어야한다. 처리는 무슨..
저장부터
그러기 위해서 맵 리듀서는 파일시스템을 제공한다. 데이터를 저장해야 하므로 클러스트에서 os가 없음 그래서 거기에 맞는 클러스트 시스템에 맞는 ++분산 파일 시스템++을 제공한다.

그런 파일 시스템을 이용해서 hdfs, gfs 데이터를 가져다 클러스트에서 저장하고 manege 하기 위해서 쓰는 파일 시스템이다. 
그러고 나서 프로세싱이다.

2개를 모두 제공한다.

## Storage Infrastructure
문제점이 만약에 노드가 죽는경우, 서버가 죽으면 데이터를 어떻게 저장할것이냐!
분산파일시스템을 제공한다. 컴퓨터에서 돌아가는 파일시스템이 아니다.
수천개의 클러스트들에서 돌아가는 파일시스템이다. 그것을 분산 파일시스템이라고 한다.(dfs)

하둡 hdfs, 하둡 distributed file system 이다. 하둡에서 제공하는 분산파일시스템이다.
gfs는 구글에서 제공하는 분산파일 시스템이다.

이런 파일시스템을 제공함으로써 문제를 해결한다.
빅데이터를 데이터를 저장한다고 말했는데 그러면 파일시스템을 갔다가 디자인 할때 이유없이 심심해서 대충만드는것이 아니다. 파일시스템은 그 데이터의 성질에 따라서 파일 시스템이 달라져야 한다.

빅데이터 특성이 어떻길래, 파일시스템을 어떻게 만들어야할까?
1. 파일이 큰 사이즈고
2. worm이라고 하는데, write once read many 이게 뭐냐면 데이터읽다가 한번쓰고 나면 열심히 읽는다. 다시 막 써잿기지 않는다. 빅데이터 뿐만 아니라 모든 데이터의 특성으로 살펴보면 통상적으로 80% read와 20% write로 구성된다. 
데이터는 굉장히 드물게 업데이트 된다. 대부분 읽는다.

## Distributed File System(DFS)
청크서버, 큰 데이터가 들어오면 어떻게 저장할것인가! 저장을 못하잖아 그래서 청크서버라고 불리는 서버가 막 자른다. 큰 데이터를 같은 크기로 자른다. 16-64M 로자른다. 하둡은 64M로 자른다. 큰데이터를 게속 자른ㄷ. 클러스트에다 확 뿌린다. 이런역할을 하는게 청크서버다.
각 청크는 2~3개 통상적으로 3개. 청크 64M 자른 데이터를 서버로 뿌려서 3개씩 복사한다. 그래야 한놈이 죽어도 살아남는다.

reliability, 우리들이 64M로 계속 자르는데 3개로 뿌린다고 가정해보면, 3개의 데이터를 복사를 하는데 똑같은 렉서버, 똑같은 하나의 렉에 64개의 서버가 있으면 똑같은 렉에 데이터 3개를 복사한다고 생각해보자 바보같은 짓이다. 왜냐하면 렉서버가 파워 빠지면 날라가기 때문에
그렇기때문에 통상적으로 복사를 할때 자기가 하나를 가지고 있고 같은 렉에 하나를 더 가지고 있고 같지 않은 따로 따로 져있는 위치에 하나를 더 카피한다. 기본이다.
본인이 갖고있거나 자기가 아닌 다른 렉에있는 하나를 보고 아니면 다른 하나는 아예 다른 동네에 있는 그런데다 복사를 하거나 그렇게 한다. 왜냐하면 그렇게 해야 한대가 죽어도 렉서버가 폭격을 당해도 데이터는 1개가 살아남는다.

하둡도 그렇게 한다.
그러한 청크서버가 있고

Master node가 있는데 아까 처럼 수많은 서버가 있으면 결과적으로 누군가는 대장이 있어야한다. 분산시스템의 특징이다. 각각의 노드가 지가 알아서 돌아가지 않는다. 없다. 
결과적으로 코딩해주는 한놈을 뽑는다. 그 노드가 master node이다. 내가 모든 스케쥴이나 데이터를 뿌린다 던지 니가 살았냐 다 끝났냐 안끝났냐 다하는역할이 있다. 
서버를 master node라고 한다.

하둡에서는 name node라고있다. name node가 master node 역할을 한다.

Client library for file access
클라이언트가 데이터를 access할때는 분산시스템이있다. 클러스트에서는 보통, 클라이언트가 있으면 내가 어떤 데이터가 필요하면 master node한테 묻는다. 내가 지금 이 데이터 필요한데 물으면 masternode가 알려준다. 렉서버 노드 알려준다. 이정보를 갖고 이 클라이언트는 걔한테 곧바로 연락한다. 데이터를 두번 카피하는짓은 안한다. ++클라이언트가 master 정보한테 위치 정보만 얻기때문에 데이터 양은 굉장히 적다.++
그 위치에 어느 ip, 그 정보를 다 얻고 나면 데이터를 갖고온다. 그런 모델로 작동한다.

## Distributed File System(DFS)
같은 얘기다.
보면 각각이 서버다. 서버가 있는데 C0가 여기도 있고 여기도 있고 3개 있다.
그림은 정확하지 않다.
3개면 3개로 줄줄 있다.
각각 다른 청크서버, 각각 데이터노드에 데이터를 갖다가 알아서 적당하게 3개 카피를 유지한다. 죽어도 멀쩡하게끔

## Programming Model: MapReduce
굉장히 텍스트 다큐먼트가 있는데, 그 파일 word count
단어들을 쭉쭉 뽑아서 그 단어가 이 데이터에 몇번 나타나는지 word count
hello world 같은 예제이다.(급)

렉 로그를 뒤져서 어느 웹사이트가 인기가 있는지 찾고 싶은것이다. 렉 로그를 가지고 와서 다 뒤져서 url 몇번 나타나는지 count하는것이다. url 횟수가 제일 많구나 하면 걔가 제일 인기있는 사이클이 된다. 실제 그렇게 동작한다.

읽어서 그렇게 counting을 하고 싶다.
라고 할때 어떻게 해야할까 word count다. 열심히 메모리로 들어가서 폰-노이만 형태로 읽어들여서 처리를 할 수 있을텐데 너무 오래걸린다.

## Task : Word Count
데이터가 doc.txt 라고 가정하자. words는 펑션, 어떤 데이터가 있을때 텍스트 데이터 한줄을 읽어서 계속 던져준다고 가정해보자 단순하게 읽어서 던져주고 를 가정해보면 sort 라는 명령어는 각각 단어를 sort해준다. uniq -c (c는 카운트) 리눅스명령어다.

이런식으로 처리를 한다고 가정하면 모델링 이다. 한줄을 읽어서 워드를 솔팅하고 그중에서 걔가 유니큐한 쭉쭉 단어를 뽑아서 -c 하면 count가 나온다.
리눅스에서 해보면 이런짓을 한다.

case2 경우는 맵 리듀스의 처리방식을 보여준다. 맵 리듀스에서 저런식으로 처리한다. 데이터를 읽어서 뭔가 워드를 뽑아낸다. 그게 map이 하는 일이다. 그러고 나서 그걸 가지고 sort를 한다 그게 shuffle하는 일이다. reduce 단계에서는 모든 뽑아낸 데이터의 sort 된것을 가져다 다시 최종 share가 있으면 쭉 뿌려준다. 결과값으로 이런 처리 방식이 map reduce의 대표적인 처리방식이다.
예제다.

## MapReduce: overall procedures
큰 그림은 보여질것이다.
단순하지 않다.
제일 먼저 굉장히 큰 데이터를 한줄씩 sequencial 하게 읽어들인다.
어쨋거나 빅데이터처리하기 위해서는 데이터를 다 읽어야 한다.
읽어 들어와서 그런다음에 맵핑을 하게 된다.(맵퍼가)
맵은 우리가 짠다 맵 리듀스는 프레임워크를 제공하는것이지 실제 맵퍼가 펑션은 짜야된다.
그리고 나서 sorting 하게 된다. sort만 하게 되는게 아니라 다한다.
group by key : sort and shuffle 다 복잡하다.
reduce가 그 데이터를 가지고 와서 ~~ 하고(원하는 데이터결과의 형태를 만들어가지고)
그 결과를 쓴다.
이게 전반적인 전체적인 맵리듀스의 프로시져다.

## MapReduce: logical view
기본적으로 결과적으로 map 과 reduce는 key value 이고
중간에 sort나 shffle은 옵션과 같은것이다.(이것은 주어진다.)
가장 중요한것은 맵과 리듀스다.

맵 단계에서는 펑션이기 때문에 인풋과 아웃풋이 있다.
파라미터로 인풋으로 받고 맵도 결과값으로 아웃풋을 내놓는다. 
인풋값으로 받을때 어떤 인터페이스를 받냐면 key-value 형태로 받는다.

key-value 형태, db의 기본키처럼 key 가 똑같은 key다. value는 key에 해당하는 값을 value라고 정의했다.

받아들일때 맵은 key-value 형태로 받아들인다. map-reduce는 모든 input과 output은 무조건 key와 value로 무조건이다.
key-value로 인풋이 들어오면 그러면 맵을 가져다 하게 되면 또다른 형태의 key-value형태로 아웃풋으로 맵퍼가 내뱉는다. 
쉽게 mapper input이 있고 아웃풋이 있다. intermediate key-value라고 써있다. 왜냐하면 아직까지 이게 끝난것이 아니다. reduce까지 끝나야 그게 final data다. 얘는 중간단계다.
중간처리를 한다. 맵퍼가 key-value 받아들여서 또 다른 형태에 key-value 형태로 내뱉는다. 

## Reduce step
reduce 단계로 가면 mapper 가 내뱉은 이 output이 reducer에 input으로 들어간다. 이게 
솔팅단계 가져다 거치게 되는데 같은 키당 value를 다 sorting해서 합쳐놓은다.

key-value형태로 맵퍼가 내뱉고 그 형태를 갔다가 다시 받아들여서 리듀서도 다른 형태의 key-value 형태로 다시 내뱉는데 얘네가 final data 가 되는것이다. 리듀서가 내뱉는데이터가 파이널 데이터가 된다. 

## MapReduce : physical view
logical한 view다. physical하게 mapreduce를 바라보게 되면 앞에 생략이 되어있는데 굉장히 큰 데이터를 하둡 클러스트에 집어넣으면 청크 서버가 잘게 쪼갠다. 64 단위로 잘라서 정확하게 잘라진 각각 행위를 input splilt 라고 한다. 각각 64M 청크가 만들어진다. 그리고 맵퍼는 64M 청크당 맵퍼가 하나씩 할당된다. 맵퍼가 있을때 거기에 하나의 청크가 할당되고 다른 맵퍼에 또다른 청크가 할당되고 각각 독립된 점선은 하나의 서버라고 생각하자.

64M로 쪼개진 각각의 데이터를 맵퍼에 던진다. 작업을 해서 결과값을 내렸다. 이게 intermediate 결과값이다. 그리고 결과값을 리듀서가 가지고 온다. 네트워크를 통해서 읽어온다. 지가 다시 리듀서가 다시 처리를 하고 결과값을 낸다.

이게 최종 결과값이고 최종결과값은 reduce당 하나씩 나온다.
두개라서 part0, part1 으로 나온다.

청크로 나누고 맵핑 , 맵퍼하고 중간 결과값을 가져다 리듀서가 처리해서 최종결과값으로 내뱉는다.
나눠둔것은 파티션이라고 한다. 파티셔너가 저짓을 하는데 

하둡 맵리듀스의 physical view이다.
## More Specifically
세부적으로 본다.
mapper와 reduce는 우리가 구현한다.
무슨 데이터를 어떤식으로 처리할지는 우리가 결정한다.
mapper reducer는 프로그래머가 짜야 된다.

mapper와 reducer는 input 자체는 key-value 형태로 이루어져있다.

## MapReduce:Word Count
데이터가 있는데 Deer Bear River.. 우리가 프로세싱할려고 하는 빅데이터로 가정을 하면 이런 데이터가 있다라고 가정하면 제일 먼저 클러스트에 집어넣으면
그 청크서버가 열심히 자른다. 64M로 자른다.
그런식으로 수십메가짜리로 큰 청크를 계속 잘게 자른다. 똑같은 사이즈로

자르는 요하나 자체가 청크가 된다. 통상적으로 64M짜리 보면된다.
64M 짜리를 Spliting을 하게 되는데, input split 이라고 한다. 그 과정은 input splish라고 한다. 데이터를 갖다가 split 해서 나온 청크가 각각 맵퍼에다가 assign 된다. 워드 카운트 

워드 카운트 example을 다시보면 어떤 test data가 있을때 모두 읽어들여서 각각 word가 몇번 나타나는지 갔다가 counting 하는게 word count program이다. mapper 입장에서도 결과적으로 받아들이는게 key-value 형태로 받아들인다. 여기서 맵퍼가 받아들이는 key-value는 key는 파일이름이고 value는 (이건 wordcount에서 이렇다는것이지 짜는것에 따라 달라진다.) 64M를 가져다 한줄로 읽어서 쭉쭉 뿌린다. 맵퍼에게 던져준다. 그럴때 읽는게 한줄이 value가 된다. 64M로 계속 읽어 들여 가는것이다. 그런식으로 key-value 형식으로 던져준다. 던져주고 나면, deer bear river로 나와있는데 얘가 받아들여서 ++mapper에서 열심히 카운트를 해서 얘가 다시 아웃풋을 만드는데 그 중간값 역시도 key-value 형태다. 모든게 key-value 형태다.++
key는 Deer, Bear, River이 key가 되는것이고, value로는 그 카운트값을 던진다. 
car 1 이 두번나오는것은 (car 2로 하지않고) 맵퍼에서 이렇게 만들어져 있다.

word count는 이런식으로 돌아간다. 맵퍼에서 count는 항상 1이다. 1로 뿌려진다. 그리고 나서 shuffling 단계를 거친다. ++사실은 맵퍼 프로세싱 자체는, local에서 이루어진다.++
자기자신이 그 데이터를 local에서 써서 지가 읽는 데이터를 읽어가지고 자기가 처리하고 있는것이다.

그러고 나서 중간결과값을 내는데 reducer는 네트워크 상에서 다른것을 읽으라는것이 대부분이다. 어떤 reducer가 데이터를 읽어올때 네트워크를 통해서 읽어와야 한다. 네트워크 과정에서 일어난다. 그 과정을 shuffling이라고 한다. sorting은 읽는건 아니다. 여러가지 merge나 다른 복잡한 과정을 거친다. 

네트워크 가져다 congesting 시키면 안된다. 네트워크로 읽고 쓰고 하는게 오버헤드 여서 나온게 맵 리듀스 프레임 워크 인데 저런식으로 데이터를 마구 뿌려 재끼면 다시 네트워크가 혼잡해진다. 
비싸다. 

최적화를 어떻게하냐면, 예로 어떤 데이터에 car 너무 많다 라고 가정해보자 car 똑같은거 갔다가++네트워크로 보내면 안되니까 전송되는 데이터 양을 줄이기 위해서 싹 보고 최적화 가 가능하면 car(1,1,1) 이렇게 줄여버린다. 데이터 양이 훨씬 줄어든다. 이게 combiner가 하는일이다. 그리++고 이게 shuffle 과정에서 일어난다. 그리고 어떤 물론 word count에서 이러지 통상적으로 구현하기 나름이다.

car 3 이렇게 하기도 하는데 컴파일러?? 어떻게 구성하는지 나름이다.
그렇게 해서 reducer가 그데이터를 가져와서 합친다. car 3, deer 2이런식으로 
이렇게 나온다 그러고 나서 final result를 합쳐져서 뿌린다. 사실은 실제 코드를 보고 하둡 소스 코드를 보고 실제 보면 이렇게 되지 않는다. 각각 reducer당 아웃풋이 하나씩 나온다.

실제 그런식으로 코드 되어있다. ppt처럼 나올려면 reducer를 새로 짜야한다.
++reducer는 당 아웃풋 하나씩 각각 낸다.++

++word count는 데이터가 오면 key-value를 쪼개서 key-value 형태로 mapper 주고 mapper가 key-value, key-value 중간결곽밧을 만들어내고 reducer가 그것을 다받아서 counting한다. 최종결과값을 만들어낸다.++

++이게 왜 빠르냐면 각각 모든것들이 독립적으로 동시에 돌아간다. 맵퍼가 수천 수만개가 동시에 쫙 돌아간다. 그것을 병렬적으로 돌려서 동시에 처리하기 때문에 굉장히 빠르다. ++
어설픈 데이터 처리는 클러스터 갔다가 처리를 하기위해서 전처리방식 후처리 방식이 있다. 전처리 방식을 하기위해서 작업들을 막 한다. 하둡이 데이터양이 정말 그 이상으로 크지 않으면 이 낭비되는 시간만큼 크지 않으면 어설픈 데이터양은 하둡에서 더 느리다. ++

굉장히 하둡 프레임워크는 자체는 큰 데이터를 기술하고 있다. 18초~20초 전처리, 20초 남칫 전처리 한다. 일도안하면서 네임노드에서 job을 갔다가 assgin하고 스케쥴링 하고 하면서 일을 한다.

굉장히 크지 않으면 하둡이 이득이 있지 않다.
애시 당초 빅데이터를 가정하고 나온거기 때문에 병렬처리를 가정하고 나와서 당연히 빅데이터 시대에서는 월등히 빠르다. 

## World Count Using MapReduce
코드다.
map이랑 reduce는 우리가 짜는것이다.
map이 있고 인풋으로 key와 value가 있다. value가 한 라인을 읽은것이 value다.
mapper에서
한 라인을 갔다가 읽은 거기서 각각 워드를 갔다가 쭉 읽어서 단어랑 1 단어랑 1 이렇게 찍어준다.car 1 car 1 이렇게 찍었다. 

combine 하는것은 reduce에서 한다.
reduce에서는 나온것을 value를 for문 돌면서 그 값을 더한다. 다더하고 나서 key값과 최종적으로 결과값을 뿌려준다.

간단한데 코드를 봐도 wordcount의 경우 map경우는 20라인 정도 간단하다
reduce는 10라인 될까 말까하다. 그렇게만 짜면 빅데이터처리가 굉장히 빨라진다. 
다른 프레임워크, 싱크 맞춰줄까 ,어떻게 스케쥴어떻게할까 고민할필요가없다. 불과 몇십라인만 짰을뿐인데 하둡이 쓰는것이다. 그래서 하둡이 대단하다. 
하둡은 빅데이터 처리를 병렬적으로 처리하기 위한 프레임워크, 인터페이스 제공한다. 


## MapReduce:Environment
하둡 환경인데 인풋 데이터를 맵 리듀스 환경에서는 인풋 데이터를 어떻게 파티션할껀지
스케쥴링은 namenode 걔가 다 담당한다.
group by key 소팅, 셔플같은것들이 다 일어나고 
failure는 어떠게 저장할것이며
수많은 서버들당 커뮤니케이션이 일어나야지 서로 싱크를 맞추는데 그런 inter communication을 어떻게 하느냐!

## MapReduce : A diagram
데이터 사이즈 똑같이 하는게 맞다.
이론강의라서 그렇지 이 그림들은 하둡에서 구현된 것은 이렇게 되어있지 않다.
구글이 이론을 바탕으로 야후에서 만든게 하둡이기 때문에 이론상으로 청크를 자른다가 이론이다.

하둡에서는 우리는 똑같이 64m 자를것이다. 하둡에서 구현한 맵 리듀스이다.
데이터를 가져다가 청크로 쪼개서 청크당 mapper 을 각각 할당하고 key-value의 중간값이 나오면 그걸 sorting, shuffling 과정을 거쳐서 reducer가 각각 받아들여서 reducer가 각각 최종결과값을 하나씩 낸다. 

## MapReduce : Parallelism
분산처리의 핵심이다. 모든것들이 돌아가기때문에 성능이 좋다.
점선 하나하나를 머신으로 보자.
이런것들이 데이터 받아서 모든일들이 동시에 일어난다. 시간이 많이 절약된다.

## MapReduce:Workflow
데이터가 있고 셔플과정 거치고 reduce...??
reduce당 아웃풋이 하나씩 나온다.

리뷰삼아서 넣놓은것이다.
맵과 리듀스를 짜고 맵과 리듀스 구현하다보면 input data 값, 다 처리를 했을 때 아웃풋의 데이터형태를 이미 계산을 해야한다. app에 따라 다르다. 
word count에 따라서 word의 개수다. word만 그렇지 빅데이터 처리에 따라서 최종결과값을 무엇을 뽑아낼지는 우리가 결정한다. input도 마찬가지 app에 따라 달라진다.
++크게는 map과 reduce 짜는거지만 궁극적으로 input data 형식과 output data 형식도 같이 짠다.++ 

## DataFlow
하둡에서는 큰 데이터를 클러스터에 넣으면 잘게 쪼개서 처리하고 중간값을 썻다가 이짓을 계속한다. 이때 데이터가 무슨 파일 시스템을 쓰는지 보면 알고보면 당연하다. 인풋 데이터값과 파이널 데이터값은 dfs를 쓴다. 아까 봤던 많은 렉서버, 통상적으로 렉서버당 리눅스가 돌아간다. ++리눅스란 얘기는 자기자신에 local 파일 시스템이란 말이다.++ 그 하나의 렉서버 자기자신에 local 파일 시스템이다. dfs는 로컬 파일 시스템 한 레벨 위에 쓰는것이다.

이 dfs는 기존 로컬 머신보다 한 레벨 위에 있다. 그래서 대용량의 데이터를 넣으면 dfs가 64m로 쪼갠다. 이렇게 할때 쪼개는 이유가 뭐냐면 저장을 할때 dfs가 담당한다. 인풋 데이터는 dfs가 담당한다. 그리고 최종결과값도 dfs 형태로 저장한다. 리눅스에서 local machine에서 찾아봐도 아무리 파일에서 뒤져봐도 안나온다. dfs로 저장되어 있어서

++맵퍼가 중간결과 값을 갔다가 던진다고 했는데 통상적으로 local에다 적는다.++ 서버에서 맵퍼가 돌아갈텐데 얘가 중간결과값을 구지 귀찮게 오버헤드 있게 네임 노드에다 보내서 거기서 뿌릴필요가 없다.(오버헤드때문에) 맵퍼같은경우는 자기가 만든 중간 결과값을 자기자신 서버에다 넣지 

맵퍼에 중간결과값은 자기의 중간 결과의 key-value가져다 자기의 로컬 파일 시스템에 적는다. 
리눅스를 이용해서 local에다 적는다. 중간결과값을 local file system을 쓴다. 

어떤 데이터가 툭 들어오면 dfs 가 쭉쭉 쪼갠다. 쪼개서 맵퍼가 처리하고 그 중간결과값은 서버 자기자신에 로컬 파일 시스템에 리눅스를 이용해서 기록을 한다. 기록을 하고 리듀서는 네트워크를 통해서 거기에 있는 파일을 카피해서 가지고 와서 리듀서를 처리를 한다.
++
그래서 카피해서 가지고 온다. sort는 셔플과정일 뿐이고 리듀서는 맵퍼를 통해서 여기에 머신에 들어와서 로컬에 있는 데이터를 카피해와서 자기자신에게 처리를 하고 나중에 일을 다하고 나면 다시 dfs를 이용해서 결과값을 던진다.++

++input값과 output값은 dfs를 이용해서 적고, 중간결과값은 수없이 지웠다 적었다를 반복한다. 그 일은 mapper가 local file system을 이용을 한다.++

## Coordination:Master
coordination 당연히 필요하다. 얘는 cluster다. 수많은 클러스트를 갔다가 누군가가 coordinate를 해줘야 하는데 그걸 하는애가 master node이다. master가 coordinate를 담당하게 된다. 어떤 놈이 바쁜지, 무슨일을 진행중에, 다끝났는지 모든것을 master는 알고있어야 한다. 반장이다. 그래서 일을 끝난놈한테 다른일을 맡긴다. 노드 스케쥴링을 얘가 담당한다.

어떤 서버에서 어떤 노드에서 map이 끝났어 얘는 반장한테 알려줘야 한다. 맵퍼 다끝났어 반드시 알려줘야 한다. 그래야 또다른 일이 할당될 뿐더러 일이 끝났다는걸 알려줘야 reducer가 가지고 간다. 그런 컨트롤 타워가 master다.

맵퍼가 다끝나면 master가 reducer에게 알려준다. 데이터 필요하지 일 다끝났어 가지고가 ! 알려준다. reducer가 데이터를 가지고 와서 작업을 시작한다.

그리고 master가 담당하고 있는일은 수많은 서버들 죽는데 죽는지 안죽는지 어떻게알아, 죽었으면 일을 맡길수가 없다. heartbeat를 쓴다. 이것은 분산처리에서 기본적인것이다. 핑을 계속 때린다. 평균적으로 3초로 master 노드가 모든 노드한테 너 살아있냐 던진다. 살아있어 ack를 날린다. 그것을 보고서 master node가 판단한다. 죽었는지 안죽었는지
몇초 정해놓은 핑값 동안 응답이 안오면 죽었다고 생각한다 마스터노드는 
++거기에 할당되있던 일들을 다 날린다. 그리고 걔한테 줬던 모든일을 다른 살아있는 놈한테 다시 준다. 왜 죽었으니까++ 그런 모든 일을 master가 담당한다. 말그대로 컨트롤 타워다.

스케쥴링까지 담당한다.

## Dealing with Failures
Failure는 반드시 일어나고, 항상 일어나고 자주 일어난다.
기본적으로 failure 경우는 worker 하고 있는 애들이 죽는거고 master도 죽을수도 있다.

worker가 죽으면 hearbeat 이용해서 찾는다. master가 지금 하고 있던 그런 걔한테 할당된 일들을 가져다 다 취소하고 다시 실행한다.

failure가 일어났을때 맵 리듀스의 기본적인 철학은 ++걔가 하고 있던일을 때려치고 다시 해!++
마스터가 그 task를 다시 execution 한다. 보면, map 같은 경우와 reducer의 팔러씨가 살짝 다르다. 맵 같은 경우는 지가 컴플릿 했던일 내가 지금 하고 있다 죽어버리면 걔가 다시 수행을 한다. 맵퍼같은경우는 나는 일이 다끝났어 라고 해도 그 일조차도 다시 시킨다. ++왜? 일끝났는데 또 시키냐! 중간결과값이 지가 같고 있다. 끝이났지만 아직 안줬다. 지가 죽어버리면 중간결과값을 맘대로 어디로 가지고가. 그래서 좋든 싫든 끝난 맵퍼도 다시 실행을 한다.++ 
++ 리듀서는 다르다. 리듀서는 본인이 끝이기 때문에 끝났기 때문에 끝난거다. 하고있다가 죽은놈만걔네들만 in-progress만 다시 실행을 한다.++
맵퍼는 그렇지 않다. 로컬 데이터는 본인이 가지고 있다.

마스터가 죽으면 곤란하다. 끝장이다. 실제 그렇다. 답이없다. 나중에 secondary master node이다. 메인의 마스터 노드가 죽으면 primary가 죽으면 secondary가 대신일을 한다. 그런개념이 나온다. checkpoint 개념이 나오는데 master node는 지금 현재 job이 assign된 형태라던지 모든 정보를 가지고 있는데 그 정보를 가진 파일이 있음. 일정시간마다 백업받는다 그게 checkpoint다. 
checkpoint는 어느순간에 죽어버려도 백업을 받아놓은 정보가 있기때문에 백업 받은 사이에 일어나는것이다. 그 전까지의 데이터는 다 살아있다. 그러면 모든것을 recovery할 필요는 없다.

그게 checkpoint의 원리다. 
republication을 하거나(카피)

실제 맵 리듀스를 보면, 1800대 1600대 죽어도 멀쩡하게 돌아간다. 굉장한 능력을 보여준다.

## How many Map and Reduce jobs?
이것 역시도 구현에 따라 달라지는데, 맵퍼는 클러스트 노드 개수보다는 훨씬 많이 할당한다. 서버성능이 좋은데 맵퍼 하나만 일 하나만 하겠냐고, 금방끝낸다. 놀지못하도록 해야한다. 서버당 맵퍼를 여러개 할당하는데 하둡을 가져다 뒤져보면 하둡을 돌려보면 걔네들의 블록썸은 코어당 한개다. ?? 서버의 코어가 10개다. 기본적으로 10개의 맵퍼가 할당한다. 

이론이기때문에 여러개 한다. 실제 구현과 이론은 다르다. 구현된 코드를 돌려보면 딱 코어마다 할당받는다. 

DFS 청크 하나당 64M 하나당 맵 하나 할당되고, 요렇고 잘게 잘게 짤라놓은 이유가 뭐냐면 ++load balancing ++ 때문에 그렇다. 그리고 recovery할때도 유리하다. 큰 파일 갖다가 일을 하다 죽어버리면 다시해야한다. 그러니까 잘게 쪼개서 죽더라도 요만큼만 일을 다시 하게 된다. 잘게 쪼개서 여러 곳에 다 뿌려놔야지 load balancing이 된다. 그 수많은 애들한테 한놈만 한다 라고 하면 컴퓨터가 뻗기 때문에 잘게 나눠서 load balancing을 좋게 해준다.
++
일반적으로 reducer의 개수는 mapper의 개수보다 통상적으로 적다.
왜냐하면 mapper일을 한 중간결과값을 받아가지고 reducer가 최종결과값을 뽑을때 필요없는 데이터들은 다 제거하기 때문에 그래서 작은데 반드시 그렇지 않다.
usually, 애플리케이션에 따라 다르다.++

## Task Granularity & Pipelining
이렇게 리커버리할때 그런시간을 최소화 하기 위해서 
그리고 shuffling할때 파이플라이닝 자체가 concurrent를 처리하는것이다. cpu의 파이플라이닝과 같은개념이다.

master가 일을 뿌려서 work1이랑 work2 맵일을해! 라고하면 worker가 맵일을 쭉한다. 놀지말고 계속일해 맵3에게 던저준다. worker2는 계속 일을 한다. 맵이 끝이나서 중간결과값이 나와야 리듀서가 그재서야 시작한다. 리듀서는 맵퍼가 끝이나야 시작할 수 있다. 맵퍼의 아웃이 리듀서의 인풋이기 때문에 그래서 worker3와 worker4는 리듀서로 할당을 했는데 맵1이 끝나자마자 reduce가 바로 읽어들인다. 실제 이런식으로 작동한다.
map3 같은경우도 끝나자마자 읽어들인다.
map2
순서는 상관없다. 끝나면 읽어들인다. 끝나야지 만이 reducer가 시작할 수 있다.
파이프라이닝 가능하다. concurrent가 돌아간다. 이런식으로 작동을 하기때문에 병렬시스템이 가능하고 그렇기때문에 성능이 높아진다.

## Refinements:Locality
Locality는 대부분의 인풋데이터는 로컬 데이터다.
mapper 같은 경우는 그 데이터를 가져다 mapper에게 일을 맞길때 다른 데 있는 데이터를 읽어와서 일을 시작하면 바보 같다. 걔한테 가서 카피해서 가지고와야한다 일을하려면
그럴거면 니가 갖고있는 데이터를 니가해라 라는 말
locally하게 니가 읽어서 니가 하면 되지 멀리있는 데이터를 가지고오냐! 
스케쥴링 자체를 name node 가 관장해준다.
하둡에서 얘기하는 locality다.

++이렇게 해야지만이 네트워크의 밴드위쓰를 줄일 수 있다.++

하둡을 실제로 돌려보면 네트워크 밴드위쓰가 굉장하다. 굉장한 양의 데이터가 왔다갔다한다.
얘때문에 성능이 떨어지기 때문에 하둡에서는 네트워크는 밴드위스를 줄일려고 노력한다. 클러스터는 어쩔수가 없다. 

어떻게 달성하냐
1. 로컬리티, 지가 가지고 있는 데이터를 우선할당한다. 의도치않게 내가 필요한데 그 데이터를 안가지고 있을수 있다. 그러면 그 다음 데이터를 카피해올때 뜬금없이 먼곳의 데이터를 가지고 오는게 아니라 이왕이면 조금이라도 더 가까이 있는 옆에있는 데이터를 가져오게끔 노력한다. master가 담당한다. 그런식으로 네트워크가 congestion 되는것을 막는다.


## Refinements:Backup Tsaks
mapper가 일이 끝이 않나, 그날따라 error가 발생했는지 성능이 느려져서 맵퍼의 시간이 지체되는 경우 있다. 많습니다. 저 한놈떄문에 일이 끝이 안나! 일을 잡고 있어서 바틀렉이 걸린다.
전체가 바보가 된다. 

백업 테스크는 일을 하다 네임노드가 체크해서 끝나갈때쯤 얘가 했던 일을 다른애한테 던져준다 2개를 실행시켜서 얘가 먼저 끝나면 이전에 했던 애의 작업을 중단시킨다. = speculative execution이라고 부른다. 끝나갈때쯤에 두개를 동시에 팍 띄어서 먼저하는애가 이기는거다.
그런식으로 작동한다.

job을 가져다 예상치못하게 끊어지는것을 막는다.