---
post : layout
title : 빅데이터4
---
## 빅데이터4
빅데이터4

## RDBMS VS Hadoop
하둡이라는것은 맵 리듀스를 프레임 워크로 구현한것이다. storage 시스템인 하둡파일시스템 까지 같이 합쳐서 본다.

하둡은 RDBMS랑 비교를 많이한다.
RDMS는 DB시간에 배웠을 그 자체가 RDMS이다. ORACLE DATABASE 아파치 MYSQL 같은거 프레임 워크들이 RDBMS이다. 
데이터 사이즈가 하둡이 월등하다. 분산 시스템 이기 때문에 여러개 머신을 가져다 한레벨 위에서 묶어서 저장을 하기 때문에 훨씬더 큰 양의 데이터를 저장할 수 있다. 빅데이터 프레임 워크다. 오라클의 RDBMS에는 데이터 크긴해도 그런식으로 디자인한 것이 아니다. 그렇게 까지 큰 데이터 용량을 지원할 필요가 없다.

RDBMS가 오라클 데이터베이스가 꼬랐다가 아니라 목적이 다르다.

access 방식을 보면 interactive랑 batch라고 써져있는데 interactive는 컴퓨터앞에 앉아서 입력하고 답변을 주고 거기에 맞춰서 입력을 하고 이게 interactive다. 은행가서 돈을 내서 찾을거야 은행단말기 앞에서 누르고 interaction을 한다. 은행 단말기 뒤에는 rdbms가 깔려있다. 계속 interaction을 하잔아! batch작업도 가능하다. 은행에서 백업을 받거나 그렇게 일을 할때 개네 admin이 하는거지 우리가 하는건 아니다. rdbms는 다 지원한다.

하둡은 interaction을 지원하지 않는다. 큰 용량의 데이터를 처리해줘 명령을 내리면 우리의 input중요하지 않다. 데이터를 처리가 끝날때까지 interaction을 받지 않는다. 그게 끝이다. batch 작업이라는것은 그런식으로 할일 뿌려주면 그 리스트에 맞춰서 한꺼번에 모아서 처음부터 끝까지 촥 진행하는게 batch 작업이다. 하둡은 batch작업만 하고 interactive 작업은 하지 않는다. 그런 interface가 안다.

업데이트는 RDBMS에는 read/write는 많이 일어난다. 왜? 은행에서 돈을 하나 뽑아도 10원 하나를 뽑아도 모든 기록들을 다 담는다. 끊임없이 쓰고 지우고 굉장히 많이 한다. 하둡은 write once 주로 쓰기 보다는 읽기 전용이다. 물론 쓰기가 안된다는건 아니지만 쓴 데이터를 읽어서 처리하는 용도다. write보다는 read에 치중되어 있다.

structure 경우 RDBMS static schems이다. RDBMS는 스키마, 스키마는 static이다 도중에 바꿀 수 없다. 정해져있다. 그래서 sql s가 structure다. data가 struct로 되어있기때문에 미리미리 스키마 사이즈를 만들 수 있다. 근데 빅데이터는 그런게 아니다. 빅데이터 struct data 거의 없다. unstruct다. 그런 데이터를 저장해야 하기 때문에 스키마한 static이지 않다. 얼마든지 바뀔 수 있다.

intergrity(데이터무결성) RDBMS는 높다. 왜냐하면 acid atomy(원자성) consistently, i는 isolate, d는 durability 다.
얘네들이 개런티를 다 하기 위한 acid 다 서포트 한다 그러다 보니 intergrity가 높을 수밖에 없다.
하둡은 그런 목적이 아니다. acid는 무시한다. 

Scaling은 RDBMS는 not linear 하다. 그리고 하둡의 경우는 linear 하다. 클러스터를 가져다 넣을때마다 linear하게 계속 증가한다. 실제 성능도 계속 그렇게 증가한다. 

Query Response Time 는 RDBMS는 굉장히 빨라야 한다. 은행에서 돈 10원 뽑는데 오래 기다리면 화가난다. RDBMS는 interactive access 구조를 염두해서 만든것이기 때문에 굉장히 빠르다. 하둡은 latency는 일부 있는데 task가 있을때 setup 타임 clear time 이 있다. RDBMS는 없다. 
하둡은 일을 처리하기 위한 일종의 준비시간(?)이 있다. 중간데이터를 썻다 지웠다 작업을 하고  다 처리하고 나면 중간데이터를 확 지워버린다. 그런것들이 다 latency에 반영이 된다. (지연시간) latency가 존재하지만 목적이 다르기 때문에 상관없다.

## Where is Hadoop used?
테크널러지 에서 search 한다든지 
은행에서 Fraud Detection은 카드를 가지고 해외 여행을 가서 카드를 긁었더니 메시지 날아온다 결제가 안된다 던지 한국에 있는거 맞아? 왜냐하면 카드사는 백그라운드를 다 가지고 있다. 실시간 체크한다. 패턴 자체가 미국에서 활동했는데 달라지면 이게 실제 니가 쓴건지 누군가가 훔쳐서 쓴건지 체크를 한다. 
그때쓰는 방식이 Fraud Detection이다. 백그라운드를 하둡같은것을 돌려서 계속 체크하는 방식이다. 그런용도로 많이 쓰인다. 실제로 
밑은 넘어간다.

## Companies Using Hadoop
어떤 회사들이 하둡을 쓰냐 시험을 내지 않는다. 이런 회사가 쓴다. 예제일뿐 굉장히 많은 회사들이 쓰고 있다. 
Almost in every domain 이라고 써있는데 굉장히 많은 도메인에서 쓰고 있는데 하둡 자체가 모든 app 서포트 할 수 없다. 애네들도 분업을 해서 우리는 기본적으로 sql방식으로 처리하고 싶으면 어떤 프레임 워크가 필요하다. 그 소프트 프레임 워크를 얹으면 묶여서 처리할 수 있다. 머신러닝을 하고싶은데 하둡은 기능을 제공을하지 않는다. 필요한 프레임 워크를 얹어서 걔네들이 한꺼번에 묶여서 실행한다. 그런식으로 돌아가서 하둡의 생태계는 굉장히 크다.

## Hadoop is a set of Apache Frameworks and more
하둡이 뭐냐, a set of 아파치 프레임 워크 이다.
and more 가 more 굉장히 큰 에코 시스템으로 이루어져 있다.
빅데이터 프레임 워크는 기본적으로 데이터를 저장해야 하고 읽어서 처리를 할 수 있어야 한다. 얘네들도 데이터를 저장하기 위해서는 필요한 분산파일 시스템인데(dfs) h는 하둡이다.
hdfs, 어쨋든 하둡은 자기자신의 분산파일시스템을 가지고 있다. 그렇지 않으면 데이터를 저장할수없다. 하둡은 자기자신의 분산파일시스템을 가지고 있고 그것을 이용해서 데이터를 저장을하고 저장된 데이터를 갔다가 읽어와서 처리를 하는데 그 처리 하기위한 프레임 워크가 ++맵리듀스다.++
구글이 디자인하고 구현은 야후가!
하둡의 코어가 되고 

얘만 있는것이 아니라 다른 tools 프레임 워크 많다.
HBase, Hive, Pig 새로운 프레임워크
Huem sqoop
모니터링 하기위한 Greeplum
코어는 하둡 파일 시스템을 이용한 스토리지 아키텍쳐와 그 데이터를 처리하기 위한 맵리듀스
이 두개가 하둡의 가장 핵심이 된다.

## Core parts of Hadoop distribution
하둡이 파일을 저장, 데이터를 저장하고 있고
3개를 데이터에 대해 3개의 카피를 저장한다.
멀티개를 저장을 하는 경우는 페이퍼에 있다. 이론과 현실은 다르다.
야후는 3카피, 0카피도 가능
그리고 빅데이터를 다루기 때문에 기본적으로 노트북으로 4k방식 8k방식 잘게잘게 쪼개서 쓰는 파일 시스템 형태는 감당이 안된다. 굉장히 크게 자른다. 기본으로 64M로 자른다. 128M는 버전에 따라 다르다.

그리고 당연히 분산파일 시스템이기 때문에 굉장히 많은 하둡의 버전이 있는데 코어가 되는건 1버전이다.

하둡 1버전은 클러스터를 6천대 를 연결할 수 있다.
최소 수천대 ~만대까지 이상 연결할수있는 그런 scale을 보여주고 있다. 

++맵 리듀스 api는 잘 알고있다싶이++
배치 프로세싱을 하게 되고 분산, locailty를 이용해서 맵퍼가 자기자신 혹은 자기한테 가까이있는 데이터를 가져다 처리하게끔 한다.
그리고, fault - tolerant 라는지 availability 나중에말한다. 
다른 라이브러리도 나올것이다. pig, hive, hbase 같은것.

## Hadoop Cluster HDFS Storage
하둡 클러스터의 아키텍쳐다.
하둡 클러스터 기본 아키텍쳐는 마스터 슬레이브 방식으로 구성되어 있다. 네임 노드가 있고, 마스터 밑에 데이터 노드가 수천개 만대 밑에 쭉 뿌린다. 데이터 노드 자체가 서버 한대 컴퓨터 한대라고 생각하면된다. 많은 데이터 노드를 연결할 수 있다.
기본적으로 하둡에는 네임노드가 하나다.

마스터노드를 네임노드라고하고, 수천대의 슬레이브노드를 데이터노드라고 한다.
기본적으로 네임노드가 있고, 네임노드에서는

지금 설명하는 버전1에서는 네임노드가 죽으면 끝장이다. 왜냐하면 두번째 네임노드가 존재만할뿐 특별히 하는일이 없다. 백보드 역할만한다.

버전2로 가면 ha, hi abiliabity 한놈이 죽으면 다른놈이 살아난다. 예전에는 네임 노드가 죽으면 끝장인데 두번째 노드가 붙어서 첫번째 노드가 죽어도 두번째 노드가 일을 전담한다.
버전업이 되면서 추가된 기능이다.

하나의 네임노드가 여러개의 데이터노드가 있다. 각노드당 3개의 중복된 데이터를 저장을 한다. 그놈이 죽어버리면 안되니까

그리고 hdfs를 이용해서 데이터를 저장하고 읽고한다.
각노드는 리눅스 위에 돌아간다. 하둡이 유명해지니까 윈도우 용도 있지만 성능이 떨어진다. 리눅스 위에서 모든 데이터, 네임 노드가 돌아간다.

## MapReduce Job: Logical View
설명했으므로 패스

## Hadoop Ecosystem
하둡의 에코시스템이다. 이게 다냐, 다가 아니다.
훨씬더 많다. 이게 핵심되는 몇개만 가지고 온것이다. 하둡을 깔면 모든 빅데이터 처리를 할 수 있겠느냐 
하둡 하나만 해도 에코시스템이 굉장히 크다.
하둡 하나만 배우고 있다. 
나머지는 설명딀수도 없고할시간도 없다. 이런게 어떤 용도로 돌아간다. 라는 측면에서 공부해야 한다.
뭔가 하둡으로 써서 해야될일이있으면 힌트를 얻는다.
한가지 확실한것은 이런게 있구나 라는것만 알면 나중에 필요하면 그것을 찾아서 쓰면 된다. 코딩도 그렇게 한다.
많은 프레임워크 
자기가 필요한것만 뽑아서 쓴다.
하둡의 에코시스템자체가 넓다. 목적에 따라서 필요한 프레임워크를 쓰면된다.

양이들어가는거자체가 버전2라는 말인데 기본적이로 데이터가 깔려있고 
하둡으로 오게 되면 맵 리듀스를 가지고 워드 카운트를 하던 애플리케이션을 돌리는데
++맵 리듀스로 데이터를 다 처리하는것은 어렵다.++
데이터 형태를 맞게 프레임워크가 나오는것이다.

맵 리듀스 프레임 워크 맞는것은 그대로 써도 된다.
그게 아니라 sql 쓰고 싶다, 데이터가 sql 형태로 저장되어있다. 읽을 방법이 없으면 새로운 프레임 워크를 얹어서 쓴다.

인공지능, 딥러닝 그게 
맵 리듀스는 한번 맵과 리듀스가 끝나면 결과가 끝이 난다.
머신 러닝 기게학습은 계속 돌고 반복적으로 돌아간다. 그렇기 때문에 생겨난게 머함?? 이 있고
스파크도 맵 리듀스 하둡처럼 굉장히 중요한 프레임 워크다.

스파크 머신러닝 라이브러리 인데, 머신러닝 라이브러리를 지원해서 딥러닝을 한다. 실질적으로 딥 러닝을 한다. 스파크 를 사용한다. hdfs 위에 스파크를 돌린다. 굉장히 전형적이다.

가장 쉬운것이  스파크이다.
기존의 하둡에다 스파크 얹어서 하는것이다.
그래서 스파크가 중요하기 보다는 기존의 하둡의 단점을 많이 해결한다. 스파크는 하둡이 나온 이후에 나온것이다.
하둡의 문제점을 해결하고 나온 프레임워크이기 때문에 성능이 월등하다. 스파크도 굉장히 중요하다.

static 데이터뿐만 아니라 스트링 데이터 처리하고 싶을때

웹서치솔라??

데이터 저장방법이 여러개있는데, 빅데이터 대부분이 비전형화된 데이터를 읽고 쓰고 싶으면 ..

struct data를 처리하고 싶을때 밑단에다 스쿱을 깐다.

하둡은 이렇게 돌아간다. 하둡은 절대로 단순한것이 아니다. 
## Common Hadoop Disatributions
하둡은 자바로 구성되어있다.

## HDFS:Architecture
네임 노드 하나있고 데이터노드 구성되어 있는데
위에 네임노드 자체가 각 블락매니지먼트를 하는것이다.
얘가 네임스페이스를 갖다가 관리를 하는것이기 때문이다.

밑에는 클러스터다.
네트워크로 다 묶여져 있다. 실제 데이터 쓰고 읽고 할때 네임노드를 통해서 데이터를 쓰고 읽고 한다.
그래서 블락매니저모드로 쓰여있다. 왜냐하면 네임노드가 hdfs윗단에 있다. 그래서 쟤를 통해서 데이터를 읽고 쓰고를 다해야 하기때문에 그래서 네임노드가 블락 매니지먼트와 네임스페이스 관리를 다하고

나머지 데이터 노드 자체는 데이터를 저장하고 있는 노드일뿐이다. 데이터를 저장하고있다.
## HDFS:Basoc Features
특징

fault-tolerant 왜? 3카피 멀티플 카피를 하고있어서 심지어 두개가 죽어도 멀쩡하게 살아난다

throughput 클러스터묶어서 쓰루풋 높다

large data 빅데이터에 적합한 app

file system data, 실제 하둡으로 돌려보면 하둡은 데이터 메모리 읽어 들어서 하나의 스트림으로 읽어들인다. 스트림 데이터를 읽어들인다. stream이란 말은 데이터를 청크있다고 하는게아니라 물 흐르듯이 1tb, 물흐르듯이 쫙 읽어 온다.
읽어와서 잘라서 처리하고.
stream 데이터 방식을 가지고 있다보니까 data split에서 일어나는 부분 지정했을텐데 이문제와 상충되는 부분이다. 하둡은 저렇게 64M 빅데이터를 집어넣어도 얄짤없이 정확하게 잘린다. 하둡이 잘라져 버리면 하둡이라는게 64M 안잘리면 골떄린다. 이런 문제 하둡에는 안생기는 이유는 64M 처리 되든 말든 stream 데이터로 처리한다. ++stream으로 읽어서 메모리에서 쫙 읽어서 64M 로 붙여버린다. ++
ISC는 그렇게 안된다.
스트림 객체를 통해 읽어들인다.

굉장히 좋다. 어떤 분산처리를하고 클러스터를 하는데 굉장히 비싼 서버 해야되 이런것이 시작하면 인기를 얻지 못한다. 인기를 얻은 이유는 싼 머신을 여러개 묶어서 하니까 잘 돌아간다. 굉장히싸다. 성능이 나온다 그런데!(이게중요)
그래서 하둡이 성공을 거둘수 있던 원동력이다.
## HDFS Write(1/2)
파일 시스템이다. 파일 시스템이라 읽고 쓰는것이다.
데이터 HDFS 이용해서 데이터를 쓴다. 읽는다. 그것을 보면 굉장히 간략하게 되어있다. 이것보다 사실 더 복잡하다.
어떤 클라이언트가 있다. 빅데이터를 하둡 클러스트에 저장을 하고 싶어, 데이터를 WRITE 하게 되면 제일 먼저 네임 노드를 찾아간다. 내가 네임노드한테 물어본다 1TB가 있는데 데이터를 쓰고 싶어 라고 물어본다. 네임 노드가 알아서 클러스터 1,2,3 에다 써 라고 알려준다. 네임노드 자체가 얘네들에 대한 정보를 가지고 있기때문에 네임스페이스를 모두 관리하고있다. 모든 정보를 가지고 잇기 떄문에 어떤놈이 여유있는지 바쁜지 가지고 있다. 네임노드가 가지고 있어서 어디에 써 알려준다. 그정보를 가지고 네트워크를 통해서 클러스터에 쓴다.

데이터를 청크를 나눠서 64M 로 써지겠다. 청크는 64M 단위로 잘려지니까 WRITE 하면 그렇게 끝이 나는게 아니라 3카피를 쓴다고. 네임노드갔다가 쓰는거 까지 하면 클라이언트 끝이난다. 네임노드가 A1. A2 A3 A4 3개씩 알아서 카피해 라고 명령을 내린다. 네임노드가 정보를 가지고 말을 한다.

자기네들끼리 네트워크를 통해서 쫙 복사가 일어난다.
1TB가 3TB가 된다. X3이니까 그래서 abilibity를 높일려고 카피를 무조건 높일 순 없다. 이미3배 낭비하고 있다.
1TB 쓸껄 3TB를 쓰는것이기 떄문에

이런식으로 일어난다.


## HDFS Read
hdfs가 데이터를 읽어라고 하게 되면 클라이언트가 똑같이 네임노드에게 물어본다. 이 데이터를 읽어야 해 어느 노드에 가서 내가 그 데이터를 가지고 와야하냐? 네임노드가 알려준다.
네임노드가 그 데이터는 어느노드의 어디에 있다 알려준다. 그정보를 가지고 클라이언트 곧바로 가서 그 데이터 청크를 읽어온다. 기왕이면 localility 따져서 그 클라이언트와 가까이 있는 데이터노드해줄려고 할당해줄려고 노력한다. 이게 
## HA(High Availability) for NameNode
기본적으로 네임 노드가 있다. 그 밑에 하나의 네임노드가 여러개의 데이터노드로 이루어져있는데 기존에는 네임노드가 죽으면 시스템이 죽어버린다. 그래서 세컨드 네임노드를 만든다.

얘를 스탠바이 시켜서 스탠바이로 끝나는게 아니라 세컨더리 네임노드가 그때그때마다 첫번째 노드에 묶여있는 데이터노드를 갔다가 자기한테 백업받게 된다. check point 한다 계속해서 일정시간마다 체크포인트해서 세컨더리 네임노드가 첫번째 네임노드가 살아있는동안 백업을 받는다.

그래서 순식간에 확 죽어버려도 체크 포인트 하는 그 시간 몇초면 몇초 그 사이의 데이터만 날아가지 그전에 모든 일들은 다 살아있기 때문에 네임노드가 죽더라도 스탠바이가 자기가 일을 전담해서 자기가 이제는 Active 네임노드가 된다.

Availability를 높인다.
## MapReduce
지금은 하둡을 얘기할때 hdfs 관점에서 storage 관점에서 말했다. 이제는 데이터를 처리를 한다.
mapreduce 프레임워크다.

하둡에서 맵 리듀스가 어떻게 됬는지 말한다.
맵 리듀스는 배치 방식이고 분산 프레임 워크 모델이다.
그리고 기본적으로 하둡 맵 리듀스 역시도 일을 던져주면 일을 쪼개서 ++테스크++ 라고하는데 맵퍼랑 리듀서에게 던져준다. 맵퍼와 리듀스가 concurrent하게 병렬 프로세싱 해서 값을 던져줌으로써 데이터 성능 을 높혀준다.
분산 프레임워크가 맵 리듀스다.

새로운 용어 job 은 다시 하둡은 크게 데이터를 저장하기 위한 hdfs sotrage 관점에서 바라보고 저장된 데이터를 처리하는관점에서 바라본다.

데이터를 저장하는 관점에서 네임 노드 
데이터를 저장하는 관점에서 슬레이브를 노드들을, 데이터노드라고 했다.

다시 돌아와서
++ 맵 리듀스 관점에서 primariay를 job tacker 
++
하둡은 크게 두가지 일로 나눠져있고 똑같은 master node를 hdfs 입장에서 네임노드라 하고 데이터노드라 하고 맵 리듀스 프로세싱 측면에서 봣을때 master 노드는 job tracker하고 밑에있는 애들은 task 라고 한다. 이둘은 같이 있는것이다.++

job이라는것은 a full program이라고 쓰여져 있는데 하고싶은 job 을 말한다. word count일 하는 전체 자체를 하나의 job이라고 한다. 
빅데이터를 이용해서 word count 하고 싶어 하나의 컴플릿트한 프로그램을 job이라고 한다. 

job을 하나 던져주면 데이터 노드가 쪼개서 잘게잘게 맵퍼한테 할당해주면 맵퍼 아웃풋을 내면 리듀서가 그일을 처리를 하는데 그 짧막 짦막한 일들 잘게 잘게 쪼개놓은 일을 task라고 한다.

보통 data splish 라고 64M 하나 청크를 갔다가 하나의 테스크로 생각한다 맵 태스크에 어사인 한다.

그러고 맵퍼, 리듀서 나오는데 맵의 일을 하는 놈을 맵퍼라고 리듀서의 일을 하는애를 리듀서라고 한다.

맵 태스크 자체를 맵퍼라 하고 리듀스 태스크 자체를 리듀서라고 한다.

하둡에서도 당연히 맵 리듀스 프레임 워크는 KEY-VALUE 형태로만 작동을 한다.

그래서, 맵을 보면 key1-value1이 인풋으로 들어오면 key-value 리스트들 여러개의 쌍 key-vlaue
word - 1 반복
새로운 키와 벨리유의 수많은 리스트들 이 중간 데이터값이 될것이고 수많은 리스트들로 아웃풋을 내뱉는다. key1-value1 가 key2-value2로 바뀐다. 맵퍼의 input key-value와 맵퍼의 output key-value는 다를것이다. 키 밸리유가 달라지는것이고

리듀서의 input의 key-value는 맵퍼의 아웃풋인 이 리스트 key-value와 인터페이스 가 같아야 한다. 2 2 
key2-value2 
맵퍼의 아웃풋이 리듀서의 인풋이 된다. 최종 파이널 아웃풋이 된다.

## A MapReduce Job
Job이 흘러가는걸 보면 클라이언트가 있고 클라이언트가 있으면 맵-리듀스 작업을 갔다가 클라이언트가 이일을 해줘 
워드-카운트 해줘 명령어 자체가 job이다. 그 job을 주고
++하둡의 맵 리듀스의 마스터 노드가 job을 쪼개서 map 이랑 리듀서의 mapper이랑 reducer 에게 각각 던져준다.++

그리고 suffle 과정에서 리듀서 작업을 끝내고 최종값을 내게 되는데
인풋을 가져다 집어넣으면 데이터를 집어넣으면 인풋 splish는 통해서 그 빅데이터가 64M 또는 128M 단위로 쪼개진다.

그 인풋 splish당 맵퍼 하나가 어사인된다.

하둡을 프레임 워크 코딩을 하게 되면 기본적으로 맵이 어떤 동작을 하냐, 리듀서가 어떤 동작을 하는지 써야한다. 소스코드 뒤에 있다. mapper가 중간 아웃풋 내뱉고 나면 shuffle ,sort 과정이 있는데

shuffle이라는 단어 자체가 sort단계도 포함하고 있다. sort는 옵션이다. suffle & sort는 정확한게 아니다. sorts는 없을수도 있다. suffle은 반듯이 들어간다. mapreduce그림보면 mapper가 있고 suffle 그다음에 reduce 형식으로 그림이 있다. 그게 왜냐하면 통상적으로 두개의 단계의 묶어서 suffle이라고 하는경우가 많다.
suffle이 데이터를 취합을 해서 솔팅을 해서 key-value 솔팅해서 리듀서의 인풋으로 던져주는 과정까지 한다. 

리듀서는 다 맵퍼의 아웃풋 취합해서 최종 hdfs이용해서 최종 아웃풋을 내뱉게 된다.

## Hadoop components
하둡에서 hdfs, 데이터 저장하는
하둡의 map reduce 관점에서 프로세싱 관점에서
마스터는 잡 트랙커다. 잡 트랙커가 네임노드에서 돌아간다.
하나의 쓰레드로 jvm위에서 돌아간다.

구현해서 리눅스로 명령줘서 프로세스 찾아보면 있다.

네임노드를 있다가 맵리듀스 관점에서 잡트래커.
윗단계에서 관리를 한다.

맵태스크 리듀서태스크 관리하고 던져주고 체크한다. manage한다. 잘하고 있는지 모니터를 한다. 만약에 태스크 죽으면 어떤 노드가 죽으면 다시 실행시킨다. 
태스크 트래커는 여러개다. 잡 트래커는 하나다. 네임노드가 하나이기때문에 태스크 트래커는 여러개다. 데이터노드당 하나인데 어쨋든 슬레이브가 여러개가 있어서 태스크 트래커 자체는 노드 당 하나. 여러개가 존재할 수 있다.

슬래이브 노드당 태스크 트래커 하나씩 돌아간다.++
각각 맵 태스크 트래커는 맵 태스트크, 아니면 리듀스태스커가 제대로 작동하는지 안하는지 어쨋든 태스크트래커는 원래는 슬레이브지만 그 슬레이브 노드내에서는 왕역할을 한다. 그노드 내에서 맵퍼랑 리듀서가 다 돌아간다. 각각 하나가 태스크인데 동시에 여러개씩 돌아가는데 걔네들이 제대로 돌아가고 안돌아가고 있는지를 체크하는 중간단계 얘다. 태스크 트래크는 개들을 관리하고 그럼 걔네들이 일을 잘했는지 못했는지 잡 트래커 자기 보스한테 다시 얘기를한다.++

핑을 때려서 보고한다.
태스크 트래커가.

## Hadoop Architecture

태스크트래커를 실행하게 되면 jvm을 띄우게 된다.
jvm 하나당 맵 태스크, 리듀스 태스크 돌아간다.
job tracker-task tracker간 hearbeat통해서 보고를한다.

++마스터 노드가 있고 각각 여러개 노드가 있는데 잡트래커가 하나가 존재하고 (맵리듀스관점에서) 잡트래커고 태스크트래커고 이렇게 되고 파일시스템 storage관점에서 얘는 네임노드이고 데이터노드이고 데이터노드이다++

## Hadoop Architecture
master node는 job tracker라고했는데 
클라이언트가 job을 서브밋하게 되면 jar file이라 나오는데 java에서 jar file. jar file은 자바의 jar file이다. 실질적으로 job을 갔다가 define해서 클라이언트가 job을 갔다가 컴플리션해서 jar 파일 형태로 던져준다. ++job의 형태를 jar파일로 만들어서 뿌린다.++
그것을 master node에 던지면 job tracker가 그걸보고 데이터를 나눠서 슬레이브 노드 ~해 이런식으로 jar file=job을 던져준다. 각각 노드한테

그리고 일을 알려준다. 일 자체를 다 coordinate를 한다. 이 task 라는게 맵의 task가 될수도 있고 reducer의 task가 될수도 있다. 하나의 슬레이브 노드의 여러개의 수십개의 task가 동시에 돌아가기도 한다. 그걸 manage 하는애가 task tracker이다. 

맵 태스크는 인풋으로 
중간결과값은 local file system 쓴다.
reducer 경우의 맵 아웃풋읽어서 최종결과값을 hdfs에 쓴다.
## Job Execution workflow
조금더 세분화 시켰다.
반복되고 있다.
클라이언트 노드가 있는데 어떤 워드 카운트 하고 싶으면 다 코딩을 한다. 그 run을 짠다. 그것을 실행시키면 하나의 jvm이 뜬다. jvm 돌아가는데 거기서 job client 에서 word count를 하고싶으면, 처음에 job tracker에게 contact을 하고 그러고 나서 숫자로 된 job id를 받고 나오고 job id를 받아야 한다. job id 단위로 job tracker가 관리를 한다. 실제로 하둡을 돌려보면 job tracker가 job을 가져다 job이 한두가지가 아니기 때문에 굉장히 많음. 코딩할때는 job이름을 줄순있지만 job tracker에게는 숫자로 본다. id number로 받아서 job tracker가 숫자로 던져주면 그것을 관리한다.

시작할준비가 완료. 데이터가 필요하다. word count 그리고++ hdfs를 이용해서 쓴다. 썻어 그러고 나면 쓰고 나서 다 ++준비다됬어 word count 시작해줘 라고 submit job이 시작을 한다. submit job을 하게 되면 submit job속에는 mapper가 어떻게 작동을 하고 reducer가 어떻게 작동을 하고 데이터를 어떻게 읽고 어떤 형태로 쓸지 모든것들이 정의가 되어야 한다. 그렇게 정의가 되어있는것을 던져주면 job tracker는 그것을 가져다 그것을 base로 해서 노드에게 각각 던진다. submit job이 하나밖에 안나왔는데 여러개가 있다. 

밑에는 슬레이브 노드들이 있는데 태스크를 갔다가 64m 해당되는 ++데이터를 가지고 작업을 하라고 던져주면 task tracker가 그 데이터가 있는지 없는지 알아야 한다. hdfs를 이용해서 체크를 한다.++ 데이터를 가지고 가지고 온다. 다 준비가 되면하나의 스레드를 띄운다. jvm을 띄어서 그속에서 child fork해서 mapper를 돌린다. 물론 map task의 경우는 mapper를 갔다가 맵 스레드를 띄우고 reducer는 reduce thread를 띄운다. 각각 태스크당 jvm이 하나씩 쫙 뜬다.

실행시켜보면 jvm이 굉장히 많이 뜬다. 
클러스트 내에서
작업을 하고 다 작업하고 취합하고 나면 task tracker가 계속 체크한다. 다했냐 다했냐 체크한다. 다끝나고 나면 기본 3초마다 알려준다. progress를 알려주고 다 끝났으면 끝 알려주면 한가해요 라고 하면 일을 다시 부여한다. 

그런식으로 작동한다.

## Bottlenecks?
성능을 높이기 위한 옵티마이제이션 과정이다.
리듀서 같은 경우는 맵퍼가 끝이 나지 않으면 일을 시작하지 못한다. 기본적으로 맵퍼는 지 노드의 로컬 노드에 있는 데이터를 가지고 작업을 하는경우가 많지만 리듀서는 맵퍼를 통해서 데이터를 가져다 fetch해온다. 그게 suffle 과정이다. mapper가 다 끝나서 데이터가 다 준비가 되고 suffle sort 과정을 거친뒤에 가져올때까지 reducer는 멍하니 있는다.

멍청하다. suffle과 sort가 일어나는 과정동안 아까운 자원을 가지고 멍하니 있을바에 100% 되기까지 기다리지 말고 50~60% 정도되면 시작해! 이렇게 정해질 수 있다. sort나 suffle 과정에서 

progress default를 계속 한다. task tracker가 progress default를 하는게 아니라 task tracker가 자기 보스 node한테 3초마다 progress default 하는이유가 이것때문에 그렇다. 보고서 한 50% 정도 됬구나 리듀서 놀지말고 일을 시작해 이렇게 된다. 리듀서 가 일이 다끝나기도 전에 100% 가 안되도 일이 일부 끝난것을 가져다 fetch하기 시작한다. 

fetch하면서 일을 갖고 오고 시작할동안 얘는 mapper는 아웃풋을 내고 아웃풋을 처리 할수있다. 
파이프 라이닝이 일어나서 훨씬더 수행시간이 줄어들고 성능이 높아진다. 

이런식으로 옵티마이션 과정을 거친다. 셔플에 솔트과정은
이론만 가지고 구현을 해보니, 성능이 잘 안나온다. 그래서 야후들이 계속해서 성능을 업그레이드 시작한다. 이론과 구현은 다르기 때문에 옵티마이제이션 과정이다.

그리고 speculative execuion이라는것은 맵퍼가 끝이나야지 50%가 되든 시작하든지 말든 그게 맵퍼 가 끝나야 리듀서가 시작하고 리듀서가 일이 끝나야 job이 끝나는데 어느 한놈이 죽은건지 살은건지 게을러서 그런건지 mapper가 하나가 끝이 안나면 이 한놈때문에 전체성능이 느려진다. 

이런 불상사 막기위해서 하는것이 speculative execution이라고 하둡 에서는 말한다. 맵 리듀스에서는 백업테스크 라고 했다. 

프로그레스가 어느정도 진행되어가고 있는데 이상하게 요놈이 끝이 나야되는데 다른 맵퍼놈들은 끝이났는데 요놈은 끝이 안나고 생각보다 자꾸 늦어짐 그러면 네임노드가 잡 트래커가 그 태스크를 다른 노드에 던져줘서 똑같은일 을 실행을 시킨다.
얘가 먼저끝나버리면 기존의 것은 죽인다. 죽여버리고 얘 데이터를 가지고 수행한다. 그게 똑같이 수행시켰는데 얘가 오리지널이 먼저 끝나면 나중에 백업디스크는 죽인다. 이게 speculative execution이다.

어느 한놈때문에 바틀렉이 걸려서 전체 job이 끝이나지않고 느려지는것을 막아준다. 맵 리듀스 에서는 이론상으로는 백업태스크라고한다. 실제 백업태스크이긴하다. 하둡에서는 저렇게 구현한것이고 부를때는 스펙큘러티브 익스큐션이라고 한다.

## The Scenario
word counter를 할껏이다.
하둡 맵 리듀스 굉장히 잘이용하는 application이다.
워드 카운트가 많은것을 담고 있다.

map과 reducer를 짜야하고 step by step 어떻게 돌아가는지 살펴본다.
## Driver Code: Setting up the MR job
자바 클라스 많이 나온다.
쉽게 말하면
클라이언트 job을 define해서 그 define된 job을 가져다 job tracker에게 던져준다. 그 클라이언트가 job을 갔다가 뭐를 명시할지 보여주는것이다. 이것을 갔다가 드라이브 코드라고 한다. 실제 자바에서 드라이브 코드는 보통 run이라고 써있는경우가 많다. 실제 하둡 코드도 보면 run이다. run 코드를 만드는데 드라이브 코드라고 하고 여기서 하는것은 맵리듀스 잡을 가져다 setup해주는것이다. configuation 해주는것이다. 

제일 먼저 해줘야 할것은 jar 파일을 뿌려줘야 한다. - setjarByClass()  어떤 job이다를 보여주고 setup해주고 

데이터를 갔다가 받아야 한다. 인풋 데이터를 받아야 한다. 파일 포맷이 어떻게되고 패스가 어떻게되는지 

최종적으로 리듀서가 끝이났을때 hdfs를 이용해서 데이터를 적는데 ㅔ이터를 어디다 적어야 될건지

맵퍼와 리듀스(사실 가장 핵심)어떻게 구현할껀지

key-value인데 아웃풋에 대한 key-value 형태가 어떤식으로 아웃풋에 대한 key-value 형태를 가져다 우리가 정의를 해줄건지 이런함수에 다 있다.

맵퍼 아웃풋 키와 벨리유가 있는데 만약에 리듀서가 다르면 따로 setup을 해줘야 하고 

마지막으로 job name을 가져다 적어줘야 한다. 옵션이고 무시하면 jarfile이 자동으로 들어간다. 

그리고 이제 default input이 있는데 mapper의 input은 어떤 리듀서 든지 간에 무조건 key-vaule 형태로 받아들인다고 했는데 맵퍼의 인풋이 왜 key-value냐?? 라는게 궁금하지 않나?
++맵퍼의 input도 key-value의 형태로 읽어들여야만한다++
default key의 input이 byte offset이다. 파일ㅇ ㅔ대한 offset이다. 기본적으로 word count를 읽고 싶으면 텍스트이기 때문에 라인 바이 라인 인데 이때 default input이 뭐가 되냐면 key가 이 파일의 각각의 라인에 대한 offset이 된다. offset정보가 key가 되고 mapper의 한 라인 전체 읽어들인 한 라인이 value가 된다. 맵퍼에서

그얘기는 그러면 offset값이 의미가 있을까? key-value 받아들였는데 맵퍼랑 리듀서에서는 key-value가 데이터니까 쓰였는데 맵퍼에 input으로 받아들이는 key-value는 초기에 제일 받아들이는경우 key값이 파일의 offset값이고  value는 한 라인이다. 한 라인의 데이터를 읽어들이는건 이해가 되는데 그러면 offset이 무슨 의미가 있느냐 key값으로? 의미가 없다. 아무런 의미가 없다.

그래서 mapper 의 키는 무시해버리는경우가 많다. 프레임워크이기 때문에 좋든 싫든 key-value 값으로 input을 받아들여야 만든거지 처음에 mapper의 input으로 받아들이는 key는 보통 그냥 허당값인경우많다. 채워주기위한 값이다. offset이 의미가 없다.

default key는 byte offset이 되고 value는 읽어들이는 라인이 된다. Driver code를 보면 

## Driver code
제일먼저 클라이언트가 job을 가지고 define해서 던져줄때 명시해줘야 하는것이다. 드라이브 코드가

함수다. 거기에 뭐를 가져다 명시해야 하는지 job을 가져다 명시해줘야 하는데

그 과정이다.
job을 만들고

job네임은 setjobname이란 함수가 있는데 이것을 통해서 jobname을 명시할 수도 있다. 이것은 옵션이다.
setjobname 함수가 빠져있다. 왜냐하면 그게 옵션이기 때문이다. 자동으로 어사인이 싫으면 셋업을 해주면 된다.

이런식으로 명시해준다.
job.waitForCompletion 하다고 나오는데 이 속에서 코드를 보면 while 문에서 무한루프를 돈다. 왜냐하면 끝났는지 안끝났는지 계속 체크한다.

그리고 나서 task tracker가 계속 핑을 때려준다.다 끝났다 라 하면 flag가 바뀌면서 끝이 나온다.

waitforcompletion에 들어가보면 그속에 submit job 함수가 들어가 있다. setup까지 다 했는데 job을 submit 한다는 코드는 없다. job을 submit 안해도 된다가 아니라, 이 waitforcompletion 속에서 submitjob이라는 함수가 들어있다. job을 가져다 submit하고 얘가 completion 될때까지 계속 핑때리면서 pulling 하면서 기다리고 있다. 

이게 run code 이다. 실질적으로 run코드를 실행시키는것이다.
++클라이언트의 jar 파일을 가져다 명시하는건 기본적으로 하고 핵심적으로 맵퍼를 어떻게 짜고 리듀서를 어떻게 짜는지가 핵심이다++

## Mapper
맵퍼는 인풋키를 쓸수도있고, 무시할수도 있다. 아까처럼 인풋 맵퍼의 인풋의 key-value는 허당인경우가 대부분이다. 필요하면 쓸수있긴 하지만 통상적으로는 무시하는경우가 많다. key값 byte offset이다. value는 한 라인씩 계속 읽어들인다. 전체 읽어들인 line이 value가 된다. 
읽어들인 한 라인을 가져다 tokenizer 할것이다.
맵퍼 클래스 경우는 당연히 기존의 맵퍼클래스를 가져다 extends를 한다. extends
key와 value 클래스 경우는 writeablecomparable과 writeableinterfaces를 implement한다. 왜? 얘네들은 interface니까

key-value을 가져다 얘네들에 interface를 이용해서 implemente를 해야한다. 

맵퍼 같은 경우는 맵이란 함수가 존재한다. 맵이라는 함수를 override해서  구현을 해줘야한다. 원하는대로 

그리고 일반적으로 k1,v1 요게, mapper의 key1-value1 인풋 키 벨리유 쌍이고 k2-v2 맵퍼의 아웃풋이다.

++
통상적으로 당연히 맵퍼의 인풋 키-벨리유 형태는 당연히 아웃풋과 다르다.++

그래서 두개를 각각 따로 명시해야 한다.


## Mapper Code
wordcount에서 mapper가 어떤식으로 작동하는지 생각해보자.
우리가 wordcount가 에서 mapper는 데이터를 읽어서 tokenizer해서 각 워드당 1 워드당 1 워드당 1 이렇게 찍어준다. 예제도 보여줬다.

map이란 코드를 ovveride한다. 원하는대로 구현을 해줘야 한다. key value가 있는데 
input은 inutkey와 inputval 형태지만 output은 context 형태로 내뱉는다. 이역시 context 에다가 key-output 형태를 적어줄 뿐이다. 인풋으로 받아들이고 아웃풋으로 정의해준다. 이제 한 라인이 읽어들인 그 데이터를 가져다 stringTokenizer을 부른다. 토크나이즈를 불러서 토크나이징을 한다. 쓸때 context.write한다. 토크나이즈한 그 워드 랑 1 워드 1 을 ... while문 돌면서 라인 읽어들여서 토크나이즈를 계속 하는데 그 토큰할 내용이 있을동안 게속 읽어서 읽어서 1 읽어서 1 이렇게 한다. 

심심해서 mapper가 그런식으로 찍는게아니다. 맵퍼가 워드카운트가 되어있다.
예제를 보면 

## What the mapper does
예제를 보면 각 한 라인씩 읽어 들여서 인풋은 
value를 읽어서 this 1 
one 1
1 1 ...
이런식으로 나온다. 맵퍼를 돌리고 나면 이런식으로 찍힌다. 
이렇게 나오는이유가 한 라인에다가 토크나이즈 해서 계속 찍어주니까 그래서 저렇게 찍히는것이다.

## Suffle and Sort
suffle같은경우는 key-value 같은것을 묶는다.
그리고 실질적으로 셔플과정 속에 있는 컴바이너가 
각 맵퍼당 integrates를 하는데 이 integrate를 할때 output파일을 적어야 하는데 wordcount의 경우는 정확히는 코드로 filedotout으로 하나의 파일로 묶어서 찍어버린다.

## What Shuffle and Sort do
워드 카운트 예제를 보면, 맵퍼가 내뱉은 파일을 가져다 컴바이너가 있고 파티셔너가 있었는데 파일나눌수도있지만 워드 카운트 경우 묶어서 한파일 filedotout 출력해버린다. 셔플이
워드카운트해서는
key 입장에서 sorting해버린다.
솔팅을 하고나서 리듀스 한테 던져주는데 맵퍼가 있고 이렇게 단어1 단어1 워드로 나온것을 셔플과정을 거치면 이런형태로 각 단어 리스트 형태로 나온다. 

이것을 자세히 보면 구현과 이론은 다르다. 
구글 페이퍼에서 셔플 과정을 거치면 컴바이너가 라이크 3 준다고 했다. 야후는 그렇게 구현한것이다. 리스트로 묶어버린다. 이런얘기다. 이게 이론과 구현과 차이점이다. 

어쨋든 컴바이너가 묶어서 셔플속에서 그룹을 할때 얘네들은 실제 합쳐버리는게 아니라 리스트별로 합쳐버린다. +하는게 아니다. +는 리듀서 니가해 로 되어있다. 이렇게 셔플과정을 거치고 컴바이닝을 해서 넘겨주고 넘겨주기 직전에 sorting 하게 되는데 key별로 sorting을 하게 되는데 당연히 그대로 sroting이다. 머지소트한다. 머지소트를한다.

## Reducer
똑같이 맵퍼처럼 리듀서클래스 가져다 extends를 한다. 그리고 똑같이 interface를 implement를 하고 그리고 reduce의 input이랑 맵퍼의 아웃풋은 key-value형태는 같아야 한다.

당연히 리듀서란 함수가 있다. 임폴트하면 거기서 오버라이드해서 구현해줘야한다. keyvalue input context output하고 프로세싱을 한다. 프로세싱을 하는데 이게 listofvalues인데 우리가 아까 던져줄때 listofvalues를 던져줬다. 컴바인하지 않았다. summation하지 않았다. list를 돌면서 그값을 리스트에 있는 값을 더해준다. 지금에서야 reducer에서 시스템매칭을 한다. 이런식으로 서메이션해서 다 처리해서 다음에 쓴다. key value(sum)을 써준다. 이렇게 작동한다.

## Hadoop summary

## landscape

## Questions?
